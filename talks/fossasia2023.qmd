---
title: OpenAI Whisper and it's amazing power to do finetuning.
author: Kurian Benoy
subtitle: FOSSASIA Summit, Singapore
date: 2023-04-15
date-format: full
comments: false
format:
  revealjs:
    theme: solarized
    footer: "You can access slides @ [kurianbenoy.com/talks/fossasia2023.html](https://kurianbenoy.com/talks/fossasia2023.html)"
---

## Outline

- OpenAI Whisper and it's awesome features
- Fine-tuning and how to fine-tune Whisper?
- Results on fine-tuning whisper in Malayalam
- Conclusion

::: {.notes}
Originial Idea
- OpenAI Whisper(under appreciated model) - 1littecoder video
- Why it's awesome(whisper.cpp, long form transcription, lot of languages, whisper_normalizer)
- What is fine tuning? (Jeremy way of explaining)
- Fine tuning whisper in my language
- Results on fine tuning
- You can also achieve SOTA in your language
:::

## $whoami

- AI Engineer & Team Lead @ Sentient.io
- Volunteer @ Swathanthra Malayalam Computing(SMC)
- Open-source enthusiast
- Not affiliated to OpenAI

## OpenAI Whisper

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/1024px-OpenAI_Logo.svg.png){width=500 fig-align="center"}

- Whisper is the most `under-rated models` released by OpenAI.
- It open-sourced on September 21, 2022 by releasing the inference code and pre-trained model weights.

::: aside
According to [research paper](https://cdn.openai.com/papers/whisper.pdf) p.2, the name Whisper is an abbrevation for WSPR:

`Web-scale Supervised Pretraining for Speech Recognition`.
:::

## About OpenAI Whisper Model

- Whisper is a computer program which can listen to people talking and write down what they say.
- Whisper can understand people speaking different languages and can even translate what they say into English.

::: {.notes}
Last 2 points are generated by BingGPT.

- The model is robust general purpose speech recognition system aka ASR.
- It is trained on large dataset of 680,000 hours of audio recordings

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.
:::

## Whisper Models {.scrollable}


|  Size  | Parameters |Required VRAM | Relative speed |
|------:|----------:|:----------:|:------------:|
|  tiny  |    39 M    |     ~1 GB    |      ~32x      |
|  base  |    74 M    |     ~1 GB    |      ~16x      |
| small  |   244 M    |    ~2 GB     |      ~6x       |
| medium |   769 M    |    ~5 GB     |      ~2x       |
| large  |   1550 M   |   ~10 GB     |       1x       |


::: {.notes}
Below are the names of the available models and their approximate memory requirements and relative speed. Large itself has two version: large-v1 and large-v2.

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.
:::

## English Speech Recognition

![Whisper is competitive with state of art commercial and open source systems. Diagram from [whisper research paper]([research paper](https://cdn.openai.com/papers/whisper.pdf)) p.9](fossasia2023/Whisper_english.png)

::: {.notes}
A list of ideas for how to use Whisper in your own applications.
- English Speech Recognition
- Multi-lingual speech recognition
- Support for multiple tasks
- Can run in almost any devices with whisper.cpp
- Awesome community plugins
:::

## Multi-lingual Speech recognition

- Whisper model is trained on 99 languages
- OpenAI Whisper API supports just 57 languages as some languages performance are not really good.

::: aside
- More details can be found in Section 3.4 of [Whisper research paper](https://cdn.openai.com/papers/whisper.pdf) pp.6 - 8.
- Zero-shot Whisper improves performance on Multilingual LibriSpeech(MLS) but is still significantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli. ([Whisper research paper](https://cdn.openai.com/papers/whisper.pdf) p.7)
:::

::: {.notes}
- Room for fine-tuning models to improve
Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.
:::

## Runs in almost any device

- Since Whisper followed the open source route, [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
developed by [Georgi Gerganov](https://github.com/ggerganov) which is a port of Port of OpenAI's 
Whisper model in C/C++.

- It supports the below platforms:

1. Mac OS (Intel and ARM)
2. iOS
3. Android
4. Linux/Free BSD
5. Web Assembly etc.

## Awesome community plugins

- Word-level time stamps with [whisper-timestamped](https://github.com/linto-ai/whisper-timestamped),[whisperX](https://github.com/m-bain/whisperX) etc.
- Fine-Tune Whisper is achieving SOTA in lot of languages
- [Speaker diarization](https://huggingface.co/spaces/dwarkesh/whisper-speaker-recognition)
- [Audio classification using OpenAIâ€™s Whisper](https://github.com/jumon/zac)

::: aside
[Thanks to Ramsri Goutham article on 7 must know libraries and addons on top of Whisper](https://ramsrigoutham.medium.com/openais-whisper-7-must-know-libraries-and-add-ons-built-on-top-of-it-10825bd08f76)
:::

## What is fine tuning?

Given a pre-trained model, which is a large model which is trained
on a very specific task. If we want to fit it into our specific dataset we will train
and use the pre-trained model to build a new model which works very well for our task.

![Picture from [fast.lesson](course.fast.ai/) covering steps in finetuning a text classifier model](https://raw.githubusercontent.com/fastai/fastbook/823b69e00aa1e1c1a45fe88bd346f11e8f89c1ff/images/att_00027.png){width=300 fig-align="center"}

## Fine tuning is still relevant

{{< tweet waydegilliam 1641228571611123712 >}}

## Why try fine-tuning in Whisper?

- In your problem, the open source Whisper model doesn't give good results.

::: {.notes}
Whisper is developed as a robust speech recognition system which is
intented to work in out of distribution dataset. While most of previous
approaches relied on training in a few open-source dataset and using them.

If we are working on a language/dataset domain where the out-of distribution
Whisper model doesn't give good result we can use whisper to get good
results.
:::

## What are steps for fine-tuning Whisper?

![[Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-whisper)](fossasia2023/blogpost_image.png){width=700 fig-align="center"}

## What are steps for fine-tuning Whisper?

1. Preparing Environment
2. Load dataset
3. Prepare Feature Extractor, Tokenizer and Data
4. Training and evaluation
5. Building a demo(optional)

::: aside
- Based on article [Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-whisper)
- [More details on fine tuning Whisper](https://github.com/huggingface/community-events/tree/main/whisper-fine-tuning-event)
:::

## Whisper Event

- HuggingFace Team conducted a whisper fine tuning event for 2 weeks from Decemeber 5th to December 9th.
- The goal was to to fine-tune the Whisper model to build state-of-the-art speech recognition systems in the languages of our choice ðŸ—£

::: aside
[Whisper Event huggingface page](https://huggingface.co/whisper-event)
:::

## Malayalam models in Whisper Event

- For the language Malayalam, the results are as follows:

![Malayalam models performance in whisper event according [to leaderboard](https://huggingface.co/spaces/whisper-event/leaderboard?dataset=mozilla-foundation%2Fcommon_voice_11_0&config=ml&split=test)](https://user-images.githubusercontent.com/24592806/222974236-44f047ec-e072-4f6a-b49f-ed88afb02999.png){width=700 fig-align="center"}

## Winning models in Malayalam in Whisper Event

- The winning model for Common voice: `thennal/whisper-medium-ml`
- The winning model for Fleurs: `parambharath/whisper-small-ml`

::: {.notes}
- I was sceptical about the results of the event
- The 10% WER was breath taking. In Malayalam there is not even a single yard stick to
compare.
- Most of evaluation in properietary datasets are not open source. So can't validate results
:::

## I was not convinced

I was sceptical about the winning models becuase of:

1. Achieving 10% WER in Malayalam is astonishing.
2. In Malayalam there is not even a single yard stick to compare. Most of
previous works were done in proprietary datasets and not open-sourced.
3. Malyalam is [incredibly morpohologically complex language](https://kavyamanohar.com/documents/tsd_morph_complexity_ml.pdf). So even achieving 30% WER
is a big deal.

## I was not convinced {.scrollable}

4. Didn't trust the Hugging Face way of evaluating models.

![thennal/whisper-medium-ml model card readme](fossasia2023/thennal_model_card.png)

![Last commit in thennal/whisper-medium-ml](fossasia2023/thennal_commit.png)

## I wanted to build something new

- New github project for [Malayalam ASR Benchmarking](https://github.com/kurianbenoy/malayalam_asr_benchmarking)

![Time for a new adventure](fossasia2023/adventure_talk.jpg)


::: {.notes}
In Malayalam, the winners were:

- CommonVoice 11.0 - thennal/whisper-medium-ml
- Fleurs - parambharath/whisper-small-ml
:::

##

{{< tweet kurianbenoy2 1633647128059969536 >}}

##

{{< tweet kavya_manohar 1633755478818959372 >}}

## Results on Benchmarking in Malayalam

- Started with 6 fine-tuned models in Malayalam

1. thennal/whisper-medium-ml
2. parambharat/whisper-tiny-ml
3. parambharat/whisper-base-ml
4. parambharat/whisper-small-ml
5. anuragshas/whisper-large-v2-ml
6. DrishtiSharma/whisper-large-v2-malayalam

- Compared it with [6 model versions released by OpenAI](https://github.com/openai/whisper).

## Results on benechmarking in Common Voice dataset

![Output from benchmarking tool](https://user-images.githubusercontent.com/24592806/230587218-96f1c95b-abaf-4c09-866c-0538d7105239.png)

## Results in Common Voice dataset

![WER in Common Voice-9 test split](https://user-images.githubusercontent.com/24592806/230526272-cadd5443-5316-40e1-a356-46c993cb174d.png)

::: {.notes}
Thennal model got the best results:

11.560000 WER, then rest of models got 21-40+ models. 
:::

## CER in Common Voice dataset

![CER in Common Voice-9 test split](https://user-images.githubusercontent.com/24592806/230526282-f6018629-1aa0-4c6c-9b9a-66b4f04e3355.png)

## Results on benechmarking in Malayalam Speech Corpus dataset

![Output from benchmarking tool](https://user-images.githubusercontent.com/24592806/230657721-4e98b75b-4641-4047-8d51-5bd098b76fc8.png)

## Results in Malayalam Speech Corpus dataset

![WER in MSC](https://user-images.githubusercontent.com/24592806/230658615-5db73907-764f-42db-8f2c-88f1089ac1ea.png)

::: {.notes}
Thennal model got the best results:

11.560000 WER, then rest of models got 21-40+ models. 
:::

## CER in Malayalam Speech Corpus

![Character Error rate in MSC](https://user-images.githubusercontent.com/24592806/230658625-d3a9541c-facf-4f4c-83f8-f93ffa4c5c7a.png)

::: {.notes}
in WER Malayalam CommonVoice9 dataset:

tiny - 102.7
base - 122.9
small - 104.8
medium - 137.8
large - 107.1
largev2 - 103.2
:::

## Conclusion

- In Malayalam we have achieved phenomenal results for fine tuned whisper models.
- We seems to have build really good ASR suitable for production use-cases.
- You can also do it in your own language especially if it is a low resource language.

::: {.notes}
You maybe able to achieve SOTA in your own language with Whisper
:::

## Thanks to

1. [OpenAI team](https://openai.com/) - Alec Radford, [Jong Wook Kim](https://github.com/jongwook), [Christine McLeavey](https://www.linkedin.com/in/mcleavey) etc. other authors of [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)
2. [HuggingFace team](https://huggingface.co/) - [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi), [Nicolas Patry](https://huggingface.co/Narsil), [Vaibhav Srivastav](https://github.com/Vaibhavs10) etc.
3. [Kavya Manohar](https://kavyamanohar.com/) and other members in [Swathanthra Malayalam Computing community](https://smc.org.in/).
4. [AbdulMajedRaja RS](https://www.youtube.com/@1littlecoder)
5. [Georgi Gerganov](https://github.com/ggerganov)
6. [Ramsri Goutham](https://ramsri.ai/)
7. [Jarvis Labs](cloud.jarvislabs.ai/)


## Tributes to Areeb Jamal

![Source: [Areeb Jamal Forever in our Hearts and our Memory](https://blog.fossasia.org/areeb-jamal-forever-in-our-hearts-and-our-memory/) - FOSSASIA blog](https://i0.wp.com/blog.fossasia.org/wp-content/uploads/2021/04/lastslide.png?w=1920&ssl=1)
