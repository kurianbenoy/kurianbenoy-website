<!DOCTYPE html>
<html lang="en"><head>
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Kurian Benoy">
  <title>Kurian Benoy ‚Äì OpenAI Whisper and it‚Äôs amazing power to do fine-tuning demonstrated on my mother-tongue</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-534cd8e3a96973385dffff3f4709048d.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="OpenAI Whisper and it‚Äôs amazing power to do fine-tuning demonstrated on my mother-tongue ‚Äì Kurian Benoy">
<meta property="og:description" content="üìç FOSS Meetup, Kochi @ KeyValue, Smart Kochi.">
<meta property="og:site_name" content="Kurian Benoy">
<meta name="twitter:title" content="OpenAI Whisper and it‚Äôs amazing power to do fine-tuning demonstrated on my mother-tongue ‚Äì Kurian Benoy">
<meta name="twitter:description" content="üìç FOSS Meetup, Kochi @ KeyValue, Smart Kochi.">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">OpenAI Whisper and it‚Äôs amazing power to do fine-tuning demonstrated on my mother-tongue</h1>
  <p class="subtitle">üìç FOSS Meetup, Kochi @ KeyValue, Smart Kochi.</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Kurian Benoy 
</div>
</div>
</div>

  <p class="date">Saturday, June 24, 2023</p>
</section>
<section id="outline" class="slide level2">
<h2>Outline</h2>
<ul>
<li>What is OpenAI Whisper?</li>
<li>Features of OpenAI Whisper <!-- - History of models before and after OpenAI Whisper (5 minutes) --></li>
<li>What is Fine-tuning and how to fine-tune Whisper?</li>
<li>About my mother tongue</li>
<li>Methodology of benchmarking whisper models</li>
<li>Results on benchmarking Whisper model</li>
<li>Future Ideas &amp; Conclusion</li>
</ul>
</section>
<section id="whoami" class="slide level2">
<h2>$whoami</h2>
<ul>
<li>AI Engineer &amp; Team Lead @ sentient.io</li>
<li>Volunteer @ Swathanthra Malayalam Computing(SMC)</li>
<li>FOSS enthusiast</li>
<li>Not affiliated to OpenAI</li>
</ul>
</section>
<section id="disclaimer" class="slide level2">
<h2>Disclaimer</h2>
<ul>
<li><span style="color:red">This talk is not generated.</span></li>
<li><span style="color:blue">If I use something generated I will explicitly mark as from an LLM.</span></li>
</ul>
</section>
<section id="openai-whisper" class="slide level2">
<h2>OpenAI Whisper</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/1024px-OpenAI_Logo.svg.png" class="quarto-figure quarto-figure-center" width="500"></p>
</figure>
</div>
<ul>
<li><span style="color:red">I think Whisper<sup>1</sup> is the most <code>under-rated model</code> released by OpenAI.</span></li>
<li><span style="color:green">It was open-sourced on September 21, 2022 by releasing the inference code and pre-trained model weights.</span></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p><span style="color:black">According to <a href="https://cdn.openai.com/papers/whisper.pdf">research paper</a> p.2, the name Whisper is an abbrevation for WSPR: <code>Web-scale Supervised Pretraining for Speech Recognition</code>.</span></p></li></ol></aside></section>
<section id="about-openai-whisper-model" class="slide level2">
<h2>About OpenAI Whisper Model</h2>
<ul>
<li>Whisper is a computer program which can listen to people talking and write down what they say. <span style="color:red">(Automatic Speech Recognition Model)</span></li>
<li>Whisper can understand people speaking different languages and can even translate what they say into English. <span style="color:green">(Supports transcription and translation to English)</span></li>
</ul>

<aside><div>
<p><span style="color:black">Note: This was generated with GPT-4 with prompt: explain what is openai whisper to a 5 year old kid?</span></p>
</div></aside></section>
<section id="whisper-models" class="slide level2 scrollable">
<h2>Whisper Models</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">Size</th>
<th style="text-align: right;">Parameters</th>
<th style="text-align: center;">Required VRAM</th>
<th style="text-align: center;">Relative speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">tiny</td>
<td style="text-align: right;">39 M</td>
<td style="text-align: center;">~1 GB</td>
<td style="text-align: center;">~32x</td>
</tr>
<tr class="even">
<td style="text-align: right;">base</td>
<td style="text-align: right;">74 M</td>
<td style="text-align: center;">~1 GB</td>
<td style="text-align: center;">~16x</td>
</tr>
<tr class="odd">
<td style="text-align: right;">small</td>
<td style="text-align: right;">244 M</td>
<td style="text-align: center;">~2 GB</td>
<td style="text-align: center;">~6x</td>
</tr>
<tr class="even">
<td style="text-align: right;">medium</td>
<td style="text-align: right;">769 M</td>
<td style="text-align: center;">~5 GB</td>
<td style="text-align: center;">~2x</td>
</tr>
<tr class="odd">
<td style="text-align: right;">large</td>
<td style="text-align: right;">1550 M</td>
<td style="text-align: center;">~10 GB</td>
<td style="text-align: center;">1x</td>
</tr>
</tbody>
</table>
</section>
<section id="english-speech-recognition" class="slide level2">
<h2>English Speech Recognition</h2>

<img data-src="../fossasia2023/Whisper_english.png" class="r-stretch quarto-figure-center"><p class="caption">Whisper is competitive with state of art commercial and open source systems. Diagram from <a href="[research paper](https://cdn.openai.com/papers/whisper.pdf)">whisper research paper</a> p.9</p></section>
<section id="multi-lingual-speech-recognition" class="slide level2">
<h2>Multi-lingual Speech recognition</h2>
<ul>
<li><span style="color:red">Whisper model is trained on 99 languages</span></li>
<li><span style="color:green">OpenAI Whisper API supports just 57 languages as some languages performance are not really good.</span></li>
</ul>

<aside><div>
<ul>
<li>More details can be found in Section 3.4 of <a href="https://cdn.openai.com/papers/whisper.pdf">Whisper research paper</a> pp.6 - 8.</li>
<li>Zero-shot Whisper improves performance on Multilingual LibriSpeech(MLS) but is still significantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli. (<a href="https://cdn.openai.com/papers/whisper.pdf">Whisper research paper</a> p.7)</li>
</ul>
</div></aside></section>
<section id="runs-in-almost-any-device" class="slide level2">
<h2>Runs in almost any device</h2>
<ul>
<li><span style="color:red">Since Whisper followed the open source route, <a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a> developed by <a href="https://github.com/ggerganov">Georgi Gerganov</a> which is a port of OpenAI‚Äôs Whisper model in C/C++.</span></li>
</ul>
<p><span style="color:orange"> - It supports the below platforms:</span></p>
<ol type="1">
<li>Mac OS (Intel and ARM)</li>
<li>iOS</li>
<li>Android</li>
<li>Linux/Free BSD</li>
<li>Web Assembly etc. </li>
</ol>
</section>
<section id="awesome-community-plugins" class="slide level2">
<h2>Awesome community plugins</h2>
<ul>
<li>Word-level time stamps with <a href="https://github.com/linto-ai/whisper-timestamped">whisper-timestamped</a>,<a href="https://github.com/m-bain/whisperX">whisperX</a> etc.</li>
<li>Fine-Tune Whisper is achieving SOTA in lot of languages</li>
<li><a href="https://huggingface.co/spaces/dwarkesh/whisper-speaker-recognition">Speaker diarization</a></li>
<li><a href="https://github.com/jumon/zac">Audio classification using OpenAI‚Äôs Whisper</a></li>
<li><a href="https://github.com/guillaumekln/faster-whisper">4x faster with same accuracy using faster-whisper</a></li>
</ul>

<aside><div>
<p><a href="https://ramsrigoutham.medium.com/openais-whisper-7-must-know-libraries-and-add-ons-built-on-top-of-it-10825bd08f76">Thanks to Ramsri Goutham article on 7 must know libraries and addons on top of Whisper</a></p>
</div></aside></section>
<section id="what-is-fine-tuning" class="slide level2">
<h2>What is fine tuning?</h2>
<p><span style="color:red">Given a pre-trained model, which is a large model which is trained on a very specific task. If we want to fit it into our specific dataset we will train and use the pre-trained model to build a new model which works very well for our task.</span></p>

<img data-src="../fossasia2023/fine_tune.png" width="300" class="r-stretch quarto-figure-center"><p class="caption">Picture from <a href="course.fast.ai/">fast.lesson</a> covering steps in finetuning a text classifier model</p></section>
<section id="fine-tuning-is-still-relevant" class="slide level2">
<h2>Fine tuning is still relevant</h2>
<p></p><div id="tweet-33340"></div><script>tweet={"url":"https:\/\/twitter.com\/waydegilliam\/status\/1641228571611123712","author_name":"Wayde Gilliam","author_url":"https:\/\/twitter.com\/waydegilliam","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E‚ÄúFine-tuning is the new training‚Äù should sound familiar to any \u003Ca href=\"https:\/\/twitter.com\/fastdotai?ref_src=twsrc%5Etfw\"\u003E@fastdotai\u003C\/a\u003E folks, we‚Äôll, since forever.  Agreed. \u003Ca href=\"https:\/\/t.co\/FuqbSB8fNr\"\u003Ehttps:\/\/t.co\/FuqbSB8fNr\u003C\/a\u003E\u003C\/p\u003E&mdash; Wayde Gilliam (@waydegilliam) \u003Ca href=\"https:\/\/twitter.com\/waydegilliam\/status\/1641228571611123712?ref_src=twsrc%5Etfw\"\u003EMarch 29, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-33340").innerHTML = tweet["html"];</script><p></p>
</section>
<section id="what-are-steps-for-fine-tuning-whisper" class="slide level2">
<h2>What are steps for fine-tuning Whisper?</h2>

<img data-src="../fossasia2023/blogpost_image.png" width="700" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://huggingface.co/blog/fine-tune-whisper">Fine-Tune Whisper For Multilingual ASR with ü§ó Transformers</a></p></section>
<section id="what-are-steps-for-fine-tuning-whisper-1" class="slide level2">
<h2>What are steps for fine-tuning Whisper?</h2>
<ol type="1">
<li>Preparing Environment</li>
<li>Load dataset</li>
<li>Prepare Feature Extractor, Tokenizer and Data</li>
<li>Training and evaluation</li>
<li>Building a demo(optional)</li>
</ol>

<aside><div>
<ul>
<li>Based on article <a href="https://huggingface.co/blog/fine-tune-whisper">Fine-Tune Whisper For Multilingual ASR with ü§ó Transformers</a></li>
<li><a href="https://github.com/huggingface/community-events/tree/main/whisper-fine-tuning-event">More details on fine tuning Whisper</a></li>
</ul>
</div></aside></section>
<section id="about-malayalam" class="slide level2">
<h2>About Malayalam</h2>
<ul>
<li><span style="color:red">Malayalam is my mother tongue.</span></li>
<li><span style="color:blue">Native speakers: 38+ million.(according to 2011 census)</span></li>
<li><span style="color:green">Spoken in: Kerala, Lakshadweep, Puducherry, wherever Malayalees are living.</span></li>
</ul>
</section>
<section id="malayalam-is-morphologically-complex-language" class="slide level2">
<h2>Malayalam is morphologically complex language</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="../delft-fastai/morphology_ttr.png" width="800"></p>
<figcaption>Comparison of Malayalam TTR with that of European Union Constitution Corpus and DoE-CIIL Corpus from K. Manohar et al.</figcaption>
</figure>
</div>

<aside><div>
<p><span style="color:black">Picture from <a href="https://kavyamanohar.com/documents/tsd_morph_complexity_ml.pdf">Quantitative Analysis of the Morphological Complexity of Malayalam Language by K. Manohar et al</a>. For more information check this paper.</span></p>
</div></aside></section>
<section id="whisper-event" class="slide level2">
<h2>Whisper Event</h2>
<ul>
<li><span style="color:red">HuggingFace Team conducted a whisper fine tuning event for 2 weeks from 5th December 2022 to 19th December 2022. The results were out on 23rd December 2022.</span></li>
<li><span style="color:blue">The goal was to to fine-tune the Whisper model to build state-of-the-art speech recognition systems in the languages of our choice üó£ </span></li>
</ul>

<aside><div>
<p><a href="https://huggingface.co/whisper-event">Whisper Event huggingface page</a></p>
</div></aside></section>
<section id="malayalam-performance-in-whisper-paper" class="slide level2">
<h2>Malayalam performance in whisper paper</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Model</th>
<th>WER</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tiny</td>
<td>102.7</td>
</tr>
<tr class="even">
<td>base</td>
<td>122.9</td>
</tr>
<tr class="odd">
<td>small</td>
<td>104.8</td>
</tr>
<tr class="even">
<td>medium</td>
<td>137.8</td>
</tr>
<tr class="odd">
<td>large-v1</td>
<td>107.1</td>
</tr>
<tr class="even">
<td>large-v2</td>
<td>103.2</td>
</tr>
</tbody>
</table>

<aside><div>
<p><span style="color:black">Appendix D2.2.2 CommonVoice9 dataset results (<a href="https://cdn.openai.com/papers/whisper.pdf">Whisper research paper</a> p.23).</span></p>
</div></aside></section>
<section id="malayalam-models-produced-in-whisper-event" class="slide level2">
<h2>Malayalam models produced in Whisper Event</h2>
<ul>
<li><span style="color:red">For the language Malayalam, the results are as follows:</span></li>
</ul>

<img data-src="../iiit-kottayam-summit/malayalam_models.png" width="700" class="r-stretch quarto-figure-center"><p class="caption"><span style="color:black">Malayalam models performance in whisper event according <a href="https://huggingface.co/spaces/whisper-event/leaderboard?dataset=mozilla-foundation%2Fcommon_voice_11_0&amp;config=ml&amp;split=test">to leaderboard</a></span></p></section>
<section id="winning-models-in-malayalam-in-whisper-event" class="slide level2 nonincremental">
<h2>Winning models in Malayalam in Whisper Event</h2>
<ul>
<li><span style="color:red">The winning model for Common voice</span>: <code>thennal/whisper-medium-ml</code></li>
<li><span style="color:blue">The winning model for Fleurs</span>: <code>parambharath/whisper-small-ml</code></li>
</ul>
</section>
<section id="i-was-not-convinced" class="slide level2">
<h2>I was not convinced</h2>
<p>I was sceptical about the winning models becuase of:</p>
<ol type="1">
<li>Achieving 11% WER in Malayalam is astonishing.</li>
<li>In Malayalam there is not even a single yard stick to compare. Most of previous works were done in proprietary datasets and not open-sourced.</li>
<li>Malyalam is a <a href="https://kavyamanohar.com/documents/tsd_morph_complexity_ml.pdf">morpohologically complex language</a>. So even achieving 30% WER is a big deal.</li>
</ol>
</section>
<section id="i-was-not-convinced-1" class="slide level2 scrollable">
<h2>I was not convinced</h2>
<ol start="4" type="1">
<li>Didn‚Äôt trust the Hugging Face way of evaluating models.</li>
</ol>

<img data-src="../fossasia2023/thennal_model_card.png" class="r-stretch quarto-figure-center"><p class="caption">thennal/whisper-medium-ml model card readme</p></section>
<section id="i-was-not-convinced-2" class="slide level2 scrollable">
<h2>I was not convinced</h2>
<ol start="4" type="1">
<li>Didn‚Äôt trust the Hugging Face way of evaluating models.</li>
</ol>

<img data-src="../fossasia2023/thennal_commit.png" class="r-stretch quarto-figure-center"><p class="caption">Last commit in thennal/whisper-medium-ml</p></section>
<section id="objective-of-my-benchmarking" class="slide level2">
<h2>Objective of my benchmarking</h2>
<ul>
<li><span style="color:red">To test whether 10% WER was possible in available academic datasets.</span></li>
</ul>
<p><strong>Datasets</strong></p>
<ul>
<li><span style="color:blue">Common Voice 11 malayalam subset</span></li>
<li><span style="color:blue">SMC Malayalam Speech Corpus</span></li>
</ul>
</section>
<section id="metrics-for-evaluating-asr-models" class="slide level2">
<h2>Metrics for evaluating ASR models</h2>
<ul>
<li>ASR evaulation relies on comparission between <span style="color:red">ground-truth</span> and <span style="color:red">ASR output</span>.</li>
<li><span style="color:blue">Common metrics for ASR evaluation which are popular and good enough<sup>1</sup> are</span> :</li>
</ul>
<p><span style="color:green">1. Word Error Rate(WER)</span></p>
<p><span style="color:green">2. Character Error Rate(CER)</span></p>

<aside><div>
<p><span style="color:black">To learn more about ASR evaluation check this <a href="https://aws.amazon.com/blogs/machine-learning/evaluating-an-automatic-speech-recognition-service/">blogpost by AWS</a></span></p>
</div><ol class="aside-footnotes"><li id="fn2"><p><span style="color:black">According to Rethinking Evaluation in ASR: Are our models robust enough? by Likhomanenko T, Xu, Q., Pratap etc.</span></p></li></ol></aside></section>
<section id="methodology-for-benchmarking" class="slide level2">
<h2>Methodology for benchmarking</h2>
<ol type="1">
<li><span style="color:red">Create as a python library so further whisper-based transformer models can be benchmark.</span></li>
<li><span style="color:blue">Calculate WER, CER, model size and time taken to benchmark the model for the listed datasets.</span></li>
<li><span style="color:green">Build a reproducible approach, so results of benchmarking is stored as dataset.</span></li>
</ol>
</section>
<section id="i-wanted-to-build-something-new" class="slide level2">
<h2>I wanted to build something new</h2>
<ul>
<li><span style="color:red">New github project for <a href="https://github.com/kurianbenoy/malayalam_asr_benchmarking">Malayalam ASR Benchmarking</a></span></li>
</ul>

<img data-src="../fossasia2023/adventure_talk.jpg" class="r-stretch quarto-figure-center"><p class="caption">Time for a new adventure</p></section>
<section id="libraries-i-used-for-benchmarking" class="slide level2">
<h2>Libraries I used for benchmarking</h2>
<ul>
<li><p>Dependencies:</p>
<ul>
<li>transformers</li>
<li>datasets</li>
<li>jiwer</li>
<li>whisper_normalizer</li>
<li><code>numerize pandas librosa soundfile</code></li>
</ul></li>
<li><p>Development library:</p>
<ul>
<li>nbdev</li>
<li>Jupyter Lab</li>
</ul></li>
</ul>
</section>
<section id="loading-the-dataset-for-benchmarking" class="slide level2">
<h2>Loading the dataset for benchmarking</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-code-line-numbers="2-6|7-9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">def</span> load_common_voice_malayalam_dataset():</span>
<span id="cb1-2"><a></a>    dataset <span class="op">=</span> load_dataset(</span>
<span id="cb1-3"><a></a>            <span class="st">"mozilla-foundation/common_voice_11_0"</span>,</span>
<span id="cb1-4"><a></a>            <span class="st">"ml"</span>,</span>
<span id="cb1-5"><a></a>            split<span class="op">=</span><span class="st">"test"</span></span>
<span id="cb1-6"><a></a>    )</span>
<span id="cb1-7"><a></a>    dataset <span class="op">=</span> dataset.cast_column(<span class="st">"audio"</span>, Audio(sampling_rate<span class="op">=</span><span class="dv">16000</span>))</span>
<span id="cb1-8"><a></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(normalise)</span>
<span id="cb1-9"><a></a>    dataset <span class="op">=</span> dataset.<span class="bu">filter</span>(is_target_text_in_range, input_columns<span class="op">=</span>[<span class="st">"norm_text"</span>])</span>
<span id="cb1-10"><a></a>    <span class="cf">return</span> dataset</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="benchmarking-a-particular-model-weight-in-common-voice" class="slide level2">
<h2>Benchmarking a particular model weight in common voice</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>evaluate_whisper_model_common_voice(</span>
<span id="cb2-2"><a></a>    <span class="st">"openai/whisper-large"</span>,</span>
<span id="cb2-3"><a></a>    [], [], [], []</span>
<span id="cb2-4"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="evaluating-benchmarking-code" class="slide level2">
<h2>Evaluating benchmarking code</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" data-code-line-numbers="2|9-11|12|17-19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="kw">def</span> evaluate_whisper_model_common_voice(</span>
<span id="cb3-2"><a></a>        model_name: <span class="bu">str</span>, <span class="co"># The model name</span></span>
<span id="cb3-3"><a></a>        werlist: List[<span class="bu">float</span>], <span class="co"># WER List</span></span>
<span id="cb3-4"><a></a>        cerlist: List[<span class="bu">float</span>],<span class="co"># CER list</span></span>
<span id="cb3-5"><a></a>        modelsizelist: List[<span class="bu">str</span>], <span class="co"># model size list</span></span>
<span id="cb3-6"><a></a>        timelist: List[<span class="bu">float</span>], <span class="co"># time(s) list</span></span>
<span id="cb3-7"><a></a>        bs:<span class="bu">int</span> <span class="op">=</span><span class="dv">16</span>, <span class="co"># batch size. Default value is 16.</span></span>
<span id="cb3-8"><a></a>)<span class="op">-&gt;</span><span class="va">None</span>:</span>
<span id="cb3-9"><a></a>    whisper_asr <span class="op">=</span> pipeline(</span>
<span id="cb3-10"><a></a>            <span class="st">"automatic-speech-recognition"</span>, model<span class="op">=</span>model_name, device<span class="op">=</span><span class="dv">0</span></span>
<span id="cb3-11"><a></a>        )</span>
<span id="cb3-12"><a></a>    dataset <span class="op">=</span> load_common_voice_malayalam_dataset()</span>
<span id="cb3-13"><a></a>    </span>
<span id="cb3-14"><a></a>    predictions <span class="op">=</span> []</span>
<span id="cb3-15"><a></a>    references <span class="op">=</span> []</span>
<span id="cb3-16"><a></a>    start <span class="op">=</span> time.time()</span>
<span id="cb3-17"><a></a>    <span class="cf">for</span> out <span class="kw">in</span> whisper_asr(data(dataset), batch_size<span class="op">=</span>bs):</span>
<span id="cb3-18"><a></a>        predictions.append(normalizer((out[<span class="st">"text"</span>])))</span>
<span id="cb3-19"><a></a>        references.append(normalizer(out[<span class="st">"reference"</span>][<span class="dv">0</span>]))</span>
<span id="cb3-20"><a></a>    end <span class="op">=</span> time.time()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="calculating-wer-cer" class="slide level2">
<h2>Calculating WER, CER</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-code-line-numbers="2|4-5|7|12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>    ...</span>
<span id="cb4-2"><a></a>    df <span class="op">=</span> pd.DataFrame({<span class="st">"predictions"</span>: predictions, <span class="st">"ground_truth"</span>: references})</span>
<span id="cb4-3"><a></a>    df[<span class="st">"model_name"</span>] <span class="op">=</span> model_name</span>
<span id="cb4-4"><a></a>    df[<span class="st">"wer"</span>] <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> row: wer(normalizer(row[<span class="st">"ground_truth"</span>]), normalizer(row[<span class="st">"predictions"</span>])), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-5"><a></a>    df[<span class="st">"cer"</span>] <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> row: cer(normalizer(row[<span class="st">"ground_truth"</span>]), normalizer(row[<span class="st">"predictions"</span>])), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-6"><a></a>    df[<span class="st">"total_time"</span>] <span class="op">=</span> end<span class="op">-</span>start</span>
<span id="cb4-7"><a></a>    rwer <span class="op">=</span> wer(references, predictions)</span>
<span id="cb4-8"><a></a>    rwer <span class="op">=</span> <span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> rwer, <span class="dv">2</span>)</span>
<span id="cb4-9"><a></a>    werlist.append(rwer)</span>
<span id="cb4-10"><a></a>    <span class="bu">print</span>(<span class="ss">f"The WER of model: </span><span class="sc">{</span>rwer<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-11"><a></a></span>
<span id="cb4-12"><a></a>    rcer <span class="op">=</span> cer(references, predictions)</span>
<span id="cb4-13"><a></a>    rcer <span class="op">=</span> <span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> rcer, <span class="dv">2</span>)</span>
<span id="cb4-14"><a></a>    cerlist.append(rcer)</span>
<span id="cb4-15"><a></a>    <span class="bu">print</span>(<span class="ss">f"The CER of model: </span><span class="sc">{</span>rcer<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="calculating-model_size-and-storing-as-a-dataset" class="slide level2">
<h2>Calculating model_size and storing as a dataset</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" data-code-line-numbers="2|8|10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>    ...</span>
<span id="cb5-2"><a></a>    <span class="bu">print</span>(<span class="ss">f"The model size is: </span><span class="sc">{</span>get_model_size(whisper_asr.model)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-3"><a></a>    modelsizelist.append(get_model_size(whisper_asr.model))</span>
<span id="cb5-4"><a></a>    df[<span class="st">"model_size"</span>] <span class="op">=</span> get_model_size(whisper_asr.model)</span>
<span id="cb5-5"><a></a>    </span>
<span id="cb5-6"><a></a>    save_name <span class="op">=</span> model_name.split(<span class="st">"/"</span>)</span>
<span id="cb5-7"><a></a>    <span class="bu">print</span>(save_name)</span>
<span id="cb5-8"><a></a>    df.to_parquet(<span class="ss">f"</span><span class="sc">{</span>save_name[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>save_name[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">_commonvoice.parquet"</span>)</span>
<span id="cb5-9"><a></a>    </span>
<span id="cb5-10"><a></a>    clear_gpu_memory()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="benchmarked-models" class="slide level2">
<h2>Benchmarked models</h2>
<ul>
<li><p><span style="color:red">Started with 6 fine-tuned models in Malayalam and compared it with <a href="https://github.com/openai/whisper">6 model versions released by OpenAI</a>.</span></p>
<ol type="1">
<li>thennal/whisper-medium-ml</li>
<li>parambharat/whisper-tiny-ml</li>
<li>parambharat/whisper-base-ml</li>
<li>parambharat/whisper-small-ml</li>
<li>anuragshas/whisper-large-v2-ml</li>
<li>DrishtiSharma/whisper-large-v2-malayalam</li>
</ol></li>
</ul>
</section>
<section id="results-on-benechmarking-in-common-voice-dataset" class="slide level2">
<h2>Results on benechmarking in Common Voice dataset</h2>

<img data-src="../iiit-kottayam-summit/cv_output.png" class="r-stretch quarto-figure-center"><p class="caption">Output from benchmarking tool</p></section>
<section id="wer-in-common-voice-dataset" class="slide level2">
<h2>WER in Common Voice dataset</h2>

<img data-src="../iiit-kottayam-summit/cv_wer.png" class="r-stretch quarto-figure-center"><p class="caption">Word Error Rate in Common Voice-9 test split</p></section>
<section id="cer-in-common-voice-dataset" class="slide level2">
<h2>CER in Common Voice dataset</h2>

<img data-src="../iiit-kottayam-summit/cv_cer.png" class="r-stretch quarto-figure-center"><p class="caption">Character Error Rate in Common Voice-9 test split</p></section>
<section id="section" class="slide level2">
<h2></h2>
<p></p><div id="tweet-94467"></div><script>tweet={"url":"https:\/\/twitter.com\/kurianbenoy2\/status\/1633647128059969536","author_name":"Kurian Benoy","author_url":"https:\/\/twitter.com\/kurianbenoy2","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EI have been working on benchmarking a few Malayalam ASR models. The results have been far better than expected. Check out the results of Malayalam based fine tuned Whisper models performance in Common voice dataset:\u003Ca href=\"https:\/\/t.co\/ajoYq6FBpT\"\u003Ehttps:\/\/t.co\/ajoYq6FBpT\u003C\/a\u003E\u003C\/p\u003E&mdash; Kurian Benoy (@kurianbenoy2) \u003Ca href=\"https:\/\/twitter.com\/kurianbenoy2\/status\/1633647128059969536?ref_src=twsrc%5Etfw\"\u003EMarch 9, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-94467").innerHTML = tweet["html"];</script><p></p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p></p><div id="tweet-55850"></div><script>tweet={"url":"https:\/\/twitter.com\/kavya_manohar\/status\/1633755478818959372","author_name":"Kavya Manohar (‡¥ï‡¥æ‡¥µ‡µç‡¥Ø)","author_url":"https:\/\/twitter.com\/kavya_manohar","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EThere has never been a benchmark for comparing Malayalam ASR models. Thank you \u003Ca href=\"https:\/\/twitter.com\/kurianbenoy2?ref_src=twsrc%5Etfw\"\u003E@kurianbenoy2\u003C\/a\u003E \u003Ca href=\"https:\/\/t.co\/Ay84OGzxWn\"\u003Ehttps:\/\/t.co\/Ay84OGzxWn\u003C\/a\u003E\u003C\/p\u003E&mdash; Kavya Manohar (‡¥ï‡¥æ‡¥µ‡µç‡¥Ø) (@kavya_manohar) \u003Ca href=\"https:\/\/twitter.com\/kavya_manohar\/status\/1633755478818959372?ref_src=twsrc%5Etfw\"\u003EMarch 9, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-55850").innerHTML = tweet["html"];</script><p></p>
</section>
<section id="results-on-benechmarking-in-malayalam-speech-corpus-dataset" class="slide level2">
<h2>Results on benechmarking in Malayalam Speech Corpus dataset</h2>

<img data-src="../iiit-kottayam-summit/msc_output.png" class="r-stretch quarto-figure-center"><p class="caption">Output from benchmarking tool</p></section>
<section id="wer-in-malayalam-speech-corpus" class="slide level2">
<h2>WER in Malayalam Speech Corpus</h2>

<img data-src="../iiit-kottayam-summit/msc_wer.png" class="r-stretch quarto-figure-center"><p class="caption">Word Error Rate in MSC</p></section>
<section id="cer-in-malayalam-speech-corpus" class="slide level2">
<h2>CER in Malayalam Speech Corpus</h2>

<img data-src="../iiit-kottayam-summit/msc_cer.png" class="r-stretch quarto-figure-center"><p class="caption">Character Error rate in MSC</p></section>
<section id="links-to-project" class="slide level2">
<h2>Links to Project</h2>
<p><span style="color:red">Github project</span></p>
<p><span style="color:black">https://github.com/kurianbenoy/malayalam_asr_benchmarking</span></p>
</section>
<section id="links-to-project-1" class="slide level2">
<h2>Links to Project</h2>
<p><span style="color:blue">Benchmarking results</span></p>
<ul>
<li><p><span style="color:orange">Results on SMC Malayalam Speech corpus</span></p>
<p>https://huggingface.co/datasets/kurianbenoy/</p>
<p>malayalam_msc_benchmarking</p></li>
<li><p><span style="color:orange">Results on Common Voice 11</span></p>
<p>https://huggingface.co/datasets/kurianbenoy/</p>
<p>malayalam_common_voice_benchmarking</p></li>
</ul>
</section>
<section id="future-ideas-for-benchmarking" class="slide level2">
<h2>Future Ideas for Benchmarking</h2>
<ul>
<li><span style="color:red">Something very similar to OpenLLM Leaderboard with results of latest malayalam speech models.</span></li>
<li><span style="color:blue">Should include results for Kaldi, Meta‚Äôs MMS, Wav2Vec etc.</span></li>
</ul>

<img data-src="../delft-fastai/openllm.png" class="r-stretch quarto-figure-center"><p class="caption">Open LLM leaderboard in <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">huggingface spaces</a></p></section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li class="fragment"><span style="color:red">In Malayalam we have achieved phenomenal results for fine tuned whisper models.</span></li>
<li class="fragment"><span style="color:blue">The best model after benchmarking is: <code>thennal/whisper-medium-ml</code></span></li>
<li class="fragment"><span style="color:green">I think their seems to be a good ASR model suitable for production use-cases.</span></li>
<li class="fragment"><span style="color:orange">You can also do it in your own language especially if it is a low resource language.</span></li>
</ul>
</section>
<section id="thanks-to" class="slide level2">
<h2>Thanks to</h2>
<div class="columns">
<div class="column" style="width:70%;">
<ol type="1">
<li><a href="https://openai.com/">OpenAI team</a> - Alec Radford, <a href="https://github.com/jongwook">Jong Wook Kim</a>, <a href="https://www.linkedin.com/in/mcleavey">Christine McLeavey</a> etc. other authors of <a href="https://cdn.openai.com/papers/whisper.pdf">Whisper paper</a></li>
<li>Creators of <a href="https://opennmt.net/CTranslate2">CTranslate2</a> and <a href="https://github.com/guillaumekln/faster-whisper">faster-whisper</a> - <a href="https://github.com/guillaumekln">Guillaume Klein</a></li>
<li><a href="https://huggingface.co/">HuggingFace team</a> - <a href="https://huggingface.co/sanchit-gandhi">Sanchit Gandhi</a>, <a href="https://huggingface.co/Narsil">Nicolas Patry</a>, <a href="https://github.com/Vaibhavs10">Vaibhav Srivastav</a> etc.</li>
<li><a href="https://kavyamanohar.com/">Kavya Manohar</a></li>
<li><a href="https://thottingal.in/">Santhosh Thottingal</a></li>
<li><a href="https://huggingface.co/thennal">Thennal D K</a></li>
</ol>
</div><div class="column" style="width:30%;">
<ol start="7" type="1">
<li><a href="https://www.youtube.com/@1littlecoder">AbdulMajedRaja RS</a></li>
<li><a href="https://github.com/ggerganov">Georgi Gerganov</a></li>
<li><a href="https://ramsri.ai/">Ramsri Goutham</a></li>
<li><a href="https://twitter.com/waydegilliam">Wayde Gilliam</a></li>
<li>Other members in <a href="https://smc.org.in/">SMC</a>.</li>
<li><a href="cloud.jarvislabs.ai/">Jarvis Labs</a></li>
</ol>
</div></div>


</section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://kurianbenoy.com/">Kurian Benoy</a> || OpenAI Whisper and it‚Äôs amazing power to do fine-tuning demonstrated on my mother-tongue</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="../../site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'multiplex': {"secret":null,"id":"330a7170843419c5","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/kurianbenoy\.com");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>