[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kurian Benoy",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nðŸ¦Learning more about birds and bird calls with Merlin app\n\n\n0 min\n\n\n\nkaggle\n\n\nexperience\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEverything is about to be changed and launch of GPT-4\n\n\n2 min\n\n\n\nDeep learning\n\n\nNLP\n\n\ncoding\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy daily fitness routine\n\n\n1 min\n\n\n\nmyself\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttending MBIFL 2023, a literature festival\n\n\n3 min\n\n\n\nLife\n\n\nTravel\n\n\nconference\n\n\n\n\n\n\n\nKurian Benoy\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Quarto Website\n\n\n0 min\n\n\n\nTIL\n\n\ncoding\n\n\nWebdev\n\n\n\n\n\n\n\nTristan Oâ€™Malley, Kurian Benoy\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow well does CLIP models classify corn seeds?\n\n\n1 min\n\n\n\ncoding\n\n\nDeep learning\n\n\nexperience\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Paddy Disease Classification competition\n\n\n0 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)\n\n\n2 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nJul 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA strange bug when using fastai library with Weights & Biases\n\n\n1 min\n\n\n\nfastai\n\n\n\nWhen I was using fastai library along with weights & biases callback to track my model training, I noticed a strange error when inferencing the same models created withâ€¦\n\n\n\nKurian Benoy\n\n\nJul 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting featured in Spaces of the week and my latest two gradio spaces\n\n\n0 min\n\n\n\nhuggingface\n\n\nDeep learning\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to approach learning Vim - tips from two monks\n\n\n6 min\n\n\n\nopensource\n\n\nfastaicourse\n\n\nterminal\n\n\n\nVim is a powerful text editor and can be used for data wrangling. Two monks shared resources and their thoughts on how to learn this tool.\n\n\n\nKurian Benoy\n\n\nJun 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a baseline model for Malayalam Text Classification\n\n\n3 min\n\n\n\nmalayalamtextmodels\n\n\nmalayalam\n\n\nNLP\n\n\nopensource\n\n\nML\n\n\nDeep learning\n\n\nSMC\n\n\n\nSimple baseline model with Logistic Regression gave an accuracy close to 87-89% validation accuracy in a private dataset.\n\n\n\nKurian Benoy\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting an open-source project - Malayalam Text Classifier\n\n\n4 min\n\n\n\nmalayalamtextmodels\n\n\nmalayalam\n\n\nNLP\n\n\nopensource\n\n\nML\n\n\nDeep learning\n\n\nSMC\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python Packages & setting up libraries for Data Science - the right way\n\n\n6 min\n\n\n\nfastaicourse\n\n\nterminal\n\n\nPython\n\n\nsetup\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly trying out a NLP model for Kaggle Competition\n\n\n0 min\n\n\n\nkaggle\n\n\nfastaicourse\n\n\nNLP\n\n\nhuggingface\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Lesson 4\n\n\n5 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nMay 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Lesson 3\n\n\n3 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nMay 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Lesson 2\n\n\n4 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusic genre classifier using fast.ai\n\n\n3 min\n\n\n\nfastai\n\n\nfastaicourse\n\n\n\nA side-project done while attending Practical Deep Learning for Coders Course\n\n\n\nKurian Benoy\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Lesson 0\n\n\n1 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\nfastai\n\n\n\n\n\n\n\nKurian Benoy\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders Course - Lesson 1\n\n\n4 min\n\n\n\nfastaicourse\n\n\nfastbook\n\n\n\nThis blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland withâ€¦\n\n\n\nKurian Benoy\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a fine-tuned translation system for English-Malayalam\n\n\n1 min\n\n\n\nfastai\n\n\nhuggingface\n\n\ntranslation\n\n\nfine tuning\n\n\nmalayalam\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)\n\n\n1 min\n\n\n\nhuggingface\n\n\nNLP\n\n\nfastai\n\n\n\n\n\n\n\nKurian Benoy\n\n\nFeb 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Vacation Experience\n\n\n1 min\n\n\n\nTravel\n\n\nLife\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConversation with Vue.js Core Developer Ben Hong\n\n\n1 min\n\n\n\nVue\n\n\nVue3\n\n\nInterview\n\n\n\n\n\n\n\n\n\n\nDec 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Tamil and weird things in Malayalam\n\n\n0 min\n\n\n\ntamil\n\n\nLife\n\n\n\n\n\n\n\nKurian Benoy\n\n\nNov 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessy git commit relation with master and develop branches\n\n\n0 min\n\n\n\ngit\n\n\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12 - What is Convolution? Looking into CNNs\n\n\n3 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nSep 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 - Embeddings & Recommender systems, Learning FastBook along with Study Group\n\n\n2 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nAug 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputting String in C programming language\n\n\n2 min\n\n\n\nTIL\n\n\nC\n\n\n\n\n\n\n\n\n\n\nAug 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 And 9 - Learning FastBook along with Study Group\n\n\n6 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nAug 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding get_image_files in fastai\n\n\n8 min\n\n\n\nTIL\n\n\nPython\n\n\nfastai\n\n\nfastbook\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew adventure begins with MTech\n\n\n0 min\n\n\n\nLife\n\n\nmyself\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog7- Learning FastBook along with Study Group\n\n\n1 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJul 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy should NPM packages be as small as possible?\n\n\n1 min\n\n\n\nTIL\n\n\njavascript\n\n\n\n\n\n\n\n\n\n\nJul 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular Expressions in Python\n\n\n1 min\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog6- Learning FastBook along with Study Group\n\n\n5 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJul 18, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nResizing an image to smaller size\n\n\n1 min\n\n\n\nTIL\n\n\nPython\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog5- Learning FastBook along with Study Group\n\n\n2 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJul 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog4- Learning FastBook along with Study Group\n\n\n5 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJul 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to selectively allow multiple URL origins in flask?\n\n\n7 min\n\n\n\ncoding\n\n\nexperience\n\n\nPython\n\n\nFlask\n\n\n\n\n\n\n\n\n\n\nJul 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog3- Learning FastBook along with Study Group\n\n\n5 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJun 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog2- Learning FastBook along with Study Group\n\n\n4 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nMentoring ML students in Tinkerhub\n\n\n0 min\n\n\n\nTinkerhub\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJun 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog1-Learning FastBook along with Study Group\n\n\n3 min\n\n\n\nfastbook\n\n\nmyself\n\n\nML\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog 0- Learning FastBook along with study group\n\n\n1 min\n\n\n\nfastbook\n\n\nmyself\n\n\n\n\n\n\n\n\n\n\nJun 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo no harm - COC for programmers\n\n\n6 min\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nTrying out ngrok\n\n\n1 min\n\n\n\n\n\n\nKurian Benoy\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nNotes on Javascript Asynchronicity\n\n\n4 min\n\n\n\n\n\n\nKurian Benoy\n\n\nMay 17, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n0 min\n\n\n\n\n\n\n\n\n\nMay 11, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nFew Vue concepts\n\n\n0 min\n\n\n\ncoding\n\n\nVue\n\n\njavascript\n\n\nWebdev\n\n\n\n\n\n\n\n\n\n\nApr 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Financial Lessons(not advisory)\n\n\n1 min\n\n\n\nLife\n\n\nfinance\n\n\n\n\n\n\n\n\n\n\nApr 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Issue I had with fetching data with Axios\n\n\n6 min\n\n\n\ncoding\n\n\njavascript\n\n\n\n\n\n\n\n\n\n\nApr 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatic type ecosystem - python types & Typescript\n\n\n2 min\n\n\n\ncoding\n\n\nPython\n\n\njavascript\n\n\n\n\n\n\n\nkurianbenoy\n\n\nApr 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShutdown Routine\n\n\n3 min\n\n\n\nLife\n\n\nwork\n\n\n\n\n\n\n\n\n\n\nMar 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 qualities I learned from great software developers\n\n\n3 min\n\n\n\ncoding\n\n\nLife\n\n\n\n\n\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled - Random thoughts on my Supplies\n\n\n1 min\n\n\n\n\n\n\n\n\n\nMar 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHosting your website ðŸŒ with Github pages\n\n\n4 min\n\n\n\ncoding\n\n\nfrontend\n\n\nWebdev\n\n\n\n\n\n\n\n\n\n\nMar 6, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nðŸ  WFH for 6 months\n\n\n3 min\n\n\n\nLife\n\n\nWFH\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Vue.js Part 2\n\n\n3 min\n\n\n\njavascript\n\n\nfrontend\n\n\ncoding\n\n\nWebdev\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Vue.js Part1\n\n\n3 min\n\n\n\njavascript\n\n\nfrontend\n\n\ncoding\n\n\nWebdev\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nWhat are Parameters and filters in Redash?\n\n\n1 min\n\n\n\nredash\n\n\ncoding\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n3 Python libraries to make your code more pythonic\n\n\n2 min\n\n\n\ncoding\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBooks read in 2021\n\n\n1 min\n\n\n\nbooks\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJan 1, 2021\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nAdvent of Code - Day 1 to Day 5\n\n\n8 min\n\n\n\ncoding\n\n\n\n\n\n\n\nKurian Benoy\n\n\nDec 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Reading like AI\n\n\n0 min\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I lost almost 4Kgs in 3 weeks - my wegiht loss Journey\n\n\n4 min\n\n\n\n\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nVarious Python Distributions\n\n\n2 min\n\n\n\n\n\n\n\n\n\nNov 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Visual Studio Setup\n\n\n1 min\n\n\n\ncoding\n\n\nLife\n\n\nsetup\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nPycon India 2020 Highlights\n\n\n0 min\n\n\n\nconference\n\n\ncoding\n\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nNotable quotes from Mind master - Viswanathan Anand\n\n\n5 min\n\n\n\nbooks\n\n\n\n\n\n\n\nKurian Benoy\n\n\nAug 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwTw-1(July 3 - July 8)\n\n\n1 min\n\n\n\nmyself\n\n\nTWTW\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nCS229 - Lesson Notes\n\n\n1 min\n\n\n\nmyself\n\n\n\n\n\n\n\n\n\n\nJun 9, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBooks read in 2020\n\n\n0 min\n\n\n\nbooks\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nIdeas for ML computing\n\n\n1 min\n\n\n\nMalayalam\n\n\nSMC\n\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColorising old black and white images with Deoldify\n\n\n0 min\n\n\n\ncoding\n\n\nLife\n\n\n\n\n\n\n\n\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBengali AI Competition Experience\n\n\n2 min\n\n\n\ncoding\n\n\nkaggle\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nExploring a new language - Swift(Part 1)\n\n\n3 min\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBooks- January and February\n\n\n1 min\n\n\n\nbooks\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 7, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nReplace one host name with another host name in Javascript\n\n\n0 min\n\n\n\n\n\n\n\n\n\nFeb 26, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nMy 2020 resolutions\n\n\n0 min\n\n\n\n\n\n\nKurian Benoy\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nAttending an Aws meetup\n\n\n1 min\n\n\n\n\n\n\nKurian Benoy\n\n\nJan 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst thoughts on Swift\n\n\n8 min\n\n\n\nSwift\n\n\nDeep learning\n\n\n\n\n\n\n\nKurian Benoy\n\n\nDec 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevsprints Experience\n\n\n2 min\n\n\n\ncommunity\n\n\nfossmec\n\n\n\n\n\n\n\nKurian Benoy\n\n\nNov 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to go beyond Hacktoberfest?\n\n\n0 min\n\n\n\nHacktoberfest\n\n\nopensource\n\n\n\n\n\n\n\nKurian Benoy\n\n\nOct 1, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nPinch of old project\n\n\n1 min\n\n\n\nDeep learning\n\n\n\n\n\n\n\nKurian Benoy\n\n\nSep 29, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nSpeaking style adaption in text-to-speech synthesis usng sequence-to-sequence models with attention\n\n\n4 min\n\n\n\nTTS\n\n\nTactron\n\n\n\n\n\n\n\n\n\n\nSep 23, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBreak the Half-way mark(0.52 LB score)\n\n\n0 min\n\n\n\nkaggle\n\n\n\n\n\n\n\n\n\n\nSep 22, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nLiterature review for my project\n\n\n6 min\n\n\n\nTTS\n\n\nAudio\n\n\n\n\n\n\n\n\n\n\nSep 21, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nFloat/decimal points precision in C++ and python3\n\n\n0 min\n\n\n\nPython\n\n\nC\n\n\n\n\n\n\n\nKurian Benoy\n\n\nSep 17, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBest slack communities for Machine Learning\n\n\n0 min\n\n\n\ncommunity\n\n\n\n\n\n\n\n\n\n\nSep 15, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nResearch paper Summary\n\n\n3 min\n\n\n\nAudio\n\n\nTTS\n\n\n\n\n\n\n\nKurian Benoy\n\n\nSep 11, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nFOSSMEC Report(1/n)\n\n\n2 min\n\n\n\nfossmec\n\n\nopensource\n\n\n\n\n\n\n\nKurian Benoy\n\n\nSep 8, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nWhat is the difference between Open stack and AWS EC2?\n\n\n1 min\n\n\n\nAWS\n\n\nOpenstack\n\n\ncontainers\n\n\n\n\n\n\n\nKurian Benoy\n\n\nSep 3, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nà´†àµ¼à´Ÿàµà´Ÿà´¿à´«à´¿à´·àµà´¯àµ½ à´‡à´¨à´±à´²à´¿à´œàµ»à´¸àµà´‚ à´•à´®àµà´®àµà´¯àµ‚à´£à´¿à´¸àµà´±àµà´±àµ à´ªà´šàµà´šà´¯àµà´‚\n\n\n1 min\n\n\n\nMachine Learning\n\n\nMalayalam\n\n\n\n\n\n\n\nGopikrishnan Sasikumar\n\n\nAug 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning for Japanese Classical Literature-Paper Review\n\n\n8 min\n\n\n\nresearch paper summary\n\n\nacademic\n\n\nDeep learning\n\n\nML\n\n\n\n\n\n\n\nKurian Benoy\n\n\nAug 10, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nFinal Year Project Abstract\n\n\n2 min\n\n\n\nacademic\n\n\n\n\n\n\n\nKurian Benoy\n\n\nAug 7, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nSeminar ideas for project\n\n\n1 min\n\n\n\nmyself\n\n\n\n\n\n\n\n\n\n\nAug 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMalayalam Computing(Highlight from talk by Santhosh Thottingal)\n\n\n1 min\n\n\n\nmalayalam\n\n\nconference\n\n\nSMC\n\n\n\n\n\n\n\n\n\n\nJul 30, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nWhat is DevSprints (MEC.conf)?\n\n\n1 min\n\n\n\nfossmec\n\n\nopensource\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 25, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nEmacs vs VIM war\n\n\n0 min\n\n\n\nEmacsvim\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson-7 Notes\n\n\n2 min\n\n\n\nfastai\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 21, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nThoughts on linux distros\n\n\n0 min\n\n\n\nlinux\n\n\ndgplug\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 19, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBrownie points on Single Shot Multibox Detector\n\n\n2 min\n\n\n\nPython\n\n\nlinux\n\n\nDeep learning\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJul 14, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nJewels from DGPLUG training\n\n\n3 min\n\n\n\nPython\n\n\nlinux\n\n\nLife\n\n\ndgplug\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJun 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotes from the book Alchemist\n\n\n2 min\n\n\n\nbooks\n\n\n\n\n\n\n\nKurian Benoy\n\n\nJun 22, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nMonoliths vs Microservices\n\n\n3 min\n\n\n\nconference\n\n\n\n\n\n\n\nKurianBenoy\n\n\nMay 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOSSASIA Summit - Day 5\n\n\n1 min\n\n\n\nconference\n\n\nfossasia\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOSSASIA Summit - Day 4\n\n\n0 min\n\n\n\nconference\n\n\nfossasia\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOSSASIA Summit - Day 3\n\n\n1 min\n\n\n\nconference\n\n\nfossasia\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOSSASIA Summit - Day 2\n\n\n1 min\n\n\n\nconference\n\n\nfossasia\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOSSASIA Summit - Day 1\n\n\n1 min\n\n\n\nconference\n\n\nfossasia\n\n\nTravel\n\n\n\n\n\n\n\nKurian Benoy\n\n\nMar 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContributing to OpenSource With CloudCV\n\n\n1 min\n\n\n\nopensource\n\n\nexperience\n\n\n\n\n\n\n\nKurianBenoy\n\n\nMar 1, 2019\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nWhat is SVD and NMF? (Learning from Computational Linear Algebra course)\n\n\n2 min\n\n\n\nfastai\n\n\nlinear_algebra\n\n\n\n\n\n\n\nKurian Benoy\n\n\nFeb 11, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#outline",
    "href": "presentations/fossasia2023/index.html#outline",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Outline",
    "text": "Outline\n\nOpenAI Whisper and itâ€™s awesome features\nFine-tuning and how to fine-tune Whisper?\nResults on fine-tuning whisper in Malayalam(my mother tounge)\nConclusion\n\n\nOriginial Idea - OpenAI Whisper(under appreciated model) - 1littecoder video - Why itâ€™s awesome(whisper.cpp, long form transcription, lot of languages, whisper_normalizer) - What is fine tuning? (Jeremy way of explaining) - Fine tuning whisper in my language - Results on fine tuning - You can also achieve SOTA in your language"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#whoami",
    "href": "presentations/fossasia2023/index.html#whoami",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "$whoami",
    "text": "$whoami\n\nML Engineer & Team Lead @ Sentient.io\nVolunteer @ Swathanthra Malayalam Computing(SMC)\nOpen-source enthusiast\nNot affiliated to OpenAI"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#openai-whisper",
    "href": "presentations/fossasia2023/index.html#openai-whisper",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "OpenAI Whisper",
    "text": "OpenAI Whisper\n\n\n\n\n\n\nWhisper is the most under-rated models released by OpenAI.\nIt open-sourced on September 21, 2022 by releasing the inference code and pre-trained model weights.\n\n\n\nAccording to research paper pp.2, the name Whisper is an abbrevation for WSPR:\nWeb-scale Supervised Pretraining for Speech Recognition."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#about-openai-whisper-model",
    "href": "presentations/fossasia2023/index.html#about-openai-whisper-model",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "About OpenAI Whisper Model",
    "text": "About OpenAI Whisper Model\n\nThe model is robust general purpose speech recognition system aka ASR.\nIt is trained on large dataset of 680,000 hours of audio recordings\nHence sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection\n\n\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\nA Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#whisper-models",
    "href": "presentations/fossasia2023/index.html#whisper-models",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Whisper Models",
    "text": "Whisper Models\n\n\n\n\n\n\n\n\n\n\n\nSize\nParameters\nEnglish-only model\nMultilingual model\nRequired VRAM\nRelative speed\n\n\n\n\ntiny\n39 M\ntiny.en\ntiny\n~1 GB\n~32x\n\n\nbase\n74 M\nbase.en\nbase\n~1 GB\n~16x\n\n\nsmall\n244 M\nsmall.en\nsmall\n~2 GB\n~6x\n\n\nmedium\n769 M\nmedium.en\nmedium\n~5 GB\n~2x\n\n\nlarge\n1550 M\nN/A\nlarge\n~10 GB\n1x\n\n\n\n\nBelow are the names of the available models and their approximate memory requirements and relative speed. Large itself has two version: large-v1 and large-v2.\nThe .en models for English-only applications tend to perform better, especially for the tiny.en and base.en models. We observed that the difference becomes less significant for the small.en and medium.en models."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#openai-whisper-features",
    "href": "presentations/fossasia2023/index.html#openai-whisper-features",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "OpenAI Whisper Features",
    "text": "OpenAI Whisper Features\n\n\nEnglish Speech Recognition\nMulti-lingual speech recognition\nSupport for multiple tasks\nCan run in almost any devices with whisper.cpp\nAwesome community plugins"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#english-speech-recognition",
    "href": "presentations/fossasia2023/index.html#english-speech-recognition",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "English Speech Recognition",
    "text": "English Speech Recognition\n\nWhisper is competitive with state of art commercial and open source systems"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#multi-lingual-speech-recognition",
    "href": "presentations/fossasia2023/index.html#multi-lingual-speech-recognition",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Multi-lingual Speech recognition",
    "text": "Multi-lingual Speech recognition\n\nWhisper supports 99 languages\nIt really supports just 57 languages really well though, as these languages are provided in OpenAI Whisper API.\n\n\nAfrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#multiple-tasks",
    "href": "presentations/fossasia2023/index.html#multiple-tasks",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Multiple tasks",
    "text": "Multiple tasks\n\nLike Language recognition\nTranslation of audio from X-> en"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#runs-in-almost-any-device",
    "href": "presentations/fossasia2023/index.html#runs-in-almost-any-device",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Runs in almost any device",
    "text": "Runs in almost any device\n\nSince Whisper followed the open source route, Whisper.cpp developed by Georgi Gerganov which is a port of Port of OpenAIâ€™s Whisper model in C/C++.\nIt supports the below platforms:\n\n\nMac OS (Intel and ARM)\niOS\nAndroid\nLinux/Free BSD\nWeb Assembly\nWindows\nRaspberry Pi"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#awesome-community-plugins",
    "href": "presentations/fossasia2023/index.html#awesome-community-plugins",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Awesome community plugins",
    "text": "Awesome community plugins\n\nWord-level time stamps with whisperX\nFine-Tune Whisper is achieving SOTA in lot of languages\nSpeaker diarization\nAudio classification using OpenAIâ€™s Whisper\n\n\nThanks to Ramsri Goutham"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#what-is-fine-tuning",
    "href": "presentations/fossasia2023/index.html#what-is-fine-tuning",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "What is fine tuning?",
    "text": "What is fine tuning?\nGiven a pre-trained model, which is a large model which is trained on a very specific task. If we want to fit it into our specific dataset we will train and use the pre-trained model to build a new model which works very well for our task.\n\nSteps in finetuning"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#fine-tuning-is-still-relevant",
    "href": "presentations/fossasia2023/index.html#fine-tuning-is-still-relevant",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Fine tuning is still relevant",
    "text": "Fine tuning is still relevant"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#why-fine-tune-whisper",
    "href": "presentations/fossasia2023/index.html#why-fine-tune-whisper",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Why fine-tune Whisper?",
    "text": "Why fine-tune Whisper?\nWhisper is developed as a robust speech recognition system which is intented to work in out of distribution dataset. While most of previous approaches relied on training in a few open-source dataset and using them.\nIf we are working on a language/dataset domain where the out-of distribution Whisper model doesnâ€™t give good result we can use whisper to get good results."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#what-are-steps-for-fine-tuning-whisper",
    "href": "presentations/fossasia2023/index.html#what-are-steps-for-fine-tuning-whisper",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "What are steps for fine-tuning Whisper?",
    "text": "What are steps for fine-tuning Whisper?\nWell explained in this article"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#fine-tuning-whisper-in-malayalam",
    "href": "presentations/fossasia2023/index.html#fine-tuning-whisper-in-malayalam",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Fine tuning whisper in Malayalam",
    "text": "Fine tuning whisper in Malayalam\nThe base-whisper model was trained on Malayalam:\n\nbut just half an hour dataset was used to train the model\n\n\nThanks to Whisper event."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#results-in-common-voice-dataset",
    "href": "presentations/fossasia2023/index.html#results-in-common-voice-dataset",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Results in Common Voice dataset",
    "text": "Results in Common Voice dataset\n\nThennal model got the best results:\n11.560000 WER, then rest of models got 21-40+ models."
  },
  {
    "objectID": "presentations/fossasia2023/index.html#results-in-msc-dataset",
    "href": "presentations/fossasia2023/index.html#results-in-msc-dataset",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Results in MSC dataset",
    "text": "Results in MSC dataset\n\nThennal model got the best results:\nGot model with 1 WER\nOther models"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#result-improvement-from-whisper-base-model",
    "href": "presentations/fossasia2023/index.html#result-improvement-from-whisper-base-model",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Result improvement from whisper base model",
    "text": "Result improvement from whisper base model\n\nin WER Malayalam CommonVoice9 dataset:\ntiny - 102.7 base - 122.9 small - 104.8 medium - 137.8 large - 107.1 largev2 - 103.2"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#tools-for-benchmarkings",
    "href": "presentations/fossasia2023/index.html#tools-for-benchmarkings",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Tools for benchmarkings",
    "text": "Tools for benchmarkings\n\nhttps://github.com/kurianbenoy/malayalam_asr_benchmarking"
  },
  {
    "objectID": "presentations/fossasia2023/index.html#conclusion",
    "href": "presentations/fossasia2023/index.html#conclusion",
    "title": "OpenAI Whisper and itâ€™s amazing power to do finetuning.",
    "section": "Conclusion",
    "text": "Conclusion\nAccording to whisper in future work section:\nStudying fine-tuning \n\nIn this work, we have focused on the robustness properties of speech processing systems and as a result only studied the zero-shot transfer performance of Whisper. While this is a crucial setting to study due to it being representative of general reliability, for many domains where high-quality supervised speech data does exist, it is likely that results can be improved further by fine-tuning. An additional benefit of studying fine-tuning is that it allows for direct comparisons with prior work since it is a much more common evaluation setting.\n\n\nYou maybe able to achieve SOTA in your own language with Whisper\n\n\n\nkurianbenoy.com/presentations/fossasia2023/index.html"
  },
  {
    "objectID": "presentations/lt_talk_idea/index.html#what-i-expected-if-speak-in-conferences",
    "href": "presentations/lt_talk_idea/index.html#what-i-expected-if-speak-in-conferences",
    "title": "You too should speak here",
    "section": "What I expected if speak in conferences",
    "text": "What I expected if speak in conferences\n\nBeing a rockstar\nGetting a lot of job offers and companies queing behind me"
  },
  {
    "objectID": "presentations/lt_talk_idea/index.html#what-actually-happend",
    "href": "presentations/lt_talk_idea/index.html#what-actually-happend",
    "title": "You too should speak here",
    "section": "What actually happend?",
    "text": "What actually happend?\n\nI was able to share my expertise\nMade tons of friends and mentors\nVisit lot of unique places in India and Abroad"
  },
  {
    "objectID": "presentations/lt_talk_idea/index.html#ending-note",
    "href": "presentations/lt_talk_idea/index.html#ending-note",
    "title": "You too should speak here",
    "section": "Ending Note",
    "text": "Ending Note\n\nâ€œYou are never too young to shareâ€ â€œYou are never too old to learnâ€\n\nThatâ€™s five minute, see you speak in the next conference"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "ðŸŽ¤ Talks",
    "section": "",
    "text": "I like to frequently attend conferences, meetups, devsprints and workshops to learn and interact with awesome people.\n\nThese contain the Slides photos and video links for few of the talks which I have delivered.\n\n\n\n#\nTopic\nVenue\nEvent\nDate\nSlides\nLinks\n\n\n\n\n1.\nDemystifying Async & Await in Python & JavaScript\nOnline using Hoppin\nPycon India\nSeptember 19, 2021\nSlide Link\nTalk Video\n\n\n2.\nMachine Learning Models and Dataset Versioning\nChennai Trade Center ,Chennai, India\nPycon India\nOctober 13, 2019\nSlide Link\nTalk Video\n\n\n3.\nAt the Eye of Flood - Keralarescue.in\nLife Long Learning Institue, Singapore\nFOSSASIA Summit\n17 March 2019\nSlide Link\nTalk Video\n\n\n4.\nProject Showcase - ToonIt project\nOnline\nTensorflow UserGroup India Summit 2020\nSeptember 6, 2020\nSlide Link\nTalk Video\n\n\n5.\nTinkerhub Learning class- Python for beginners\nOnline\nLearn From Home Program\nFebruary-March, 2020\nN/A\nTalk Video\n\n\n6.\nIntroduction to Django2\nKochi\nKochi Python Meetiup\nJuly 07, 2018\nSlides\nN/A\n\n\n7.\nPitfalls, Glamour and Valuable advice from an AI/ML Expert\nOnline\nInnovate At MEC Podcast\n7th January, 2021\nN/A\nPodcast Link\n\n\n8.\nHuman Library - Introduction to Datascience and ML\nGovt. Model Engineering College\nKochi Foss\nOctober 15, 2022\nN/A\nlink"
  },
  {
    "objectID": "posts/2021/2021-12-25-Conversation-with-vuejs-core-developer.html",
    "href": "posts/2021/2021-12-25-Conversation-with-vuejs-core-developer.html",
    "title": "Conversation with Vue.js Core Developer Ben Hong",
    "section": "",
    "text": "1. Will class based components supported in Vue3? What is support of it?\nThere are already support for Options API and Composition API in Vue.js. The Vue.js core team is already supporting two options already, and supporting another option will be difficult. That being we know a lot of people like class based API, and prefer that for their project. The class based components is actually being a maintained by one of Vue.js core developer members. So if they support, then it will be available to be compatible to Vue3.\n2. When will Vue2 be kind of deprecated, and Vue3 be the official version? When can we expect it to happen, and will it take 10+ years like Python2 to Python3?\nFirst of all it has been almost 1 years since Vue3 was officialy released on September, 2020. Vue3 is definetely way better in term of itâ€™s features and almost 90% of features are backward compatible in Vue3. Recently Nuxt.js version for Vue3 was released as Beta. So for any new projects, itâ€™s definetely recommended to start with Vue3. I feel Vuetify beta release will be like the last block to complete it. So I am sure it wonâ€™t take like 10 years to migrate Vue2 to Vue3, and in a few years we will see Vue3 as default version. Companies like Wikimedia, have almost completely migrated from Vue2 to Vue3.\n3. What do you think about Work life balance as a full-stack developer? You are working in Netlify, Vue.js core developer and even have a youtube channel?\nHmm. Itâ€™s a tough question to be honest. I am personally not a fan of 9-5 job culture. In case of you are a developer, itâ€™s very important to understand the priorities in your life. Also you should link your priorities to what you are interested in. When I was working, I was looking for jobs which was for Vue.js developer, not a React developer. In long run, you will have just 2-3 months were you focus only on your work, and not catching up with latest developer trends.\n4. How many core members are in Vue.JS. Are there any core developers from India?\nIf you compare to other frameworks, Vue.js is one framework which is totally open source and is not being backed by any big-tech companies. There are almost 20+ core developers in Vue.js team. To my knowledge, I am the core developer who is based in USA and I am not aware of any core developer form India."
  },
  {
    "objectID": "posts/2021/2021-09-24-messy-git-commits-tale.html",
    "href": "posts/2021/2021-09-24-messy-git-commits-tale.html",
    "title": "Messy git commit relation with master and develop branches",
    "section": "",
    "text": "In github, there are usually dev and main branches. Usually developers advocate protecting branch features, to avoid any of user to not merge directly without commiting.\nThe reason we are getting merge conflicts when merging from develop to master is simple. When merging PRs we are having a new commit named Merge pull request #no in master branch. According to my understanding branches are like a straight line, if there is a new curve cause by merging using git with the new commit). That curve should be in dev branch as well.\nIf we are not adding that merge commit(the curve), and continuing development in the dev branch. The things are not in sync, and we are playing by messing history ðŸ˜. You are allowed to mess with history by rebasing, and messing history in protected branches is not approved.\nThis is why git is raising a merge conflict when pushing latest changes from dev branch to master branch. Thatâ€™s why we are needing to fix history in dev branch always with a new PR to have the commit called Merge pull request #no also be included in our history.\n~ Mera Thoughts"
  },
  {
    "objectID": "posts/2021/2021-01-21-pythonformatting.html",
    "href": "posts/2021/2021-01-21-pythonformatting.html",
    "title": "3 Python libraries to make your code more pythonic",
    "section": "",
    "text": "Python is one of the most used programming libraries. It is important to follow proper decorum while coding in Python by following rules such as PEP8. Today I am going to introduce a few libraries for formatting and making your code prettier in Python.\n\nblack\nIt is an uncompromising python code formatter. The best thing about black is the project is built with a core goal to avoid any conflicts in the python coding formats over the time and act as a unifying interface for code formatting. To use black, itâ€™s pretty easy:\nblack {source_file or directory}\nBy default, itâ€™s a bit intentional by only allowing double-quoted string, with a maximum line number by default set as 88 and follow some more opinions which are PEP8-compliant. black is widely used in the python community. To check more options about black, look at the documentation. Thanks to ambv & PSF volunteers for this awesome library.\n\n\nisort\nisort is a python library for sorting your imports in python. According to PEP8, there are few recommendations to order when importing libraries:\n\nstandard library imports\nrelated third party imports\nlocal application/library specific imports\n\nisort contains 12 possible settings for multiple line imports. Also, you can add comments, change the import order and remove or append imports with the various setting in isort.\nDocumentation\nThe default configuration of isort and black has a few style conflicts, which we will cover how to fix in the latter part of this article.\n\n\npip-chill\npip-chill is a library for fixing the ugly output of pip-freeze which lists all the packages and associated sub-packages. Most of the people create their requirements based pip freeze output. This is where pip-chill to list just the packages you have installed with the command pip-chill.\nDocumentation\n\n\nInter-operability with black and isort\nLetâ€™s look at how we can make both black and isort compatible with each other. The compatibility crisis can be solved by two approaches:\n1. Using Scripts to run both black and isort\nformatter.bat\n@ECHO OFF\nisort --profile black .\nblack -S -t py38 .\nECHO Complete formatting code with black and isort\nformatter.sh\n#!/bin/bash\nisort --profile black .\nblack -S -t py38 .\necho \"Complete formatting code with black and isort\"\n2. Integrating with pre-commit\npre-commit is an awesome git hook to identify simple issues and autoformatting the code with black and isort before the project is being run. Install pre-commit and create a .pre-commit-config.yaml file for setting up formatting with black and isort.\n.pre-commit-config.yaml\nrepos:\n-   repo: https://github.com/psf/black\n    rev: 19.3b0\n    hooks:\n    -   id: black\n-   repo: https://github.com/pycqa/isort\n    rev: 5.6.4\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\", \"--filter-files\"]\n\nThen run the git commit and the file gets added to git only if the tests for both black and isort are passed.\n\nKurian Benoy"
  },
  {
    "objectID": "posts/2021/2021-07-28-fastgroup-7.html",
    "href": "posts/2021/2021-07-28-fastgroup-7.html",
    "title": "Log7- Learning FastBook along with Study Group",
    "section": "",
    "text": "In week 7, we covered the second half of lesson 5 and started with lesson 6 on Other Computer Vision problems like Multi-Label classification. We have covered so far almost 25% of the FastBook aka. Deep Learning for Coders.\nLast week, I wrote about what is covered in rest of lesson 5. Some of the new things which Aman introduced was:\n\nThe valley function in fastai, which is available in latest version to get the exact learning rate to be passed.\n\n\n\n\nimage\n\n\n\nMore scheduling algorithms were mentioned like Torch optimizers.\nI tried working on these applying techniques in a different dataset consisting of images of various painters in Kaggle. The notebook used for training can be found here:\n\nThe chapter 6 consists of discussion other Computer Vision problems like Multi-label classification, Regression. During the session we covered more about loading dataset for multi-label classification.\nWe used PASCAL 2007 dataset for this task, which is consisting of images and tabular data with the labels, filename and whether itâ€™s part of validation datset.\n\n\n\nimage\n\n\nAman during the lesson explained the difference between lambda functions and normal funcitons with a simple example. To check out more about lambda functions checkout this article from RealPython.\n\n\n\nimage\n\n\n\nNext we can create a Datablock, based on the input dataset. With training file path as independendent variable, and labelled list as dependent variable.\n\nWhen looking into Pytorch and fast.ai, there are two classes for representing datasets:\n\nDatasets - a collection that returns a tuple of independent and dependent variables for a single item. In fastai it returns an iterator for bringing your training and validation dataset.\nDataLoaders - a iterator which provides stream of mini-batches where each mini-batches is a couple of batch of independent variables and dependent variables.\n\n\n\n\nimage\n\n\nThis week I tried more about learning Pytorch Datasets and Dataloaders based on this tutorial. The details can be found in week 6 training notebook.\nWe used a Mulicategory block this time and used one hot encoding technique to identify where is the correct category for each labelled image. Using a splitter for training and validation splitting, the dataset was finally loaded as follows:\n\n\n\nimage\n\n\nThe full notebook of week 6 can be found here\nThe full session recording can be viewed in the below link and thanks for reading ðŸ™.\n\n\n\nIMAGE ALT TEXT\n\n\n\nInteresting Article Links\n\nNitron Open-Source GPU Programming for Neural Networks\nMethods for automating lr finders"
  },
  {
    "objectID": "posts/2021/2021-04-18-financial_lessons.html",
    "href": "posts/2021/2021-04-18-financial_lessons.html",
    "title": "Some Financial Lessons(not advisory)",
    "section": "",
    "text": "Hey, I thought about sharing a few lessons which I learned about finance:\nImagine you are given an offer:\n\nI will give you 10 lakhs over 5 years\nI will give you 50 lakhs today\n\nIf you said option (a) like 90% of the people. You donâ€™t understand the concept of Inflation. The reason is simple, the 10 lakhs of today will never be the same amount in 5 years, itâ€™s going to be even lesser.\nLet me talk about the 7 baby steps created by Dave Ramsey throughout his journey:\n\nSave $1,000 for your starter emergency fund.\nPay off all debt (except the house) using the debt snowball.\nSave 3â€“6 months of expenses in a fully-funded emergency fund.\nInvest 15% of your household income in retirement.\nSave for your childrenâ€™s college fund.\nPay off your home early.\nBuild wealth and give.\n\nKunal Shah recently talked about what every Indian need to know about finance in their 20s:\n\n\n\n\n\nSo this is my short rambling for this week. Stay safe and curious.\nThree links for this week ðŸ‘‰:\n\nA talk about what do developers need to learn and work on as they are becoming older and donâ€™t want to be a manager. The answer is provided by this awesome talk for developers who want to continue their adventurous path by Jesse Jiryu Davis, MongoDB\nHow do Python Dictionaries work by Rahul Jha\nDifference between Amateurs and Professionals"
  },
  {
    "objectID": "posts/2021/2021-04-11-JavascriptPromises.html",
    "href": "posts/2021/2021-04-11-JavascriptPromises.html",
    "title": "An Issue I had with fetching data with Axios",
    "section": "",
    "text": "I was working on building Camunda- formio Tasklist as part of my work. In the Javascript ecosystem, a lot of folks use the Axios library to work with fetching data from APIs. There are a lot of other ways also to fetch data like Ajax calls, using fetch API etc as well.\nSo let me talk to you about my problem. I wrote the below snipped to fetch a GET request API data on passing the API URL, parametrised data, token which I am passing will return the data.\n import axios from 'axios';\n\nexport const httpGETRequest = (url: string, data: any, token: string, isBearer = true) => {\n  return axios.get(url, {\n    params: data,\n    headers: {\n      Authorization: isBearer\n        ? `Bearer ${token}`\n        : token,\n    },\n  });\n};\nNow I am going to call this method to pas the associated ApiUrl, applicationId, token etc to fetch a specific API to get the history on passing a particular applicationID. I was expecting that it was going to on using then() method, I can extract the value from the API, like this way:\nexport const getformHistoryApi = (ApiUrl: string, applicationId: string,  token: string) => {\n  httpGETRequest(ApiUrl+\"/application/\"+applicationId+\"/history\",{}, token).then((result) => {\n      //fetching necessary data\n  }\n}\nYet it doesnâ€™t work. On realising why it doesnâ€™t work is because bought me to look at the previous works realise instead of working on that method, I called the following:\nexport const getformHistoryApi = (ApiUrl: string, applicationId: string,  token: string) => {\n  return httpGETRequest(ApiUrl+\"/application/\"+applicationId+\"/history\",{}, token)\n  \n}\n\n\ngetformHistoryApi(\"https://kurianbenoy.com\", 250, alasdfjadf). then((result) => {\n// fetch API data object\n})\n.catch((error)=> {\n//in case of any errors\n}\nThis solved my issue. But I was curious about the why part. Then I looked into the documentation of Axios, and it starts with the tagline:\n\naxios: Promise based HTTP client for the browser and node.js. It was not knowing axios used promises which caused me the issue.\n\n\nWhat are Promises?\nI am stealing this from Chandelier Axels article on Promises\nAccording to MDN web documentation definition for promises is the following :\n\nA Promise is a proxy for a value not necessarily known when the promise is created. It allows you to associate handlers with an asynchronous\naction's eventual success value or failure reason\n\nIt doesn't make sense to me at all. \n\nSo what are promises?:\n\nFirst, the basics. Javascript is a synchronous and mono-threaded language. It means that all your code will execute in the order it's written, and it only has one call stack. To keep it simple, we'll stand that JS is a language where everything happens in order, without any external add-ons.\n\nPromises are a way to execute certain pieces of code asynchronously, meaning they'll be executed behind the scenes, while the rest of the synchronous code is done.\n\nWhy do we need them?\n\nLet's take a real-life example with two waiters. To keep it simple, our waiters are responsible for taking the orders and delivering the dishes from the kitchen to the clients.\n\nOur first waiter will be the synchronous one (like if Javascript promises never existed). He would take the order, give it to the kitchen, wait for the dishes to be ready, and finally serve them, and so on. Kinda awkward and inefficient.\n\nOur second waiter will handle things asynchronously. He'll take the orders from the clients and give them to the kitchen. By the time the dishes are ready, he will go do something else and come back later for the dishes whenever they are ready.\n\nThis is exactly what's happening in JS. The kitchen will give a promise to the waiter that the dishes will be ready sometime in the future.\n\nA promise always takes a function with two arguments: resolve and reject. When the promise must return the result, we call resolve with the results. If something wrong happened, let's say we're missing some ingredients, the whole promise is compromised, we must cancel the order and get something different from the client, this is where we call reject.\n\nHow do we access the value then?\n\nIt allows you to associate handlers, the keywords here are handlers. Let's go back to our previous example, but let's get it to work for real this time.\n\nconst preparingDishes = new Promise((resolve, reject) => {\n  // See the code above\n});\n\n// Now that our promise is created, let's trigger it, and then read the results\npreparingDishes\n  .then((dishes) => {\n    // dishes is a arbitrary name, usually it's called result\n\n    /* Within this function, we can access the result of the promise. The parameter will be the value you gave to the resolve.\n    You are guaranteed that anything you put in here, will execute when the promise is fulfilled (successful) */\n    callWaiter(dishes);\n  })\n  .catch((error) => {\n    // In case an error happened, this handler will catch the return value inside your above reject or any error that could happen in your promise code\n    if (error === 'An ingredient is missing !') {\n      sendWaiterBacktoClients();\n    }\n  })\n  .finally(() => {\n    // This one will execute anything that you put inside, either the promise succeed or not\n\n    // For example, whether the kitchen succeed in preparing the dishes or not, they'll have to start the next one\n    prepareNextDishes();\n  });\n\nAs you must have noticed by now, the .then, .catch and .finally are the handlers MDN is talking about. Each will execute under different circumstances as stated above.\nPlease take note that attaching all the handlers isn't mandatory, you could only attach a .then for example (but I wouldn't recommend it), or only a .then and a .catch, which is what you'll use most of the time.\nIf you look at MDN documentation, there are three states of promises that are:\n\nPending: initial state, neither fulfilled nor rejected.\nFulfilled: meaning that the operation was completed successfully.\nRejected: meaning that the operation failed.\n\nI will also recommend you to check out the end section of Chandelier Axels article on Promises about the async/await methods.\n\n\n\n\n\n\nI and my Tux have started reading the Pragmatic Programmer and I am sharing a few of the notes from the book with you folks:\n\n\n\n\n\n> An investment in knowledge always pays the best interest ~Benjamin Franklin\n\n\nBuilding a knowledge portfolio is similar to managing a financial journey:\n\n\n- Serious investors invest regularly as a habit: Like investing it is important to spend weekly a specific amount of time on what you are learning. - Diversification is key for long-term success: The more things you know you are more valuable. So learn ins and out of the technology you are learning, maybe get deep into the layer which you have been working throughout the years. - Smart investors balance their portfolios between conservative and high-risk: Some languages which are already established and the growth rate will be a little less. It is important to manage your technical risks and grow. - Investors try to buy low and sell high for maximum return: While if you take a bet on some new cool technology that is just getting established, you become an established figure in that language. Imagine the first folks who tried Java and used it throughout their lifetime. - Portfolios should be reviewed and rebalanced periodically: Review your knowledge skills periodically and rebalance your strengths to stay relevant in the industry.\n\n\n\nThree links for this week ðŸ‘‰:\n\nHistory of linked list and why itâ€™s asked in interviews\nWhy I write blogs by Sahil Dhiman\nCreating Streamlit dashboards using Python"
  },
  {
    "objectID": "posts/2021/2021-06-16-fastgroup-1.html",
    "href": "posts/2021/2021-06-16-fastgroup-1.html",
    "title": "Log1-Learning FastBook along with Study Group",
    "section": "",
    "text": "I am sharing a few advice shared by Jeremy in the Lesson-0 of the fastai course and the introduction section of the book:\n\nCommit to complete the course \nThe actual learning of the book, involves the following components usually:\n\nWatching lecture/book\nRunning notebook and Experimentation\nReproduce Results\nWorking on a different dataset \n\nAlways start with a clean version of the notebook and see what all you remember of the existing lessons, and understand what it is doing\nDonâ€™t hesitate to ask questions ~ Aman Arora\n\n\nLet me share a few lists of things I learned over the last week. So letâ€™s get started with the logs:\n\n\nIf you have trained any model with fastai, you may have noticed the fine_tune() method. So even setting fine_tune(0) run over the dataset once(one epoch). But why? I didnâ€™t notice that until someone asked this question\n\nSince you look at fine_tune() method you will realize the freeze_epochs is set as one by defaults, and epochs you passed in this case is set as zero\n\n\n\nimage\n\n\n\nJustin Hougston answered: on what is the difference between fine_tune and fit_one_cycle method:\n\n\nFrom my understanding - in this example, we are defining a pre-trained cnn learner (which is resnet34), and then calling fine_tune() on that model (with pre-trained weights). The. .fine_tune() takes in a few params, the first being number of epochs. So in your code above, we are explicitly saying to run through each item in the dataset once. Interestingly enough, the fine_tune actually re-uses .fit_one_cycle (which is super cool in it of itself) in addition to some unfreezing and freezing of weights. (thread discussing the difference here)\n\n\n\n\nimage\n\n\n\nWhat is the secret sauce behind the beautifully formatted doc function in nbdev? Kevin Bird explains:\n\n\n\n\nimage\n\n\n\nIn jupyter notebooks you can use ? mark to see the function definition and ?? to see the source code. Itâ€™s interesting to see how well fastai library is written, with short functions of code wherever I explored it.\nuntar_data() function is available for downloading datasets. The datasets are being downloaded in fastai folder, and you can look inside the dataset, by going to that particular path provided by the pathlib module. In the below example, I am opening the URLs.adult dataset in jupyter notebook and explore the csv file using pandas.\n\n\n\n\nimage\n\n\nOn using more features, I was able to get better accuracy than the example provided in the book:\n\n\n\nimage\n\n\n\nIn Chapter 2, in the process of downloading images from the internet. I was not interested in using Azure keys as provided by docs. So I found an alternative method by using duckduckgo as mentioned in the book website. The fastbook method for searching duck duck go was not working and it was throwing an error:\n\nAttributeError                            Traceback (most recent call last)\n<ipython-input-10-fcb0d9b3a15b> in <module>()\n----> 1 urls = search_images_ddg('grizzly bear')\n      2 len(urls),urls[0]\n/usr/local/lib/python3.7/dist-packages/fastbook/__init__.py in search_images_ddg(term, max_images)\n     55     assert max_images<1000\n     56     url = 'https://duckduckgo.com/'\n---> 57     res = urlread(url,data={'q':term}).decode()\n     58     searchObj = re.search(r'vqd=([\\d-]+)\\&', res)\n     59     assert searchObj\nAttributeError: 'str' object has no attribute 'decode'\nThe error can be easily solved by remove decode() function in line no 57. Since the PR for that changes has not yet been merged in fastai repo, I used DuckDuckGoImages package for downloading images with the below code:\n\n\n\nimage\n\n\n\nSince deciding on a GPU server is an important thing to do at first. For the time being, I am planning to use the GPUs provided by Kaggle and Google Colab. Also locally I have set up notebooks with jupyterlab for browsing and looking into source code when notebooks are being run on the server.\n\nRelevant links for this week âž¡ï¸:\n\nFree Scikit learn course by Core Developers of the project\nhttps://github.com/tunguz/ML_Resources\nVideo Classification with Transformers - Sayak Paul"
  },
  {
    "objectID": "posts/2021/2021-05-23-TryingOutngrook.html",
    "href": "posts/2021/2021-05-23-TryingOutngrook.html",
    "title": "Trying out ngrok",
    "section": "",
    "text": "ngrok is a cross-platform application software for enabling developers to deploy or expose their web applications from locally with a associated website url. This works due to the SSH tunnelling.\nSo this weekend I was trying out ngrok with few of the projects I have worked:\n\nProject 1: Cartoonizer\n\nHosting Cartoonizer is a long pending issue for me. The cartoonizer to use with ngrok. I downloaded the package: flask-ngrok. Just making a few changes as mentioned in the below article got the application up and running with a https url. The application was up and running, yet when I tried uploading an image to cartoonize, it failed to work due to the usage of os module in my library to fetch and cartoonize image.\n\nProject 2: Camunda-formio-tasklist-vue\n\nSince this is a Vue application, I was expecting to easily use along ngrok. Instead of using any packages this time, I created an account in ngrok and downloaded the client for the operating system. Then on authenicating with my token and running the command:\nngrok http 3000\nI was expecting my application to be up. Yet, I faced an error message:\n\nInvalid Host Header\n\nOn googling a bit, I stumbled upon a solution once I ran the command:\nngrok http 3000 -host-header=\"localhost:3000\nSo my application was up and running after a 20-30 seconds page delay(which I am curious why though?)\n\nProject 3: Augmentation Web App library\n\nSince this was a streamlit application. When I tried installing in windows, it gave me some SocketIO connection error, which didnâ€™t run my application. So then I used wsl to run the project and on running it up, it was not getting pointed to an internal IP address instead of localhost. So when I ran ngrok, like the previous approach. It was not working in associated port. I need to see how can I point an internal IP to ngrok or get this application running in localhost.\nSo I am winding off by sharing how ngrok works. Also some good news ðŸŒ¼, after nearly 5 months - I have contributed to Shahulâ€™s latest open source project- Twitter Emotion. The project given an input tweet, extracts the phrase in the tweet associated to express a particular sentiment.\n\n:wq"
  },
  {
    "objectID": "posts/2021/2021-03-30-shutdown-routine.html",
    "href": "posts/2021/2021-03-30-shutdown-routine.html",
    "title": "Shutdown Routine",
    "section": "",
    "text": "It was a casual Saturday morning, and I was waiting in front of an Akshaya Center to get my Aadhar Card photo updated. Even after one hour, the line hadnâ€™t moved much. So I sort of decided to start reading one of my kindle E-books on my phone.\n\n\n\nShutting Down\n\n\nThe book was Deep work written by Cal Newport. I had purchased this book, for more than 2-3 months, yet I didnâ€™t make much progress in reading this book. I had almost completed 30% of the book when I left it earlier. So I continued slowly reading through the book in a chair with five people surrounded around me.\nThe book first explains why Deep work is valuable, and later talks about how to achieve deep work. So on resuming reading in a queue, I stumbled upon the idea of having a Shutdown routine. I have to admit that, I am not good so far at managing work and life. Let me admit that writing this blog post completed only on Tuesday, because of this. There have been days when few of my colleagues have told it late and rest now at the midnight. It is always important to have a proper balance between life and work.\nLet me introduce to you Kenneth Reitz, who is the author of famous python libraries like Request, Pipenv, Maya etc. He shared his experience with burnout. He was almost in a state of 410(where he was about to delete all the source code and beautiful software he had written for humans). Yet luckily for all of us, he didnâ€™t go that extreme step. Check Kennethâ€™s essay on his blog on the reality of developer burnout\nShutdown routine is a state in the world where you decide to have a fixed end time for the various projects you build. And on the end of the day, decide to stop work with a specific shutdown routine.\nCal Newport in his book argues for a complete shutdown from work after a specific period as an important activity. He gives reasonable points why itâ€™s important to have that down time every day after book:\n\nDown Time aids insights\nDown Time helps recharge energy needed to work deeply\nThe work that evening downtime replace is usually not important\n\nThe reason to have shutdown principle is valuable is because of a principle called Zeibarnik effect. According to the Zeibarnik effect, interrupted and unresolved tasks can remain in our mind for a longer time and give a hard time. This is a cause for after-work stress which occurs even after work.\nThis stress constantly reminds the brain of the portion you havenâ€™t completed that day, makes you think about the conversations you had at work and makes you in a sense where you are very less productive. This can cause you to work late in the night to complete the pending task, check emails or instant chat messages constantly even at night. This makes your work getting occupied for 24*7 hours every day.\nSo with the Zeibarnik effect, Itâ€™s making the time after work just more and more stressful. So what is the solution for this?\nAccording to research, the best way to counter the Zeibarnik effect is to make a plan. This may involve creating a task list of pending items. Commit to a specific plan for a goal therefore not only facilitates the attainment of the goals but also free cognitive resources for other pursuits. With this, you have a clear plan for your work and helps you work more deeply at the required time. This avoids the constant amount of shallow which we are committing, which makes you less productive.\nCal calls to have a specific ritual before the end of the workday. In this time, you should create a plan on how to achieve the desired target, ensure the necessary communication is done and after this stop by saying I am shutting down(This is the the ritual which Cal Newport does to stop working). I am also planning to start planning this shutdown routine in the coming days and see how it goes.\nSo far I havenâ€™t been able to not even complete a day with the new shutdown routine. I will update the article after seeing how my progress goes in the coming days.\nBefore leaving here are my three links for this week ðŸ‘‰:\n\nCompound interest is often considered the eighth wonder in the world. Check out this SIP Calculator which gives an idea of the return of investment given the return ratio and inflation rate with monthly investment\nInterview with Sid Tobias on creating digital transformation leaders\nHow Zerodha Indiaâ€™s largest stock brokerage uses FOSS in their tech stack by CTO Kailash Nadh"
  },
  {
    "objectID": "posts/2021/2021-03-19-Untitled.html",
    "href": "posts/2021/2021-03-19-Untitled.html",
    "title": "Untitled - Random thoughts on my Supplies",
    "section": "",
    "text": "Hola!\nâ€Œ I know I am late this time. Sorry in advance for that. I know I canâ€™t still find the rhythm of writing every weekend, so I procrastinate and plan to publish it by Monday. I am awful about it and promise to be better in writing every week by Saturday evening itself even if itâ€™s short. ðŸ¤ž\nâ€Œ This week, I am going to share one of my insecure moments in my college life. It was in my second year of undergraduate studies, and when the exam results were out I failed in 3 subjects. The process of failing in the three subjects was obliviously such a disaster.\nâ€Œ There was a lot of pressure from family and everyone around. All my activities planned on the tech side was dropped - like being part of the web team, attending prestigious hackathons etc. The result of the failure helped me focus time on my studies more. Till that semester, I was someone who had just learned before the day of the exam. Yet now I realized that wouldnâ€™t work out. My CGPA was in disaster mode and it was in the range of five points something.\nâ€Œ Yet, instead of going all out on just studies. I focused on fixed time for extracurricular activities as well. And in my fifth semester, I attended my first Pycon India conference(which gave me the courage to be a speaker in the next edition), conducted interviews with two open-source developers - Mario Behling, founder of FOSSASIA and Balasankar C and attended FOSSASIA Summit in Singapore with a scholarship.\nâ€Œ With my low CGPA, I was expecting a disaster with no future placements, no internships. Yet I realised a lot of my fears were because that is what normal people expect us to have. They want a no failure success, even though success in anything can be attributed to being a perfect A student.\nâ€Œ All these things in hindsight were not so oblivious like what is value in creating a blog post, what is the value in working in Open source. Yet in the end, it all paid forward and helped my journey. I still remember writing a piece to myself, mentioning the toughest week of my life. Yet life moves on like every other day\nâ€Œ So letâ€™s be independent and say yes to more crazy decisions."
  },
  {
    "objectID": "posts/2021/2021-07-04-CORS.html",
    "href": "posts/2021/2021-07-04-CORS.html",
    "title": "How to selectively allow multiple URL origins in flask?",
    "section": "",
    "text": "If you already donâ€™t know what is Cross-Origin Resource Sharing(CORS). Please check out the below links:\nYou may have understood the following:\nItâ€™s a mechanism to allow communication of one resource to another resource in a different domain. It sets the header, Access-Control-Allow-Origin which can have the following values - *, <origin>, null.\nTo implement CORS simply in flask, there are a bunch of different ways:"
  },
  {
    "objectID": "posts/2021/2021-07-04-CORS.html#to-use-flask-cors-library",
    "href": "posts/2021/2021-07-04-CORS.html#to-use-flask-cors-library",
    "title": "How to selectively allow multiple URL origins in flask?",
    "section": "To use flask-cors library",
    "text": "To use flask-cors library\nflask-cors is a library that is like an extension used for handling CORS. It helps in enabling CORS in multiple ways like:\nThe default way to use flask_cors for all resources in all domains is as follows:\n{% highlight python linenos %} from flask import Flask from flask_cors import CORS\napp = Flask(name) CORS(app)\n@app.route(â€œ/â€) def helloWorld(): return â€œHello, cross-origin-world!â€ {% endhighlight %}"
  },
  {
    "objectID": "posts/2021/2021-07-04-CORS.html#to-use-a-decorator-on-your-own",
    "href": "posts/2021/2021-07-04-CORS.html#to-use-a-decorator-on-your-own",
    "title": "How to selectively allow multiple URL origins in flask?",
    "section": "To use a decorator on your own",
    "text": "To use a decorator on your own\n{% highlight python %} # -- coding: utf-8 -- from future import unicode_literals\nfrom datetime import timedelta from flask import make_response, request, current_app from functools import update_wrapper\ndef crossdomain( origin=None, methods=None, headers=None, expose_headers=None, max_age=21600, attach_to_all=True, automatic_options=True, credentials=False, ): â€œâ€œâ€ http://flask.pocoo.org/snippets/56/ â€œâ€œâ€ if methods is not None: methods = â€œ,â€.join(sorted(x.upper() for x in methods)) if headers is not None and not isinstance(headers, str): headers = â€œ,â€.join(x.upper() for x in headers) if expose_headers is not None and not isinstance(expose_headers, str): expose_headers = â€œ,â€.join(x.upper() for x in expose_headers) if not isinstance(origin, str): origin = â€œ,â€.join(origin) if isinstance(max_age, timedelta): max_age = max_age.total_seconds()\ndef get_methods():\n    if methods is not None:\n        return methods\n\n    options_resp = current_app.make_default_options_response()\n    return options_resp.headers[\"allow\"]\n\ndef decorator(f):\n    def wrapped_function(*args, **kwargs):\n        if automatic_options and request.method == \"OPTIONS\":\n            resp = current_app.make_default_options_response()\n        else:\n            resp = make_response(f(*args, **kwargs))\n        if not attach_to_all and request.method != \"OPTIONS\":\n            return resp\n\n        h = resp.headers\n\n        h[\"Access-Control-Allow-Origin\"] = origin\n        h[\"Access-Control-Allow-Methods\"] = get_methods()\n        h[\"Access-Control-Max-Age\"] = str(max_age)\n        if credentials:\n            h[\"Access-Control-Allow-Credentials\"] = \"true\"\n        if headers is not None:\n            h[\"Access-Control-Allow-Headers\"] = headers\n        if expose_headers is not None:\n            h[\"Access-Control-Expose-Headers\"] = expose_headers\n        return resp\n\n    f.provide_automatic_options = False\n    return update_wrapper(wrapped_function, f)\n\nreturn decorator\n{% endhighlight %}\nI will prefer to use this implementation if my use case was to just enable CORS instead of importing a third-party library.\nLetâ€™s look into some of the headers used in this method:\n\nAccess-Control-Allow-Origin: It indicates whether the response can be shared with requesting code from the given origin.\nAccess-Control-Allow-Headers: It is used in response to a preflight request which includes the Access-Control-Request-Headers to indicate which HTTP headers can be used during the actual request.\nAccess-Control-Allow-Methods: It specifies the method or methods allowed when accessing the resource in response to a preflight request.\n\nTo use this decorator, just import it and use it over any resources in Flask as following:\n@app.route(\"/predict/tree\", methods=[\"POST\", \"GET\"])\n@crossdomain(origin='*')\ndef predict_tree():\nYet both the above approaches are defaulting to set Access-Control-Allow-Origin: * or <origin> as per the HTTP standards.\n\nOthers\nThere maybe a ton of other options as well the writer may not know ðŸ˜…. Just wanted to put that straight out and I am aware flask-restx cors method is just the code script mentioned in method2.\nYet our issue in plain English is to allow just a small list of allowed origins for our API endpoint. For that, we need to check if the requested resource that is to be shared is part of our list of allowed origins. If yes, set header Access-control-allow-Origin with that requested resource URL.\n\n\nTrying to implement with flask-cors library\nIn configuration docs of flask-cors. Itâ€™s defined that Origin can be set as a string, List, regex pattern too. When passed as a string, the entire string of whitelist URLs was being set for the header Access-control-Allow-Origins. So this solution doesnâ€™t work, as expected as our requirement.\n\n\nTADA finally the solution ðŸ¤—\nFor solving the issue, we had to finally rely after_request function. Itâ€™s a function used to run after each request. It should always take response as a parameter and should return a new response object or the same one passed. The after_request method doesnâ€™t pass requests in case of any exceptions in program. Check the link if you are interested to know more about flask after_request and itâ€™s friends.\n{% highlight python linenos %} from flask import request\n@app.after_request def cors_origin(response): allowed_origins = [â€˜https://kurianbenoy.comâ€™, â€˜https://beautifuljekyll.com/â€™] if allowed_origins == â€œâ€: response.headers[â€˜Access-Control-Allow-Originâ€™] = â€â€ else: assert request.headers[â€˜Hostâ€™] if request.headers.get(â€œOriginâ€): response.headers[â€œAccess-Control-Allow-Originâ€] = request.headers[â€œOriginâ€] else: for origin in allowed_origins: if origin.find(request.headers[â€œHostâ€]) != -1: response.headers[â€œAccess-Control-Allow-Originâ€] = origin return response {% endhighlight %}\nOn checking multiple websites, I have noticed sometimes some websites donâ€™t have the header Origin or Referer header always. So we first check if there, such an Origin exist, if it exists set the Access-Control-Allow-Origin header as the Origin value, else check if the URL matches the request. headers['Host'], if yes set that URL in the Access-Control-Allow-Origin header.\nIf not implemented CORS properly, there is a possibility for using CORS misconfigurations for cracking your website like what was done to a few bitcoin brokers. We all should understand that Same Origin Security Policy is a bedrock of web application security.\nSome of the exploitable misconfigurations with CORS are:\n\nReflected Origin in Access-Control-Allow-Origin - Most of the real attacks require Access-Control-Allow-Credentials to be set True, which is a cause of this vulnerability too. Since developers are setting Access-Control-Allow-Origin dynamical they simply copy the value of Origin header. So this vulnerability can be exposed sometimes when developer checks for domain(victimdomain.com) in Allowed Origin header, then attacker use (attackervictimdomain.com) to steal confediential information.\nSetting as null origin - The specification mentions it being triggered by redirects, and a few stackoverflow posts show that local HTML files also get it. Perhaps due to the association with local files,itâ€™s commonly used by developers and it can be used to sandbox iframe.\n\nMore exploitations and misconfigurations with CORS can be found in these links:\nlink1\nlink 2\nHello dear reader! I have been trying to learn Flask and solve this issue(because I am working on a project with it). Iâ€™ve written a few hundred lines of code in Flask over the past 2 years, but honestly, Iâ€™m pretty bad at it and donâ€™t know the internal workings yet. So my goal with this approach was to learn enough while not getting confused a lot.\nI am sharing my learnings on things which I have struggled inspired by Julia Evans. Please let me know if there is any better approach for this. Thanks for reading ðŸ™."
  },
  {
    "objectID": "posts/2021/2021-07-14-fastgroup-5.html",
    "href": "posts/2021/2021-07-14-fastgroup-5.html",
    "title": "Log5- Learning FastBook along with Study Group",
    "section": "",
    "text": "Just a quick recap In the first chapter, we learned Arthur Samuel mentioned the following:\n\nSamuel said we need an automatic means of testing the effectiveness of any current weight assignment in terms of actual performance. In the case of his checkers program, the â€œactual performanceâ€ of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins. Finally, he says we need a mechanism for altering the weight assignment so as to maximize the performance. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction.\n\n\n\n\nimage\n\n\nAnd the book mentions:\n\nNeural networks - a function that is so flexible to be used to solve for any given problem SGD - a mechanism for automatically updating weight for every problem\n\nThe whole training loop of any ML task:\n\n\n\nimage\n\n\nThis week in the group we were looking more into Second half of Chapter 4 which covers the gradient descent, specifically stochastic gradient descent. As a whole the week 5 was pretty code intensive and letâ€™s looks the whole things covered this week section by section from middle of chapter 4.\n\n\n\nimage\n\n\nSince last week covered we checked how to classify 3s and 7s with pixel similarity. I was trying to classify and do all the steps using the full MNIST dataset which can be found in this Kaggle notebook.\nI am sharing rough notes of this section for me to remember what we covered in each section of Chapter 4:\nStochastic Gradient Descent\nIt consists of seven steps: - Initialize - Predict - Loss - Calculate gradient(weight assignment) - Step (repeat forever till epoch ends) - Stop\nCalculating Gradients Stepping with a learning rate End to End SGD\nThe MNIST Loss Function\n\nIn Pytorch we have used the code with view function(-1, 28*28), to convert a rank3 tensor(matrix) to rank-2 tensor(vectors).\nWe took training and valdataion_dataset.\nUsed it with a model and initialized by init_params() for weights and bias which are parameters in ML model.\nWe used a linear model.\n\ndef linear1(xb):\n  return xb@weight + bias\nThen used mnist_loss() function which is calculation difference with loss and targets.\nSigmoid function\n\nWe made predictions with this function to smoothen between value 0 and 1 with sigmoid in mnist_loss()\n\nSGD and mini batches\nDataLoader can take inputs and split inputs to small batrch to process things in GPU memory. WHole dataset need not always fit in memory.\nPutting it together\n-We are implementing gradient descent algorithm(7 steps here) - call init_params, define train_dl and valid_dl, calculate the gradients, calc_gradients. - Then train_epoch - batch_accuracy - validate_epoch\nfinally got 96% acuracy\nCreating an optimizer\n\nUsed Basic Optim Class maethod\nmodified training_epoch function\nnew train_model func()\n\nGot 97% accuracy\nUsing a non-linearity\n\nUse an optimizer like relu\nUse a Learner, more accruacy imporvement\n\nIn the book after this step got 98.2%\nGoing Deeper with Resnet18\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\nfinally got 99.99% accuracy\nAfter the lesson, I tried answering the questionnare at the end of chapter and I scored: 20/37. I just wanted to turn up this week also by writing this blogpost.\n\nThere are 2 types of people. Players and commentators. Players donâ€™t stop playing to listen to the commentators. They just play their game. - Prashant Choubey"
  },
  {
    "objectID": "posts/2021/2021-02-22-learningvue2.html",
    "href": "posts/2021/2021-02-22-learningvue2.html",
    "title": "Learning Vue.js Part 2",
    "section": "",
    "text": "Learning frontend is a superpower. Even Eugene Yan wrote in his blog post How to win a Data Hackathon in conclusion:\nTraining bespoke machine learning models wasnâ€™t a differentiating factor at this hackathon. Instead, what made a difference was:\n\n- Using readily available data via public datasets or APIs\n- Using libraries / pre-trained models to speed up ML iteration\n- Building UIs to make machine learning and insights easy to consume\n- Deploying models and UIs so people can use them\n\n\n\nimage\n\n\n\nVue is a progressive framework for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable."
  },
  {
    "objectID": "posts/2021/2021-02-22-learningvue2.html#vue.js-getting-started-guide",
    "href": "posts/2021/2021-02-22-learningvue2.html#vue.js-getting-started-guide",
    "title": "Learning Vue.js Part 2",
    "section": "Vue.js getting started guide",
    "text": "Vue.js getting started guide\nLast week we started by picking out a few basics of Javascript. I also recommended it would be a good idea to read Vue getting started guide.\nLetâ€™s first look at Vue getting started guide:\nFollow this link\nThe core library is focused on the view layer only and is easy to pick up and integrate with other libraries or existing projects. If you have read the getting started guide, you may have realised the following features:\n\nDeclarative render - the property of adding data elements\nThen conditional statements using v-if and v-else\nv-for for declaring all elements in the todo list\nv-model to handle two-way input\nFunctions to call on click which can return the necessary functions as appropriately\nUsing components and using bind statements and passing prop\nVue components are similar to web components and are inspired and modelled with Slot API\n\nThe above points are a brief gist of the basic getting started guide about Vue.js. We made a small todo list with the above getting started guide:"
  },
  {
    "objectID": "posts/2021/2021-02-22-learningvue2.html#todo-list---based-on-vue.js",
    "href": "posts/2021/2021-02-22-learningvue2.html#todo-list---based-on-vue.js",
    "title": "Learning Vue.js Part 2",
    "section": "TODO list - based on Vue.js",
    "text": "TODO list - based on Vue.js\nTODO list Code pen\nIf you look at the source code inside"
  },
  {
    "objectID": "posts/2021/2021-06-10-Fast-group.html",
    "href": "posts/2021/2021-06-10-Fast-group.html",
    "title": "Log 0- Learning FastBook along with study group",
    "section": "",
    "text": "I have been recently planning to delve more into learning deep learning and doing more machine learning projects for some time. As a first step, I am excited to share about the Fast Book Study Group Sessions. Itâ€™s great to have experts who are 10 steps ahead of you to help in your learning journey and learn together for the next 25+ weeks ahead.\nI ordered the hard copy of the book Deep Learning for Coders with fastai & PyTorch with a resolution to complete the book, over the fast.ai course. I am indebted to Jeremy Howards, for all the amazing work they have done over the year which has bought me in a position to even buy this book. Thanks Jeremy for your involvement in Masks for all movement :).\n\n\n\nWhatsApp Image 2021-06-10 at 4 03 20 PM\n\n\nI ordered the book from Computer Bookshop and it reached my house in a week despite the Covid situation here in India.\nSo I am sharing a photo of my setup for these sessions, every thursday morning for next 25 weeks lead by Aman Arora.\n\n\n\nWhatsApp Image 2021-06-10 at 4 03 26 PM\n\n\nWish me good luck in my journey. I am thrilled to have few of my doubts answered by Jeremy Howards, Aman Aroroa, Kevin Bird, Sam Watkins and Will Sanger already."
  },
  {
    "objectID": "posts/2021/2021-05-17-notes-js-asynchronousity.html",
    "href": "posts/2021/2021-05-17-notes-js-asynchronousity.html",
    "title": "Notes on Javascript Asynchronicity",
    "section": "",
    "text": "A few points to remember about JavaScript:\nI am single-threaded non-block asynchronous concurrent language. I have a call stack, an event loop, a callback queue and some other APIs.\nExample Link\nMain thread: Task A Task B Promise: |async operation|\nComing on that topic of Event Loop, there is an excellent talk by Phillip Roberts about â€œWhat the heck is event loop anyways?â€. Itâ€™s the most watched video for a particular reason."
  },
  {
    "objectID": "posts/2021/2021-05-17-notes-js-asynchronousity.html#rough-notes-from-that-talk",
    "href": "posts/2021/2021-05-17-notes-js-asynchronousity.html#rough-notes-from-that-talk",
    "title": "Notes on Javascript Asynchronicity",
    "section": "Rough Notes from that talk",
    "text": "Rough Notes from that talk\n\nIn v8, there is a stack, heap. Javascript is single threaded, with single call stack.\nblowing stack(MAximum call stack exceeded)\nWhat if browser were synchronous. In browser, we canâ€™t do anything during program running, so browsers should be ideally asynchronous\nSimplest solution is asynchronous callbacks. In call stack how is behavious happening? After 5 seconds on Set time out function is printed again.\nJavaScript Runtime can do just one time, browser can do multiple times like WebAPIs. In case of NodeJS, there is C APIs instead of webapis.\nIn Stack, webAPIs, task Queue.\nThe event loop elements in queue is only pushed when the stack is clear.\nTime out is minimum guaranteed time which is promised by Event loop\n\nAccording to him JS was build for web, so any web time instance is build not just merely with JavaScript runtime, but under the hood their is queues, Web APIs to implement the event loop. All this is because javascript is used in application for billions of people daily, because it should be used real time. Just when something is running, our browser canâ€™t freeze to wait for another instance to be loaded right.\nNow after watching that 25 minute long talk. Letâ€™s look more into asynchronousity in JavaScript\nThere are two types of asynchronous code style you will come across in JavaScript code: - Old style callbacks - Newer promise style codes\nIn case of promises and callbacks they are different in many ways:\n\nPromises helps in ordered execution of statements with then statements\nbetter error handling in case of promises\nIn case of failure in callbacks it causes the call back hell, which can be pretty difficult to fix the issue\nCallbacks loose full control of how functions are executed when passing to a third party library.\n\nNow as MDN concludes:\nIn its most basic form, JavaScript is a synchronous, blocking, single-threaded language, in which only one operation can be in progress at a time. But web browsers define functions and APIs that allow us to register functions that should not be executed synchronously, and should instead be invoked asynchronously when some kind of event occurs (the passage of time, the userâ€™s interaction with the mouse, or the arrival of data over the network, for example). This means that you can let your code do several things at the same time without stopping or blocking your main thread.\nAlso before concluding the link of this week:\n\nDid you know that switch/case statements are not available in Python?\nHow to implement Sorting in Python, an excellent tutorial by Andrew Dalke and Raymond Hettinger\nDid you hear about the latest library Icream to never use print and log again\n\nReferences:\n\nhttps://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Introducing\nhttps://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Concepts\nhttps://mixstersite.wordpress.com/2020/10/20/javascripts-asynchronicity/\nhttps://javascript.info/async-await"
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html",
    "title": "5 qualities I learned from great software developers",
    "section": "",
    "text": "I am pretty new to the field of software engineering and my journey as a developer has just begun. Yet in my short time, I have got to work with awesome developers both in open-source space and in my workplace."
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html#technical-architecture-and-reusability",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html#technical-architecture-and-reusability",
    "title": "5 qualities I learned from great software developers",
    "section": "Technical architecture and Reusability",
    "text": "Technical architecture and Reusability\nSome of the best developers when working on any project care not just about getting and satisfying requirements. They care about how various technical architecture should be built and has a good grasp of various technologies required for the successful execution of any program. The problem of building and choosing technical architecture canâ€™t be done by any automation software or any language models like GPT3 (which can write good code). This is why itâ€™s so important to be good at it and build that skillset.\nAnother reason why technical architecture is important is to reduce technical debt(which is the amount of software written can be useless later because of changing project requirements). This is where we should try to build reusable software which can be built from the same project in very minimal time. So next time when you are building a project if you are planning to hard code some project-specific thing. Wait a minute and think about how you can improve it further, and avoid tightly coupled dependencies. One of the main things which are pushing companies to be overtaken by Amazons in the industry is legacy software."
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html#breadth-wise-knowledge-of-technical-stack",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html#breadth-wise-knowledge-of-technical-stack",
    "title": "5 qualities I learned from great software developers",
    "section": "Breadth wise knowledge of Technical stack",
    "text": "Breadth wise knowledge of Technical stack\nThey care deeply about the various software required for building products. They have a fundamental knowledge of why each technical stack is required for various programming purposes. This knowledge helps in process of designing systems in a way better way of developing the product with good architecture and care about reusability.\nAlso, itâ€™s very important to know the depth of the programming language. It is recommended to learn the language you use in work in-depth, to the very bottom in this advice by Georgely Orsz."
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html#get-the-shit-done",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html#get-the-shit-done",
    "title": "5 qualities I learned from great software developers",
    "section": "GET the Shit Done",
    "text": "GET the Shit Done\nI remember this quote hiring rounds from one of the prominent company which is a leading social media for working professionals with this as their main tagline:\n\nGet the shit doneðŸ’©\n\nWe as programmers need to realize and achieve the task required for the product. There may be a hard task that may seem tedious and difficult at first. But start working on it early and donâ€™t make it slip deadlines. Any task is doable once we put the necessary effort into it.\n [Image from Pexels]"
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html#deep-understanding-of-their-programming-stack",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html#deep-understanding-of-their-programming-stack",
    "title": "5 qualities I learned from great software developers",
    "section": "Deep understanding of their programming stack",
    "text": "Deep understanding of their programming stack\nItâ€™s very important to understand the tech stack you work in depth. When you are working with Javascript, good programmers use type hints provided by typescript to reduce the errors in the codebase. Having an A to Z knowledge of one language can be useful.\nAs we express our fluency with words or pronunciation when we are communicating, this is how we developers show our knowledge. If you have the required knowledge, it just shines brightly and everyone can easily understand it. Thatâ€™s why itâ€™s important to learn your programming stack thoroughly."
  },
  {
    "objectID": "posts/2021/2021-03-21-qualities_software_developers.html#humility",
    "href": "posts/2021/2021-03-21-qualities_software_developers.html#humility",
    "title": "5 qualities I learned from great software developers",
    "section": "Humility",
    "text": "Humility\nMost of the best folks I met are humble and are always willing to help. They have a life outside of this even though they care about their craft so much as to work even on weekends out of work to various side projects. They donâ€™t brag about their achievements and like to help things to folks who are clear with it.\nItâ€™s also important to realize the review comments developers pass has nothing to do with you, but it is with your codebase and code quality. Personally, I need to work gaining more depth on python like the async function, generators, and revising the knowledge gained with the book High-performance Computing in practice in the coming days.\nAlso in the end of each weekly blog posts, I am going to start sharing three links for this week onwards.\nThree Links for this week ðŸ‘‰:\n\nHow Netflix recommendation engines work for Learning a Personalized home page\nTop 5 Lessons learned from being Netflix CEO - Reed Hastings\nA long chat about git and few of the common commands like commit, stash and their git mental model with Nabarun, Sayan and Jason"
  },
  {
    "objectID": "posts/2021/2021-07-15-Resize-images.html",
    "href": "posts/2021/2021-07-15-Resize-images.html",
    "title": "Resizing an image to smaller size",
    "section": "",
    "text": "Resizing images to smaller dimension to satisfy the file size or height/width requirements is something we have faced multiple times. Most of the online tools which I previously used were full of advertisments, and even required to login which is something I hated a lot. Today I learned to do this tedious job with python and itâ€™s pretty easy also.\nLibrary used: Pillow\nimport os, sys\nfrom PIL import Image\n\nsize = (720, 720)\n\nfor infile in sys.argv[1:]:\n    outfile = os.path.splitext(infile)[0] + f\"{size[0]}.jpg\"\n    if infile != outfile:\n        try:\n            with Image.open(infile) as im:\n                im.thumbnail(size)\n                im.save(outfile, \"JPEG\")\n        except OSError:\n            print(\"cannot create thumbnail for\", infile)\n            \nSave file as resize.py and run the script with:\n\npython3 resize.py image.jpg\n\nThanks to Jishnu and Rajeesh, I learned there is another tool called ImageMagick. It can also resize images, along with a ton more other cool stuff.\n\n\nTry image magic, I think it is built into most distros (if not, easy to install via system package manager) and supports more than 200 formats. Easy to use as well.\n\nâ€” Jishnu (@jishnu7) July 20, 2021\n\n\nTo Install ImageMagick in your debian based distro:\nsudo apt install imagemagick\nTo resize images with height, width use the option:\nconvert -resize 300x720 image.jpg resized_image.jpg\nYou can resize also based on the quality as follows:\nconvert -quality 50 image.jpg resized_image.jpg"
  },
  {
    "objectID": "posts/2021/2021-11-11-Learning_Tamil.html",
    "href": "posts/2021/2021-11-11-Learning_Tamil.html",
    "title": "Learning Tamil and weird things in Malayalam",
    "section": "",
    "text": "Thanks to Sankar M, I have been learning a bit of Tamil.\n\nA few words I learned so far are:\nHow are you? - Ninkal Yeppadi irikkengal\nI am fine - Nannayi irrikenga\nbrother - Anna\nSister - Akka\nReally good(adipoli) - Sema \nKallai Vanakkam - Good Morning\nNandri  - Thank you\nRomba / Nerya - More\nnal - days\nromba nal achu eppadi irikkenga -\nIn malayalam, we have different slangs and itâ€™s obviously difficult to understand few of words like Mema - which means mothers sister in Kottayam side. In my place, the same words we use Kunnaunty, Vellaunty etc.\nI will be adding more words, as I learn more tamil.\n\nSigning off,\n\nKurian Benoy"
  },
  {
    "objectID": "posts/2021/2021-07-21-regex.html",
    "href": "posts/2021/2021-07-21-regex.html",
    "title": "Regular Expressions in Python",
    "section": "",
    "text": "Regex image\n\n\nIn Python, regular expressions is a very powerful gun. Letâ€™s look what these gun can do:. I first learned about regular expressions in Theory Of automate(TOC).\nAccording to wikipedia regext is defined as follows.\n\nA regular expression (shortened as regex or regexp) is a sequence of characters that specifies a search pattern. Usually such patterns are used by string-searching algorithms for â€œfindâ€ or â€œfind and replaceâ€ operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.\n\nThe formal definition of Regular expression as mentioned in Theory of Computer text books is as follows:\nRegular expressions consist of constants, which denote sets of strings, and operator symbols, which denote operations over these sets. The following definition is standard, and found as such in most textbooks on formal language theory.[20][21] Given a finite alphabet Î£, the following constants are defined as regular expressions:\n(empty set) âˆ… denoting the set âˆ…. (empty string) Îµ denoting the set containing only the â€œemptyâ€ string, which has no characters at all. (literal character) a in Î£ denoting the set containing only the character a.\nSince we are learning regex in Python. Let me share two handy resorurces to learn regex:\n\nHow to use regex in Python\nData sheet for regular expressions\n\n\nItâ€™s very useful to search and find things quickly. It consists of various methods like:\n\n\nre.match()\nre.matchall()\n\nhttps://docs.python.org/3/howto/regex.html https://www.infoworld.com/article/3306798/regex-tutorial-matching-sets-of-characters.html https://brilliant.org/practice/regular-expressions/ https://elisonsherton.github.io/fastbook/deep%20learning/2021/07/19/fastbook-week6-session-summary.html\nToday regarding regex, learned about techniques like re.match, re.matchall()\n*, /,/d etc. Practising problems can only make you perfect.\nRegular Expressions is a very useful feature to search and find things quickly. The various methods of regular experessions consists of:\n\nre.match()\nre.matchall()"
  },
  {
    "objectID": "posts/2021/2021-06-05-DoNotHarm.html",
    "href": "posts/2021/2021-06-05-DoNotHarm.html",
    "title": "Do no harm - COC for programmers",
    "section": "",
    "text": "Doctor Attacked\nThe above video shows how a Junior Doctor has been attacked in Assam, India. Itâ€™s very unfortunate to see doctors, attacked like this and we have seen even things worse than this happen to doctors. Generally most of doctors in the critical care units who work are usually Junior Doctors, who have to work long hours to get the initial experience. Yet these Junior doctors are usually at the receiving ends of the blows from relatives of deceased people in most of the cases.\nWhen I watched this video, I was remembered about the oath doctors take and Uncle Bobâ€™s Clean Coder book. The oath is simple:\nDO NO HARM\nThe human body is too complex to understand in itâ€™s entirety and take years of expertise to specialize in something, but doctors still take an oath to do no harm. New viruses can comes with lot of new novelty like Nipah, Covid-19, yet doctors still adhere to there oath of doing no harm.\nI am sharing why Uncle Bobs suggest coders like you and me to take this oath of DO NO HARM. Uncle Bob is sharing the story of one of his software failures:\nNow Uncle Bob says why we should take this simple oath as coders:"
  },
  {
    "objectID": "posts/2021/2021-06-05-DoNotHarm.html#do-no-harm",
    "href": "posts/2021/2021-06-05-DoNotHarm.html#do-no-harm",
    "title": "Do no harm - COC for programmers",
    "section": "Do No Harm",
    "text": "Do No Harm\n\nSo how do we take responsibility? There are some principles. Drawing from the Hippocratic oath may seem arrogant, but what better source is there? And, indeed, doesnâ€™t it make sense that the first responsibility, and first goal, of an aspiring professional is to use his or her powers for good? What harm can a software developer do? From a purely software point of view, he or she can do harm to both the function and structure of the software. Weâ€™ll explore how to avoid doing just that.\n\n\nDo No Harm to Function\n\nClearly, we want our software to work. Indeed, most of us are programmers today because we got something to work once and we want that feeling again. But we arenâ€™t the only ones who want the software to work. Our customers and employers want it to work too. Indeed, they are paying us to create software that works just the way they want it to. We harm the function of our software when we create bugs. Therefore, in order to be professional, we must not create bugs. â€œBut wait!â€ I hear you say. â€œThatâ€™s not reasonable. Software is too complex to create without bugs.\n\n\nOf course you are right. Software is too complex to create without bugs. Unfortunately that doesnâ€™t let you off the hook. The human body is too complex to understand in itâ€™s entirety, but doctors still take an oath to do no harm. If they donâ€™t take themselves off a hook like that, how can we? â€œAre you telling us we must be perfect?â€ Do I hear you object? No, Iâ€™m telling you that you must be responsible for your imperfections. The fact that bugs will certainly occur in your code does not mean you arenâ€™t responsible for them. The fact that the task to write perfect software is virtually impossible does not mean you arenâ€™t responsible for the imperfection. It is the lot of a professional to be accountable for errors even though errors are virtually certain. So, my aspiring professional, the first thing you must practice is apologizing. Apologies are necessary, but insufficient. You cannot simply keep making the same errors over and over. As you mature in your profession, your error rate should rapidly decrease towards the asymptote of zero. It wonâ€™t ever get to zero, but it is your responsibility to get as close as possible to it.\n\n:wq"
  },
  {
    "objectID": "posts/2021/2021-08-07-get_image_files.html",
    "href": "posts/2021/2021-08-07-get_image_files.html",
    "title": "Understanding get_image_files in fastai",
    "section": "",
    "text": "In fastai library we use Datablocks like the below example for loading datasets, and to train models. The below code is DataBlock, which is used to load a dataset of various types of bears to split into train and validation datasets along and to resize images to size 128*128. For a detailed explanation, check on From Data to DataLoaders section in Chapter 2 of Fastbook.\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nIn this Datablock get_items, we are using the get_image_files to load the images. I was curious how to see how get_image_files worked under the hood to return all the image files in a dataset. As Jeremy always suggests, I started looking into source code by handy question mark functionality in Jupyter Notebooks. The source code for get_image_files can be found in fastai repo here. The source code for get_image_files function is:\n{% highlight python linenos %} def get_image_files(path, recurse=True, folders=None): â€œGet image files in path recursively, only in folders, if specified.â€ return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders) {% endhighlight %}\nYou can see itâ€™s expecting the path to the folder where files are present in the image folder. Also the function signature, consists of recurse=True and folder=None by default.\nYou can see get_image_files function is calling get_files(path, extensions=image_extensions, recurse=recurse, folders=folders) on passing with extensions set as image_extensions.\n\nWhat is image_extensions doing?\n\n{% highlight python linenos %} import mimetypes\nimage_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith(â€˜image/â€™)) {% endhighlight %}\nThe image extensions is just a variable returning a set of images from the mimetypes, which is part of Python standard library to map filenames to MIME types. Letâ€™s see image_extensions output to see whole set of image type extensions.\n>>> image_extensions\n{'.jpg', '.svg', '.pgm', '.png', '.xbm', '.jpe', '.pbm', '.pnm', '.rgb', '.tiff', '.xpm', '.jpeg', '.ras', '.ico', '.tif', '.ppm', '.xwd', '.gif', '.bmp', '.ief'}\nmimetypes.types_map.items() returns a dictionary items. It consists of key, value pair and we are selecting value pairs starting with the word image/ to return a set of image_extensions as shown in the above output.\nNow letâ€™s look at get_files function, which returns a list of files, based on extensions passed, folders. Letâ€™s look into the source code of get_files(path, extensions=None, recurse=True, folders=None, followlinks=True function as well:\n{% highlight python linenos %} def get_files(path, extensions=None, recurse=True, folders=None, followlinks=True): â€œGet all the files in path with optional extensions, optionally with recurse, only in folders, if specified.â€ path = Path(path) folders=L(folders) extensions = setify(extensions) extensions = {e.lower() for e in extensions} if recurse: res = [] for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames) if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders] else: d[:] = [o for o in d if not o.startswith(â€˜.â€™)] if len(folders) !=0 and i==0 and â€˜.â€™ not in folders: continue res += _get_files(p, f, extensions) else: f = [o.name for o in os.scandir(path) if o.is_file()] res = _get_files(path, f, extensions) return L(res) {% endhighlight %}\nLetâ€™s understand what the function get_files(path, extensions=None, recurse=True, folders=None, followlinks=True) is doing line by line.Letâ€™s look at the first few lines of code.\nfrom fastcore.all import L, setify\nfrom pathlib import Path\n\ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n    path = Path(path)\n    folders=L(folders)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\nWe are converting the path provided to us into a Pathlib object, and folders are converted to a special Python-like list called (L) based on the fastcore library. The extensions are converted to a set if itâ€™s being passed as a list, range, string etc. using setify. All the extensions are converted to lower case characters if any extension is in upper case. To read more about the setify function check the fastcore docs.\nIf function definition, we have by default passed recurse=True. If itâ€™s True, it goes through all the files in the File path we have passed as well as going inside various folders inside the File Path recursively. Else if recurse=False, we just go through all files in the File Path we have passed without going inside various folders.\nimport os\n\ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n    ...\n    ...\n    if recurse:\n        ...\n        ...\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        res = _get_files(path, f, extensions)\nFor the sake of understanding, letâ€™s take an example a .git directory, with the following file structure.\n\n\n\nimage\n\n\nos.scandir returns an iterator of Directory objects. In Python os module, there is an os.listdir(path='.') which does the same functionality as scandir. Yet scandir gives a better performance for most of common use cases. [1]\nf = [o.name for o in os.scandir(path) if o.is_file()]\nIt returns a list of file extensions as shown below with list comprehensions, where is_file() returns, if itâ€™s a file or whether itâ€™s pointing to a directory with followlinks.\n>>> path=Path('.git')\n>>> [o.name for o in os.scandir(path) if o.is_file()]\n['index', 'HEAD', 'packed-refs', 'config', 'description']\nIf recurse=True, it goes through all the directories and works on files recursively. Letâ€™s look at the sources code and try to understand more.\n{% highlight python linenos %} import os\ndef get_files(path, extensions=None, recurse=True, folders=None, followlinks=True): â€¦ â€¦ if recurse: res = [] for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames) if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders] else: d[:] = [o for o in d if not o.startswith(â€˜.â€™)] if len(folders) !=0 and i==0 and â€˜.â€™ not in folders: continue res += _get_files(p, f, extensions) else: f = [o.name for o in os.scandir(path) if o.is_file()] res = _get_files(path, f, extensions) {% endhighlight %}\nI would highly recommend the functionality of os.walk by checking this article. You can see that on iterating through os.walk(), we can get the directory path, and associate file path as a list. This is being passed to _get_files(p, f, extension) function.\n>>> for i,(p,d,f) in enumerate(os.walk(path, followlinks=True)): # returns (dirpath, dirnames, filenames):\n...     print(p, f)\n...\n. ['index', 'HEAD', 'packed-refs', 'config', 'description', '.env']\n./.gitignore []\n./branches []\n./hooks ['fsmonitor-watchman.sample', 'update.sample', 'pre-applypatch.sample', 'pre-push.sample', 'pre-receive.sample', 'applypatch-msg.sample', 'pre-commit.sample', 'prepare-commit-msg.sample', 'commit-msg.sample', 'post-update.sample', 'pre-rebase.sample']\n./objects []\n./objects/pack ['pack-0ae71ead7e875289ae1c9a4b14ca65dbb7a9fc83.pack', 'pack-0ae71ead7e875289ae1c9a4b14ca65dbb7a9fc83.idx']\n./objects/info []\n./info ['exclude']\n./refs []\n./refs/tags []\n./refs/remotes []\n./refs/remotes/origin ['HEAD']\n./refs/heads ['master']\n./logs ['HEAD']\n./logs/refs []\n./logs/refs/remotes []\n./logs/refs/remotes/origin ['HEAD']\n./logs/refs/heads ['master']\nJust to summarise how the get_files function is working it will be useful to look at the below illustration:\n\n\n\nWhatsApp Image 2021-08-09 at 8 12 10 AM\n\n\nWhen recurse=False, for path bears. It returns just returns file README excluding (.gitignore) and directories.\n>>> get_files(path, recurse=False)\n['README']\nWhile recurse=True, for path bears. It returns all valid files inside the root directory as well as in folders such grizzly, black, teddy, details, folder etc.\nAfter that, itâ€™s passed to _get_files function, which returns the list of filenames to a list of pathlib Path of various filenames.\n{% highlight python linenos %} def _get_files(p, fs, extensions=None): p = Path(p) res = [p/f for f in fs if not f.startswith(â€˜.â€™) and ((not extensions) or fâ€™.{f.split(â€œ.â€)[-1].lower()}â€™ in extensions)] return res {% endhighlight %}\nfs, the list of files returned. We are not passing files that are starting with . like .gitignore or .env as itâ€™s not usually very useful for our dataset to get as files. Also, itâ€™s not returning file extensions passed or f'.{f.split(\".\")[-1].lower()}' in extensions.\nres on passing p/f for the list of files will become a list of paths as shown in the below result. Transforming from a list of file names, we are transforming it to a list of Pathlib module Path, pointing to various filenames.\n>> get_files(Path('.'))\n[Path('index'), Path('HEAD'), Path('packed-refs'), Path('config'), Path('description'), Path('hooks/fsmonitor-watchman.sample'), Path('hooks/update.sample'), Path('hooks/pre-applypatch.sample'), Path('hooks/pre-push.sample'), Path('hooks/pre-receive.sample'), Path('hooks/applypatch-msg.sample'), Path('hooks/pre-commit.sample'), Path('hooks/prepare-commit-msg.sample'), Path('hooks/commit-msg.sample'), Path('hooks/post-update.sample'), Path('hooks/pre-rebase.sample'), Path('objects/pack/pack-0ae71ead7e875289ae1c9a4b14ca65dbb7a9fc83.pack'), Path('objects/pack/pack-0ae71ead7e875289ae1c9a4b14ca65dbb7a9fc83.idx'), Path('info/exclude'), Path('refs/remotes/origin/HEAD'), Path('refs/heads/master'), Path('logs/HEAD'), Path('logs/refs/remotes/origin/HEAD'), Path('logs/refs/heads/master')]\nThis is how the get_image_files, returns a L object based on fastcore for any object. For the BIWI Dataset, the output of get_image_files and get_files is as following:\n\n\n\nimage\n\n\n\nConclusion\nI hope with this blog post, you now have understood how get_image_files, fetch the list of images under the hood by looking into the source code.\nIn case, if I have missed something or to provide feedback, please feel free to reach out to me @kurianbenoy2.\n\n\nReferences\n[1] Fastai source code\n[2] Python os module\n[3] Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD"
  },
  {
    "objectID": "posts/2021/2021-09-01-fastgroup-10.html",
    "href": "posts/2021/2021-09-01-fastgroup-10.html",
    "title": "Week 12 - What is Convolution? Looking into CNNs",
    "section": "",
    "text": "The first section covered about filters. We used kernel which is a little matrix that is applied to the image which is used as a feature engineering step.\nWe have defined various version of filters, which can be used to extract features from the image like filters based on the top_edge:\ntop_edge = tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]).float()\nThe above diagram shows how a three is actually represented actually in itâ€™s pixel format.\nTo retain the orginal size, we usally use the following:\nTo understand the concept of filters, and how different convolutions can be applied to notebook. I highly recommend to check out Ravi Mashruâ€™s blogpost.\nThe general formula for output shape of a convolutional arthimetic is as shown:\nWhen applied to an image with full padding the result is as follows:\nUnderstanding the input shape and output shape and why itâ€™s comes that way is a challenging part of Pytorch.\nThen we are looking at the applying convolutions in pytorch. The edges of each shape of 28*28 size images when applied to a filter are returning 64 batches of 4 filters and height and weight is 26*26 as it loses 2 pixels.\nIf you look at the reference link, you will realize how actually the matrix multiplication in our convolution is actually happening.\nAlso another interesting update for this week is the plan to create a movie recommender. Ravi Mashru and I had a call to kick off the project, which we are planning to complete by investing 2-4 hours per week. More about it can be found here."
  },
  {
    "objectID": "posts/2021/2021-09-01-fastgroup-10.html#references",
    "href": "posts/2021/2021-09-01-fastgroup-10.html#references",
    "title": "Week 12 - What is Convolution? Looking into CNNs",
    "section": "References",
    "text": "References\n\nhttps://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c"
  },
  {
    "objectID": "posts/2021/2021-01-01-booksread.html",
    "href": "posts/2021/2021-01-01-booksread.html",
    "title": "Books read in 2021",
    "section": "",
    "text": "This a continution where, I jot down the books I have read so far:\nThis book talks about the story of a young Muslim girl and captures her drama, with a mix of things which reveal a lot about Muslim lives, Kozhikodden or north kerala malayalam slang. The book is having the flow and the climax is when KunnuPathu realises her boasted about the elephant, but it was just a small fly they owned during their grandpas time. - Pragmatic Programmer : A coding book for you coder, your future colleagues and coderâ€™s in the year 2500 also. - Clean Coder: Thank you Uncle Bob for teaching all these hard earned lessons to make me a professional programmer. - Deep Work: Cal Newport\nCal produced high quality work as a professor in MIT by working deeply and specifically on selected topics. He produced 2 peer journals every year in MIT, completed a postdoc and even got a professorship. When he never mostly worked in weekends, and completed most of work by evenings. 4-5 deep hours is all you need."
  },
  {
    "objectID": "posts/2021/2021-01-01-booksread.html#currently-reading",
    "href": "posts/2021/2021-01-01-booksread.html#currently-reading",
    "title": "Books read in 2021",
    "section": "Currently Reading",
    "text": "Currently Reading\n\nHigh Performance Python Computing (rereading)\nintroduction to deep learning, fastai, and PyTorch : Jeremy Howards & Sylvain Gugger (reading along with Study group for FastBook)"
  },
  {
    "objectID": "posts/2021/2021-01-01-booksread.html#wish-list",
    "href": "posts/2021/2021-01-01-booksread.html#wish-list",
    "title": "Books read in 2021",
    "section": "Wish List",
    "text": "Wish List\n\nEPI\nHow to win friends and Influence People\nHigh sschool English grammar and Composition\nGood English - How to writie it\nMake it Stick: The Science of Successful Learning https://www.goodreads.com/book/show/18770267-make-it-stick\nThe Juggling Act: Bringing Balance to Your Faith, Family, and Work (Pat Gelsinger)"
  },
  {
    "objectID": "posts/2021/2021-03-01-WFH6.html",
    "href": "posts/2021/2021-03-01-WFH6.html",
    "title": "ðŸ  WFH for 6 months",
    "section": "",
    "text": "Itâ€™s been 6 months since I joined AOT Technologies as a SE-Data scientist and due to Covid -19 pandemic, on the day of joining itself, our director said this year you wonâ€™t probably need to come to the office. I have been a big fan of thing Work from Home for a long time, and during my college days, it was a big dream of mine to work in a remote company. It felt nearly impossible then, but then Covid-19 happened.\nI first heard of remote work from Balu chettai whom I met during FOSSMEET 2018, NIT Calicut. Balasankar C has been working in Gitlab as a Distribution Engineer from 2016 onwards. He was working in a 100% remote company from the beginning of his career. It was during MEC.conf after his talk on how it felt working remotely. When he watered the plants at his house, the public perspective of his neighbours was that he was jobless, at the same time he had a great job, while the public perspective was that.\nIt was possible to work as remote teams even before the pandemic occurred. There were a few companies like Gitlab, Zapier, few teams with Google, Microsoft etc. embracing 100% remote work even before the pandemic began and a lot of companies supported work from the office if employees preferred that.\nOne of the best things about Covid-19 was it accelerated the whole scenario of work from home by at least 10 years. Now when I say, I work remotely from home, it is not anything ridiculous. A lot of people reply telling that a few of their neighbours who work in TCS, Infosys is also working remotely. Companies like TCS are embracing WFH and are aiming by 2025 to have just 25% of their workforce in office.\nI am writing this article to have more clarity for myself to improve my remote work experience and reflect how the past few months to improve my workflow better."
  },
  {
    "objectID": "posts/2021/2021-03-01-WFH6.html#hits",
    "href": "posts/2021/2021-03-01-WFH6.html#hits",
    "title": "ðŸ  WFH for 6 months",
    "section": "Hits ðŸŽ¯",
    "text": "Hits ðŸŽ¯\nLess Travel\nSince my job, was in Thiruvananthapuram, it would have meant I should have travelled and stayed in a new city. If that was the case, it would have required me to travel weekly in Vanchinad express, every morning on Monday and return on Friday. I would have stayed with my aunt which takes daily about 1 hour 30 minutes in commute. So with a remote job, now I have more time to work even more productively.\nSave Cost One of the best things is less cost of living, I am actually living in my house which incurs to me almost zero costs. Eating healthy food is way better than eating food from the hotel almost every day. This has helped me work on pretty cool things. And as always it is the skills that help us pay the bills even while working from home(Thank you Tinkerhub).\nMore time for health and side projects\nI have always slacked on health, yet remote work has helped me find time to work out and lose some weight. It helps me to find time to walk every day. All this has helped improve my health over the past few months.\nFlexibility\nRemote offers great flexibility even while working for a fixed period of time. This flexibility is going to be very crucial in the upcoming years as more and more companies are moving to lesser work hours. I am seeing in the coming years we will be having more companies which offer 4 workdays instead of the current five and even 20 hour work weeks or less like how it is in companies like GumRoad.\n\nMisses\nDaniel H Pink in his book Drive: The truth behind motivation has a fantastic quote:\n\nBeing Professional is doing things you love to do on days you donâ€™t feel like doing\n\nA lot of experienced people who have been working remotely for years have commented that the current remote work is actually not similar to real remote work as that is not how it is actually supposed to be due to pandemic.\nLess contact with people\nThe number of real-world people we met during the pandemic is very few. That was a big missing, even though I prefer remote work even it the future? (I am leaving a question mark because I have never worked in an office ever and the decision may be reversed in near future). I miss the conferences like Pycon India(even though 2021 Pycon will also be held online) which I loved so much to attend or the possibility of meeting colleagues somewhere.\nLetâ€™s see how will WFH be in coming few months!\n\nKurian Benoy"
  },
  {
    "objectID": "posts/2021/2021-04-07-statictyping.html",
    "href": "posts/2021/2021-04-07-statictyping.html",
    "title": "Static type ecosystem - python types & Typescript",
    "section": "",
    "text": "A prompt generated by greg rutkowski for showing Static type ecosystem\n\n\nMany of the popular languages like Javascript and Python which are being used by millions of developers are dynamically typed language. That is the code is check for correctness, type of variables only during the run times. This makes millions of developers prone to writing code with potential errors.\nMost languages run dynamically and can point to just a few syntax errors. Yet almost most of the errors which programmers face are probably type errors or run time errors. This is where the need for static code analysis comes from.\nThis is where tools for static code analysis becomes important and necessary. Itâ€™s a necessary toolkit for any amateur programmer and helps in providing strict checking and help write programmers write fewer buggy code. In both Python and JavaScript, there are a few tools that help in providing static code analysis.\nIn the Python world, the standard library comes with typing modules and type hints. Python types are implemented as a series of PEP(Python enhancement proposals) and in the latest python version i.e.Â 3.9 it provides run time support for Python typing which doesnâ€™t require importing typing every time. With the latest PEP for type hinting generics in standard collections, you can now use built-in collection types such as list and dict as generic types instead of importing the corresponding typing module.\ndef greet_all(names: list[str]) -> None:\n    for name in names:\n        print(\"Hello\", name)\nCheck out more in the below links ðŸ‘‡:\n\nhttps://docs.python.org/3/library/typing.html\nhttps://youtu.be/H5CnZQDKfhU\nhttps://docs.python.org/3/whatsnew/3.9.html\n\nFor the JavaScript world, there is Typescript which is a very mature ecosystem. It is being built by Microsoft and is a static type-checker that provides greater clarity on top of things build on JavaScript. When using typescript, you can find potential bugs and support for almost 90% of common types used. For the rest of the cases, you can create interfaces to write associate types.\nI will highly recommend you to check one of the Typescript get started guides and start reading the Typescript handbook as well. Itâ€™s widely adopted in the JavaScript world and is used in various companies. Check out the below links ðŸ‘‡:\n\nhttps://www.typescriptlang.org/docs/handbook/intro.html#get-started\nhttps://www.typescriptlang.org/docs/handbook/intro.html\nhttps://www.typescriptlang.org/\n\nThree Links for this week ðŸ‘‰:\n\nArya provides tips about Understanding your parents and making them aware of your decisions\nTalkPython host, Michael Kennedy, and Eugene have a slightly more philosophical chat on the latest episode where they discussed about life lessons from machine learning\nSusanshu encourages us to say yes to more scary decisions"
  },
  {
    "objectID": "posts/2021/2021-06-20-TinkerHub_ml.html",
    "href": "posts/2021/2021-06-20-TinkerHub_ml.html",
    "title": "Mentoring ML students in Tinkerhub",
    "section": "",
    "text": "Hello worldâ€¦\nWrite a blogpost similar to this : https://eugeneyan.com/writing/how-to-win-data-hackathon/#undefined\nTinkerHub recently conducted a Build From Home initiative to encourage college/school going students to build from selected project. I got an opportunity to be a BFH sponsor for Machine learning, and I am simply amazed by the their work. BFH Makers were asked to do everything from collecting data, to training ML models and doing experiments, building a webapp/api to do the inference of results. Some team have even gone ahead and even deployed it in a span of 7-9 days. I would like to congratulate all the future data scientists for doing great work and waiting to see more of your awesome projects. Also I would like to thank @abid, @gopi chettai, @kurianjacob, @Tinkerhub team for this awesome initiative and giving me an opportunity to be a mentor."
  },
  {
    "objectID": "posts/2021/2021-02-04-Redash_parameters.html",
    "href": "posts/2021/2021-02-04-Redash_parameters.html",
    "title": "What are Parameters and filters in Redash?",
    "section": "",
    "text": "Letâ€™s start with what are parameters in Redash. Parameters are useful for providing user interactivity for analytics, where they can pass interactive queries. Usually, we do most of the analytics based on certain predefined values while parameters allow users to run queries they want to generate insights on months, time ranges or values they want. Redash supports queries with parameters in both SQL, MongoDB and even more as Redash supports 30+ Databases. To use parameters just use {{ }} and checkout the guide for more details.\nThe best way to write Redash queries with parameters is first to create a simple hardcoded query with value and generalise the query with the input you want to have.\neg: select * from text where color = â€˜Redâ€™\nselect * from text where color == â€œ{{ serach_term }}â€\nThere are about ten types of query parameters ranging from Text, Number to be pass as input, to options like query dropdown list, dates and date range as parameters.\nFilters are for limiting the results queries and display just the subset of rows having a particular value. Suppose you have a data source like:\nIf you want to filter query using a brand with : operator. For brand Britannica, there are two rows associated, while for sunflower there is just 1 row to be shown.\nAs a side note, Datasette recently implemented this parameter feature. Datasette is something I wish to explore for a long time now."
  },
  {
    "objectID": "posts/2021/2021-02-04-Redash_parameters.html#references",
    "href": "posts/2021/2021-02-04-Redash_parameters.html#references",
    "title": "What are Parameters and filters in Redash?",
    "section": "References",
    "text": "References\n\nhttps://redash.io/help/user-guide/querying/query-filters\nhttps://redash.io/help/user-guide/querying/query-parameters\nhttps://simonwillison.net/2020/Nov/14/personal-data-warehouses/"
  },
  {
    "objectID": "posts/2021/2021-07-18-fastgroup-6.html",
    "href": "posts/2021/2021-07-18-fastgroup-6.html",
    "title": "Log6- Learning FastBook along with Study Group",
    "section": "",
    "text": "So as always, letâ€™s start with a quick recap of what was covered during the session:\n\nTLDR: Covered Chapter 5 of Deep Learning for Coders book, includes secret recipes to build a great ML model.\n\n\nWe started with PETs dataset. This time we were classifying Breed of Cat/Dog using fastai image classifier.\nThe dataset followed a particular heuristics, which represented Species with the capital case as Cats and smaller case as Dogs. For the breeds also a similar pattern is there:\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/Maine_Coon_157.jpg') - The breed can be obtained using the regex patterns. Itâ€™s recommended to read a regex tutorial and solve it with a regex problem set. re.findall(r''(.+)_\\d+.jpg) - Most functions and methods in fastai return a special collection class called L(part of fastai core library) - Datablock used for loading pets breed classifier\npets = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                 get_items=get_image_files,\n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75)\n)\ndls = pets.dataloaders(path/\"images\")\n\nWhat is Presizing?\n\nItâ€™s a technique in which we transform input image into a larger size image, and then use augmentation techniques like Random cropping to get the full picture of what we are trying to classify.\nUsually for transforming images to larger sizes like Resize operation we use a CPU(Central Processing Unit). Then process of augmentation techniques like cropping, RandomResizing is usually done with a GPU(Graphics process units) because it requires uniform size images.\n\n\n\n\nimage\n\n\n\nIn practice, according to Aman Arora we spend a lot of time loading data correctly when working with a new dataset. This is where some errors come.\nWhen errors come in our data block, use the summary method in DataBlock\n\n\n\n\nimage\n\n\n\nNow when training our model, across a few epochs(doing a complete pass of data). The learn.fine_tune() method shows average loss over the training dataset and loss of validation set. What is this loss function used, is it the loss function we learned in chapter 4?\n\n\n\n\nimage\n\n\n\nBy default, fastai chooses the appropriate loss function based on Datablock type. Generally for an image dataset, with a categorical outcome we usually use cross-entropy loss\n\n\n\nWhat is Cross Entropy Loss?\nLetâ€™s look it step by step. Letâ€™s first view the activations and labels of our data loader\n\n\n\nimage\n\n\n\nWe can see our pets are converted into associated tensor values representing the category into 0-37 values in tensor.\nThe prediction probabilities sum all add up to 1\n\nWhat is Softmax function?\n\nWhen we were classifying 3s and 7s in MNIST dataset, it was a binary classification problem, with the target being a boolean value. We used the sigmoid function\nCross-Entropy loss is being used because of two benefits:\n\n\nIt works when our dependant variables(like classifying pet breeds) with more than two categories\nIt results in a faster and reliable training\n\n\nNow for classifying 3s and 7s, it is reliable for a set of random activations to be substracted with the difference of activation for three and seven. After calculating the sigmoid function of their difference.\n\n(acts[:, 0] - acts[:, 1]).sigmoid()\n\nThe softmax function is represented in the formula as below. Obviously there is a PyTorch function for it\n\ndef softmax(x):\n  return (exp(x) /exp(x).sum(dim=1, keepdim=True))\n\n\n\nimage\n\n\nWhat is log-likelihood loss\n\nWe need to calculate the loss for each no like mnist_loss(target=1, 1-inputs). Yet for multiple categories, we need a better method\nFor this, we use the log-likelihood loss method. When considering things in log scale, a 99.99% and 99.9% is usually 10 times larger, even though in absolute terms it seems small.\n\n\n\n\nimage\n\n\nCombining\n\n\n\nimage\n\n\n\nSoftmax + Negative Log Likelihood = Cross Entropy Loss\nMost of the time, we calculate with torch methods like nn.CrossEntropyLoss(reduction='none')(acts, targ).\n\n\n\nModel Interpretation result\n\nThe wrong results, and where our model has been wrong canâ€™t be often spotted merely with help of metrics like accuracy.\nSo we use a confusion matrix and plot the function with top losses.\n\n\n\n\nimage\n\n\n\n\nLearning rate finder and freezing pre-trained layers\n\nUsing a learning rate finder developed by Leslie Smith, we can find the minimum step and maximum steep point of learning rate\nTo find the appropriate learning rate, using at some point of the middle of lr_rate_finder() is a food idea. But always remember to use logarithmic scale even here to calculate learning rate finder\nWhen we are using models with transfer learning, they are not trained for the specific model which we are trying to classify.\nFor this, we freeze the last layers and train again\n\n\n\n\nimage\n\n\n\nFastai fine_tune method under the hood:\n\n\nTrains randomly for added layers initially for certain epochs\nUnfreeze all the layers, then train again\n\n\n\nDiscriminative learning rate, and rest of portions\n\nFirst, train the model with a slow learning rate, and then later train faster. This is the fundamental premise\nEarly stopping is a not good method as mentioned in the book below\n\n\nBefore the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the models saved in each epoch. This is known as early stopping. However, this is very unlikely to give you the best answer, because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfitted, what you should do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.\n\n\nUsing lr_max, with slice method to train the last layers with a faster speed\nWe can improve models with deeper architecture like Resnet 34, 50, 101, 152 etc. There are new techniques now like EfficentNet, Transformers, which can give a better accuracy now.\nThen itâ€™s a good idea to train with half-precision floating points, which speeds up the training process considerably\n\n\n\n\nimage\n\n\n\n\n\nIMAGE ALT TEXT\n\n\nAlso at the end of the session, @Aman Arora shared four blog post ideas:\n\nWrite about what untar_data method is doing under the hood?\nWrite about what is fastai foundation class L is doing. What are the unique features of this special list part of fastai core library?\nWrite about regex which is a pretty useful library, and write on what all the cool things you can do with it.\nWrite about cross_entropy_loss .\n\nMy work logs, trying out things mentioned in Chapter 5 can be found in this Kaggle notebook.\nI also got a chance to try out the fastai tool for graphic visualisation fastdott, which is pretty cool.\n\n\n\nimage\n\n\nThanks everyone for reading ðŸ™\n\n\nI have been attending FastBook reading group sessions hosted by wonderful @amaarora and @weights_biases team. This week we dived into some fastai techniques to improving accuracy and dived deep into cross entropy loss.https://t.co/ElnfdAoqNE\n\nâ€” Kurian Benoy (@kurianbenoy2) July 21, 2021"
  },
  {
    "objectID": "posts/2021/2021-04-24-Few-Vue-Concepts.html",
    "href": "posts/2021/2021-04-24-Few-Vue-Concepts.html",
    "title": "Few Vue concepts",
    "section": "",
    "text": "Wish me luck in diving more into Vue\n\nVue Event bus: Just another way of propating Data from Child component to parent component\n\n\nEvent Bus- tutorial\n\n\nVue Emit:\n\n\nVue Emit- comprehensive guide\nJust another tutorial on Emit\n\n\nIn any framework split as much small components as possible\nMixins - Another interesting concept to write more reusable code\n\n\nMore about mixins in Vue classbased methods\n\nLink for this week -> :\n\nI am sharing a few best practises to improve Javasript code quality which is written by the awesome Deep source team."
  },
  {
    "objectID": "posts/2021/2021-08-22-CStrings.html",
    "href": "posts/2021/2021-08-22-CStrings.html",
    "title": "Inputting String in C programming language",
    "section": "",
    "text": "In C programming language, the strings doesnâ€™t have a special data type on itâ€™s own. So itâ€™s usually represented with characters itself. Usually an array of characters is used to represent characters like this:\nint char[1000];\nCharacters can be read by variety of methods like:\n\nscanf(â€œ%sâ€, char);\n\nIt can read a single string, but reading strings with spaces is not supported by scanf operators\n\nThen there is getline() function which is defined in C programming lanaguage book. Itâ€™s actually part of the build in library.\n\n#include <stdio.h>\n#include <string.h>\n\nint main()\n{\n  int bytes_read;\n  int nbytes = 100;\n  char *my_string;\n\n  puts (\"Please enter a line of text.\");\n\n  /* These 2 lines are the heart of the program. */\n  my_string = (char *) malloc (nbytes + 1);\n  bytes_read = getline (&my_string, &nbytes, stdin);\n\n  if (bytes_read == -1)\n    {\n      puts (\"ERROR!\");\n    }\n  else\n    {\n      puts (\"You typed:\");\n      puts (my_string);\n      printf(\"%d\", strlen(my_string));\n    }\n\n  return 0;\n}\n\n\nThere is gets() functions which is generally considered as unsafe\nfgets() is more safe as it provides a bounded input\n\nfgets(string, 1000, stdin);\nFor more details check this stackoverflow answer\nAs a bonus, I am sharing how to find Pangram of a given string in C\n{% highlight linenos %} #include<stdio.h> #include<string.h> #define MAX_LIMIT 1000\nint main() { char string[MAX_LIMIT]; int character_hash[26]={0}; int pancount = 0;\nfgets(string, 1000, stdin);\n\n\nfor(int i=0; i<strlen(string); i++) {\n    if('a'<=string[i] && string[i]<='z')  {\n        character_hash[string[i]-'a'] += 1;\n    }\n    else if('A'<=string[i] && string[i]<='Z') {\n        character_hash[string[i]-'A'] += 1;\n    }\n}\n\nfor(int i=0;i<26;i++){\n    if(character_hash[i]==0) {\n        pancount=1;   \n    }\n}\n\nif(pancount==0) {\n    printf(\"Pangram\");\n}\nelse if(pancount==1) {\n    printf(\"Not Pangram\\n\");\n    for(int i=0;i<26;i++) {\n        if(character_hash[i]==0) {\n        printf(\"%c \", i+'a'); }\n    }\n}\n}\n{% endhighlight %}"
  },
  {
    "objectID": "posts/2021/2021-07-25-jspackagessize.html",
    "href": "posts/2021/2021-07-25-jspackagessize.html",
    "title": "Why should NPM packages be as small as possible?",
    "section": "",
    "text": "Most of the popular NPM packages are very small in size, as itâ€™s a crucial thing that affects the npm packages. Letâ€™s look at why itâ€™s a crucial thing:\n\n\n\nnpm\n\n\nOne is the performance factor of the applications you are building. Most of the applications you are working on are to be highly performant, and should work even in low bandwidth settings.\nImagine if your npm package letâ€™s say is 10 MB in size. I donâ€™t expect 99% of users in India never load your application in web or mobile build with node_modules. But Why?\nItâ€™s because loading your application will easily take 5-10 seconds, while an average application is expected to return a meaningful content load usually in milliseconds. This is why itâ€™s very important to keep your application small in size, and easily usable for a lot of folks. This is why popular NPM packages like React, moment.js are just Kbs in size.\nOne of the most common complaints, I heard from other folks was why to use React for a TodoList App with 3MB size when you can do the same thing easily with vanilla JS + CSS(which is in a few KBs) for a very performant reason. Any frontend framework has its pros and cons.\nAlso, itâ€™s very important to make your packages as small as possible to be highly reliable and cost less cost for computing required for hosting. So one of the simplest things we can do is to reduce dependencies as much as possible.\nThis can be done by not trying to install third-party dependencies when we are working on any application. Even though I agree a lot of stack-overflow answers recommends this approach. Yet generally the loading time is not so slow even though there are dependencies, as most of the common dependencies like JQuery which are commonly used in multiple websites are cached in the usersâ€™ browser itself if they come from CDN.\nAdam Polak recommends how to reduce size of node modules for Cost and performance reasons.\nI would like to thank Faiz Shabeeb for sharing this knowledge."
  },
  {
    "objectID": "posts/2021/2021-08-07-iiitcourse.html",
    "href": "posts/2021/2021-08-07-iiitcourse.html",
    "title": "New adventure begins with MTech",
    "section": "",
    "text": "I will be starting my persual for a 3 year MTech for Working Professionals (AI and Data Science) in IIIT Kottayam. I am looking forward to it and hope it paves a wonderful learning experience for me.\nWish me good luck in my new adventure!"
  },
  {
    "objectID": "posts/2021/2021-05-11-untitled.html",
    "href": "posts/2021/2021-05-11-untitled.html",
    "title": "Untitled",
    "section": "",
    "text": "Hello, I am sharing a bit about the unusual time period we are now in. We are now in a triple lock down situation here in Ernakulam. It is a quite unusual situation now right? - even kerala is crumbling with the fear of COVID-19 and the deaths are rising more and more.\nWhen on April 29, I realised Areeb Jamal was not longer. It send a shock in me. The life is too short, letâ€™s not play for status games and work on cool stuff. The oxygen cylinders are running out of supply.\nIs working on your dream goals worth it? On writing is what I am reading now."
  },
  {
    "objectID": "posts/2021/2021-06-30-fastgroup-3.html",
    "href": "posts/2021/2021-06-30-fastgroup-3.html",
    "title": "Log3- Learning FastBook along with Study Group",
    "section": "",
    "text": "pexels-photo-3280130"
  },
  {
    "objectID": "posts/2021/2021-06-30-fastgroup-3.html#recap-of-week-3",
    "href": "posts/2021/2021-06-30-fastgroup-3.html#recap-of-week-3",
    "title": "Log3- Learning FastBook along with Study Group",
    "section": "Recap of Week 3",
    "text": "Recap of Week 3\n\nOur captain Aman decided to cover only Chapter 2 during the session.\nAs a group, Aman asked all of us to check out the questionnaire at end of each Chapter.\nData loaders is something which passes the function one by one is called function. Fastai has a train data loader and validation data loader \nI was immediately curious about what to do for the test dataset as well. The full discussion thread can be found in this link\nGenerally for any model, there are two terms. One is X which called the independent variable and Y which is the label to predict is called the dependent variable.\n\nLetâ€™s dive deep into DataLoaders. In fastai dataloaders, there are generally four things it does:\n\nGet dependent and Independent variables with help of ImageBlock (blocks=(ImageBlock, CategoryBlock))\nGetting the actuals files for dataset (get_items=get_image_files)\nSplitters, which is used for splitting validation and testing dataset randomly. That splitting is always being done with a random seed (splitter=RandomSplitter(valid_pct=0.2, seed=42))\nGet independent variables(using get_y=parent_label). Note parent_label is something provided by fastai to get the name of the folder. I heard there is something called grandparent splitters\nItem transforms and batch transforms (for making images in the same size, for the neural network to process which is resizing images to 128 px item_tfms=Resize(128)\n\n\n\n\nimage\n\n\nBehind Datablocks and Dataloaders, there is lot of things are still happening ðŸ¤¯\n\nThere are a lot of data augmentation techniques. In the book we discussed methods like padding images, Random Resized Crop, Resize etc. Since there is a lot of black spaced borders, padding methods are not good. All these methods are the suboptimal way of doing things.\nAccording to the book, Random Resized Crop, itâ€™s way better than any other technique as it doesnâ€™t see the full images in each pass. Yet throughout moving through multiple epochs, it sees the entire images\nIn ImageDataLoaders from-name-func is internally calling DataBlock as in chapter 3.\n\nHighlights of the full session can be found by clicking the below video:\n\n\n\nIMAGE ALT TEXT\n\n\nThe homework for this week is to have an online working version of the app to be demoed to people. Aman has said those who do not complete the assignment, need not even come to the sessions.\nAs mentioned in Wayde summary of fastai Chapter 2, the datablock is used in fastai to tell four things always:\n\nWhat kind of data you are working with\nHow to get the Data\nHow to label the Data\nHow to create a validation dataset\n\nNow let me share some of my learnings this week:\nTree classifier webapp\n\n\n\nBinder\n\n\n\nI started working on creating a tree classifier, which classifies based on the images what species of tree it is. The dataset creation step, was easy to do with duck-duck-go method. Yet what was realised was that method is buggy in some cases.\nWhen I collected dataset with just names, like Mango out of domain images like image of mango juice, mangoes, all came. Cleaning of dataset is required for this, also I limited my search query to include tree with each fruit name. The dataset collection notebook can be found here.\nThe online webapp to classify trees I created can be accessed in the Binder link\n\n\n\n\nimage\n\n\n\nMy original goal, was to create a webapp for same use-case, with Vue.js and Flask, as a full stack web application. Itâ€™s being inspired by this blogpost\nBy upcoming week, I will be coming with a blogpost with how to use a fast.ai model for inferencing with Vue.js and Flask. Do remember to check it out\nI stumbled upon a shortcut key in Jupyter, that is on pressing Shift + Tab together you can seen the function definition\nI did the questionnare, after week 2. I was able to just answer 15 questions out of 27 in the first pass\nAfter re-reading the chapter again, I asked these questions to our fantastic group:\n\n1. Where do text models currently have a major deficiency?\n2. What are possible negative societal implication of text\ngeneration models?\n3. What are the three steps in deployment process?\n4. What are the downsides of deploying your app to server,\ninstead of device such as phone or PC?\nThe questions were anwered by Ravi Mashru:\n\n1 & 2: From what I read in the book, text models are really good at generating context-appropriate content. However, this does not mean the generated content is correct. Take a look at this text generated by GPT-2 saying recycling is bad for the world: https://openai.com/blog/better-language-models/#sample8.\n\n\nThe real problem is when such models are used to generate incorrect content that might appear reasonable to laymen and drive disinformation.\n\n\n3: I think the 3-step deployment process is shown in figure 2-5 and explained after the figure in the book. Did you find any particular step difficult to understand?\n\n\n4: At the end, Alexis says you can go with the approach you find easiest.\n\n\nThe downsides of deploying to a server are that the client always needs to talk to the server over a network. This could also add latency (though with fast servers doing this over a network might actually become faster).\n\n\nThe biggest drawback I noticed is the need to send data over the network to an external server. However, this can be remedied by running on-premise servers at the cost of increased complexity of managing the infrastructure.\n\n\nBut we should keep in mind that deploying to edge devices has downsides as well - you need to adhere to the manufacturer specific steps to enable your model to run on different\n\nand Vinayakh Nayakh added on some of major deficencies of Text models:\nAt my current organization, they are trying to build a text generation model which is a sort\nof captioning model for a product given it's image. The problem which they're facing is of\ncontrolled text generation i.e. let's say I have trained a model which takes an image as\ninput and produces a string og words as output. \n\nThe issue which is encountered is that if certain sentence structures are more frequent,\nthe words present in those sentences are highly repeated at the inference \ntime during generation of descriptions. Also, in tasks like keyword detection,\nif the keywords are nuanced in nature, then they cannot be picked up very easily by \nthese text models...\nThank you everyone for reading ðŸ™\nFuture Resource to ponder upon in the coming days:\n\nFastai DataBlock from Scratch\nBias in DataScience\nFind Datablock nirvana with Fastbook"
  },
  {
    "objectID": "posts/2021/2021-06-21-fastgroup-2.html",
    "href": "posts/2021/2021-06-21-fastgroup-2.html",
    "title": "Log2- Learning FastBook along with Study Group",
    "section": "",
    "text": "Video Link\n\nTLDR: A lot of ground was covered by explaining the fastai Lesson1 and Lesson2 till how to get the data for the fastai project on Grizzly bear. Tanishq Abhraham joined us with his tips on how to do projects effectively.\n\n\nWeek 2 sessions started with the whole class reminded that itâ€™s going to be 90 minutes from this week onwards\nAman explained in detail how the Cats vs Dogs classification example worked under the hoods with his beautifully annotated notebooks. \nSee how the process of data loading, to applying item transformations and batch transformation is being explained with this diagram.\nThen Aman went ahead explaining the section of how Image Classifiers work under the hood and what various layers of network do.\nThe architecture which is used for model training ie Resnets is not the state of art for Image classification now. The state of the art for Image classification will frequently change over the years. Last year it was Efficentnet models, yet now it seems Transformers have come to become the SOTA for Image classification now.\nSanyam mentioned that the state of art always keeps changing, so a good resource to be updated with the latest models are PAPERS With Code website\nThe second chapter mentions where Deep Learning is now good at. Deep learning is now really good at images, text data.\nFor tabular, Aman warned itâ€™s not always great. So it wonâ€™t be a good idea to do projects your initial projects in that domain.\nAman explained the basic steps of collecting dataset using bing APIs. He went through the steps and recommended all of us to get the datasets, ready before the next session.\nTanishq who is indeed a wonder kid, explained how fastai transformed his learning journey and helped him become a better Kaggler.\nHe explained why itâ€™s very important to repeat with a different dataset\nHe also shared his experience of making projects more accessible and improving his Kaggle rankings over the years with the help of fastai community and course.\n\nBefore winding up, I am just sharing a reminder by Aman Arora on things to do this week:\n\nThis week please make sure that youâ€™re able to run code from chapter-1 and understand at a high level the 6 lines of code used to create the Pets classifier. Donâ€™t forget to run doc() if you want to dig deeper into any fastai function. This week you should really start messing with code.\nDonâ€™t forget to the questionnaire at the end of Chapter-1! This will help you with your understanding!\nPlease make sure that you have your dataset ready - I personally found setting Azure API key a little tricky cause things have changed since the book was first written. Will post a discussion post with the exact steps and how you can get the API key too.\nConsider sharing your work here or create a W&B forum post - itâ€™s okay if your project isnâ€™t ready and its half baked. Remember we donâ€™t want to be perfectionists, rather do things in multiple iterations and improve as we go.\n\nNow let me share a list of a few things I learned over the past week. So here I am starting my own logs:\n\nRead all the questions which were asked during week2 session- A hidden gem indeed\nRead the article on Designing great Data Products using the Drive train approach. Few key highlights\n\nWe can make product recommendations that surprise and delight (using the optimized recommendation outlined in the previous section).\nWe could offer tailored discounts or special offers on products the customer was not quite ready to buy or would have bought elsewhere.\nWe can even make customer-care calls just to see how the user is enjoying our site and make them feel that their feedback is valued.\n\nThe data collection and recommendation steps are not an add-on; they are Zafuâ€™s entire business model â€” womenâ€™s jeans are now a data product. Zafu can tailor their recommendations to fit as well as their jeans because their system is asking the right questions.Starting with the objective forces data scientists to consider what additional models they need to build for the Modeler. We can keep the â€œlikeâ€ model that we have already built as well as the causality model for purchases with and without recommendations, and then take a staged approach to adding additional models that we think will improve the marketing effectiveness.\n\nExperimented with building Image classification using another dataset for Mamootty vs Mohanlal Image Classification in a different dataset.\nPeeked into Chapter 8 on Diving deep into Collaborative filtering as itâ€™s something I am very interested in building a recommender system.\nThe TabularDataloaders.from_csv method procs function Normalize under the hood uses z-score ie (x- mean)/(std). Thanks @girijesh for pointing it out.\n\nThatâ€™s all for this week. I am winding up by sharing something Rachael Tactman has mentioned in her blog on why you should blog:\n\nYou are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner(or an intermediate) and have forgotten why the topic is hard to understand when you hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what youâ€™re writing about."
  },
  {
    "objectID": "posts/2021/2021-03-06-hostwebsite.html",
    "href": "posts/2021/2021-03-06-hostwebsite.html",
    "title": "Hosting your website ðŸŒ with Github pages",
    "section": "",
    "text": "Disclaimer: This may not just be the first blog post on this topic, yet we are not merely talking how to do it only but also why part which will delve a bit into some theoretical portions like DNS, webhosting etc.\nI am discussing in this article how to host a self domain with GitHub:\nBuy a domain name - I got mine from bigrock.in, while there are sites like google domains, hover, GoDaddy etc. My domain name cost about Rs 900+ after taxes like GST. If you are using a name with more than 5 characters a suffix like .com, .in etc. you will surely get a domain name in that price range. If the name is short, you may need to pay more and I have seen prices go up to Rs 6000 for a short domain name.\n\n\n\ndomainname\n\n\nCreate a repository with GitHub pages with the name: username.github.io. By default without a domain name, your default domain name is username.github.io. I used Beautiful Jekyll theme for building my blogging website. If you have a custom name, Enter your custom domain name in GitHub pages, and enforce HTTPS for our custom domain. Thanks to Lets Encrypt which is providing free SSL/TLS Certificates. This process will create a CNAME record in your repository.\n\n\n\ngithubrepo\n\n\n\n\n\nimage\n\n\nGo to your DNS management system. In the bigrock, scroll all the way down in your domain name page, and reach the setting for DNS Management\n\n\n\nimage\n\n\nNow on clicking Manage DNS create an A record, which is used to point to four name servers hosted by GitHub. First and foremost create a CNAME record that points your subdomain to the default domain for your site. For example, if you want to use the subdomain www.example.com for your user site, create a CNAME record that points www.example.com to .github.io\n\n\n\nimage\n\n\nSo what is a CNAME record?\n\nItâ€™s a record type in resource records that allows an alias to be created. It is a macro definition that helps in redirecting, from one website to another. So really we are actually mapping the server of kurianbenoy.github.io to kurianbenoy.com.\n\nThe next step is to create an A record in the DNS management system. In docs for Configuring a custom domain for your GitHub Pages site - GitHub Docs, itâ€™s mentioned the various IPs used for creating A records in DNS providing servicer.\n\n\n\nimage\n\n\nWhat is A Record\n\nIt is used to point your apex domain to the default domain for your site.\nA record is the most important record type and it holds a 32 bit IP address for some host. Every Internet host must have at least one IP address to communicate with others.\n\nTada, your site is hosted like mine -kurianbenoy.com\n\nNow to the theory portion. I also had hosted my domain earlier following the above steps more than once and it simply worked. Yet how it worked and why we needed black box was a big mystery to me, which I am trying to demystify with this article.\n\nDNS is used to map a specific name into an associated IP address, an application program called library procedure called a resolver. Domain names were initially created specifically for solving the problem in ARPNET to understand which all nodes were there in the system. In those days they used a text file named hosts.txt to keep track of all hostnames and update every night. Yet as time progressed, things started changing for good, and the Domain name system was bought in place.\nThe domain name system consists of Top-level domain names which are managed by ICANN. The various top-level domains are classified into two types:\n\nGeneric top-level domain names - which maps domains that are commonly used all over the world like .com, .net, .org etc. They can be used for any purpose, and the bost TLD by far is .com with more than 100+ million .com domains.\nCountry-level domain names - which maps domain names based on the associated country names. Like our country India has .in, various government sites use domains based on it like data.gov.in\n\nA domain name can be mapped to one or more associated servers. So even though when calling google.com, it can be mapped to multiple servers in various locations of the world\nBesides these top-level domain names, there are second-level domains which is the name which you want to assign for your individual domain name. Also,subdomains can be used to map domain names to certain specific or other sites by adding a word followed by a dot. For example, if you want to create a specific blogging website in my domain name - you can easily add:\n\nblog.kurianbenoy.com\n\nAnother interesting thing about domain names is they can only be in ASCII, even though with certain software some certain Unicode words can be used for domain names. The business of Domain name registries who are accredited with ICANN sell the various domain names and are responsible for providing basic functionalities like name server for our use case. Domain names are considered like real estates, the shorter and attractive it is. The more money it costs.\nThe domain name is a component of a uniform resource locator (URL) used to access web sites(this is a common question in computer networks theory class), for example: [4]\nURL: http://www.example.net/index.html Top-level domain: net Second-level domain: example Hostname: www\nResource Records are associated with a single host or top-level domain names. The real function of DNS is to map domain names into resource records. The resource records are five-tuple format as follows:\n\nDomain_name\nTime_to_live\nClass\nType\nValue\n\nI know it is not a proper ending, just let me know in the comments if I have missed anything. Itâ€™s always to better to do something than doing nothing. Thatâ€™s why I am continuing my weekly ramblings even though itâ€™s not perfect\n~ Kurian Benoy\nðŸ“¢ Resource alert I found this awesome cartoon comic which explains how DNS works\n\nReferences\n\nGithub docs\nhow to host your website on GitHub pages-medium\nComputer Networks, Andrew S Tanenbaum\nUse in website hosting-Domain Name, Wikipedia"
  },
  {
    "objectID": "posts/2021/2021-07-07-fastgroup-4.html",
    "href": "posts/2021/2021-07-07-fastgroup-4.html",
    "title": "Log4- Learning FastBook along with Study Group",
    "section": "",
    "text": "The topic covered for this week in the study was chapter 4 of the book Deep Learning for Coders on going deeper into computer vision with MNIST dataset to build a Digit Classifier.\n\nI am jotting down some of topics covered during the session:\n\nPixels are the foundation of images.\nHere we are using the MNIST dataset to explore Deep learning.\nWe are using dataset URLs.MNIST_SAMPLE, which is a sample of images of 3 and 7. There is another dataset the popular MNIST dataset which can be accessed via URLs.MNIST containing handwritten images from digits 0-9.\nIn computer ideally for easier computations we need everything in an array or in mathematic terms a symmetric matrix.\nAman during the session, asked how to classify an image of 3 and 7?\nLot of wonderful ideas were discussed on how to classify images in the discussion thread.\nLike classifying number based on surface percentage of the coloured densities in the tensor of background to tell what percentage is covered by number to classify.\nWe decided to go with Pixel similarity, that is to find the average value for every pixel of images of 3s and 7s. Then the two group averages define what is the ideal 3 and ideal 7\nwhat is the difference between tensor and array? > An array is a terminology in NumPy and tensor for PyTorch.\nrank is the number of axes or dimensions in a tensor.\nshape is the size of each axis of a tensor.\nNow for our ML model, to distinguish between images of 3 and 7. With pixel similar what it does is:\nCheck by comparing our mean images of 3 and 7 obtained by stacking images.\nThen you compare the new image(to predict) and find the difference between the mean of 3 and 7.\nAs Aman pointed out, when subtracting between two unlike quantities there is a possibility to cancel out.\n\n\n\n\nimage\n\n\n\nSo for calculating calculating difference with the predictions and correct value we used loss functions.\nL1 loss is basically just calculating absolute difference between actual & predicted.\nAnother loss function calculates by finding the difference and then squaring(L2 loss- Root mean squared loss)\nMajor differences b/w Pytorch and Numpy framework are:\n\n\nPytorch works on GPU and Numpy is working on CPU\nPytorch can automatically calculate gradients & while numpy canâ€™t do it\n\n\nWe compute metrics using broadcasting.\nBroadcasting is the automatic filling of lists to match matrix shape.\nCheck the mnist_distance function which is used to do pixel similarity comparison:\n\n{% highlight python %}\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) {% endhighlight %}\n\nWhy in mnist_distance we are using mean((-1, -2)) is used to find the mean across width and height of the image?\nA detailed explanation on this was provided by Srinivas Raman in Week 4 discussion forum:\n\n\nSince the valid3_tens has a shape of (1010, 28, 28) and the ideal3 will have a shape of (1, 28, 28) with broadcasting, the ideal 3 will be broadcast over the 1st dimension. Now we want the average of the pixel values over the last two dimensions. The last two dimensions which are the height & width of the image are indexed by using the -1 index and -2 index. In python -1 refers to the last element and in this case, the last dim of a tensor and -2 is the 2nd last dim of tensor and so refers to the 28x28 dims of our valid3_tens and valid_tens\n\n\nThen we found the accuracy of the validation dataset.\n\nFor the guest lecture, we had Parul Pandey. Parul had come with an awesome article titled Building a complelling Data Science Portfolio with writing. I will recommend you to watch the Parul Guest lecture from 1:09:30 of the video.\nDuring Parul Pandeyâ€™s session, some of my session highlights where:\n\nParul sayâ€™s writing is a personal thing.\nParul doesnâ€™t care about applause. Some of her blogposts has just 83 claps\nWork on small small projects, and bring that small projects into life.\nAlways remember writing takes a lot of time and needs consistency. Yet it has a lot of benefits.\nWriting for yourself(growth mindset).\n\n\nHighlights of the full session can be found by clicking the below video:\n\n\n\n\nIMAGE ALT TEXT\n\n\n\n\n\nimage\n\n\nWeek 4 Discussion forum"
  },
  {
    "objectID": "posts/2021/2021-07-07-fastgroup-4.html#now-let-me-share-some-of-my-learnings-over-the-week",
    "href": "posts/2021/2021-07-07-fastgroup-4.html#now-let-me-share-some-of-my-learnings-over-the-week",
    "title": "Log4- Learning FastBook along with Study Group",
    "section": "Now let me share some of my learnings over the week:",
    "text": "Now let me share some of my learnings over the week:\nSince I was interested to know more about broadcasting, I watched Andrew NG lesson on broadcasting. He gives an interesting problem to use broadcasting to calculate the percentage of calories of carbs, proteins, fats in 100 g of the following food\n\n\n\nimage\n\n\n{% highlight python %} import numpy as np"
  },
  {
    "objectID": "posts/2021/2021-08-12-fastgroup-8.html",
    "href": "posts/2021/2021-08-12-fastgroup-8.html",
    "title": "Week 8 And 9 - Learning FastBook along with Study Group",
    "section": "",
    "text": "So, letâ€™s start another week of learning logs:"
  },
  {
    "objectID": "posts/2021/2021-08-12-fastgroup-8.html#sota-technniques-for-image-classification",
    "href": "posts/2021/2021-08-12-fastgroup-8.html#sota-technniques-for-image-classification",
    "title": "Week 8 And 9 - Learning FastBook along with Study Group",
    "section": "SOTA technniques for Image classification",
    "text": "SOTA technniques for Image classification\nIn Chapter 7, we learned techniques like :\n\nNormalization\nProgressive Resizing\nTest Time Augmentation\nMixup\nLabel Smoothing\n\nI have been working on normalization in another dataset, and just using normalization alone gave me a 20% boost in accuracy compared to the baseline model used. If you are interested in checking my experiments, just check the notebook here. Also this is the Kaggle In-class comp which I was working on.\nAlso do checkout Aman Aroraâ€™s wonderful article on Label Smoothing. I learned a ton by reading his article and corresponding paper.\nAlso this week Prof.Â Zach shared how to use the fast.ai framework to its fullest extent in a tweet. Do check it out:\n\n\nHow can you learn to use the @fastdotai framework to its fullest extent? A thread on what I believe is the most important lesson you can teach yourself: ðŸ‘‡1/\n\nâ€” Zach Mueller (@TheZachMueller) August 7, 2021\n\n\nThe full session recording can be viewed in the below link and thanks for reading ðŸ™. In case, if I have missed something or to provide feedback, please feel free to reach out to me @kurianbenoy2."
  },
  {
    "objectID": "posts/2021/2021-02-15-learningvue.html",
    "href": "posts/2021/2021-02-15-learningvue.html",
    "title": "Learning Vue.js Part1",
    "section": "",
    "text": "Why learning Vue.js?\nJavaScript is a language which I always wanted to learn. Yet recently when I was asked to learn as part of my day job, thatâ€™s when I started thinking about learning it seriously. It is always important to learn some new languages/frameworks from time to time. Since itâ€™s a new year, letâ€™s plan to learn more tech topics in this year.\nBefore Learning Vue.js - learn JS first\nTo learn the Vue.js, first, it is essential to learn the basics of JavaScript, HTML, CSS etc. It is very important to get the fundamentals of JavaScript right. Read what Nadia has to say ðŸ‘‡\n\n\n\nimage\n\n\nLetâ€™s explore a few Javascript concepts which Nadia mentioned in her tweet.\nPromises\nPromises consist of both producers and consumers. The producers can consist of resolve and reject statements. While consumers can consist of statements with then, finally and catch statements embedded in it. Check the below article to learn more:\n\nhttps://dev.to/spartakyste/the-promises-guide-i-would-have-loved-as-a-junior-developper-3621\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Using_promises\nhttps://javascript.info/promise-basics\n\nArrow function\nThis is an easy hack designed in ES6 and is a widely used feature in most of the codebases. {% highlight javascript linenos %} test = (a,b) => { // Return function details } {% endhighlight %}\nNullish Coalescing Operator\nThe nullish coalescing operator (??) is a logical operator that returns its right-hand side operand when its left-hand side operand is null or undefined and otherwise returns its left-hand side operand.\nTaken from MDN Docs\nConsider the expression a ?? b.\n\nThis evaluates to b if a is null or undefined, otherwise, it evaluates to a\n\nDestructing\nItâ€™s a short way to get elements of an array into variables or get properties of an object into variables.\n{% highlight javascript linenos %} const user = { name: â€˜Bhanu Tejaâ€™, blogs: 3, timeSpan: 2. }\nconst {name, blogs, timeSpan} = user console.log(name); console.log(blogs); console.log(timeSpan); {} {% endhighlight %}\nArrays\nLike any languages, a list of like elements is represented using Arrays which are equivalent to lists in Python. Some of the common array operators which are a bit unique are:\n\nmap - The process of transforming an existing array to some other new form.\n\n{% highlight javascript linenos %} const names = [ { firstName: â€˜Caseyâ€™, lastName: â€˜Jonesâ€™ }, { firstName: â€˜Philâ€™, lastName: â€˜Leshâ€™}, { firstName: â€˜Bradâ€™, lastName: â€˜Traversyâ€™}, { firstName: â€˜Augustâ€™, lastName: â€˜Westâ€™}]\nnames.map(name=> ${name.firstName} ${name.lastName}) {% endhighlight %}\n\nReduce - The array reduce method reduces the array of values into a single value. It executes the callback function for each value of the array.\n\nconst values = [2, 4, 6, 7, 8]\n\nconst initialValue = 0\nvalues.reduce((total, currentValue) => total + currentValue, initialValue)\n\nSplice - This method is used to add or remove elements in an array.\n\nFetching APIs\nTo fetch we usually use libraries like axios to do API request manipulation. It would be a good idea to learn more about it and let me share how to use these APIs in case of Vue.js in the below cookbook example.\nUsing Axios to Consume APIs\nEvent Loops\ncontinuously running programs without waiting, so the web browser wonâ€™tâ€™ be waiting infinitely long for a process to end. The concept of Javascript concept - Event loop.\n{% highlight javascript linenos %} let a = true\nsetTimeout(() => { a = false; }, 2000);\nwhile(a) { console.log(a); } {% endhighlight %}\nThe reason why it continuously runs the code even after 2 seconds is because of the concept of event loops since javascript is single-threaded language and setTimeout runs in a separate thread, the value of a as false canâ€™t be replaced as the while loop is still getting run.\nA very interesting talk on this topic is by Phillip Roberts titled - What the heck is the event loop anyway?\n\n\n\n\n\nsetTimeout and setIntreval\nsetTimeout is a function to run a specific function once for a period of milliseconds specified. setIntreval is a function which runs periodically during a time interval clearIntreval is a function to exit from this set interval on passing the function\nWe havenâ€™t started Vue yet\nWe havenâ€™t even touched the meat of our matter where we want to learn VUEJS. I would highly suggest you do yourself a favour by reading Vue.js getting started guide for this week and check out the sample project which we have created. We will continue our VueJs series for next week as well.\nSample Vuejs application\n\nSigning Off,\n\nKurian"
  },
  {
    "objectID": "posts/2021/2021-08-25-fastgroup-9.html",
    "href": "posts/2021/2021-08-25-fastgroup-9.html",
    "title": "Week 10 - Embeddings & Recommender systems, Learning FastBook along with Study Group",
    "section": "",
    "text": "Consider the case of buiding a movie recommendation. We usually use collabrative filtering technique to recommend a specific movie to a user of XYZ platform. Yet how will we categorize the movies into various categories? We can do a decent job to recommend movies based on category of movie it belongs to like blockbuster, comic etc. as shown in the picture:\n\n\n\nimage\n\n\nResource 1\nYet realisatically, we wonâ€™t be able to identify our users taste by just the categories, as people are more diverse in nature. So this is usually identified by understanding the latent factors corresponding to various users and movies. So in a scale of ratings of each movies we can map how much users like it by calculating latent factors as done with pandas cross-tab functionality. This is as shown in below diagram representing movies and users.\n\n\n\nimage\n\n\nConsider the case of a large orgs like netflix with millions of movies listed. Yet the no of movies watched by a user for our example, be corresponding to a very empty matrix who just watched 3 movies. We are going to predict, how much he will like our movies in full list and give some recommendations for the user.\n\n\n\nimage\n\n\nSo how are we going to calculate the remaining predictions for rest of movies. Itâ€™s a 3 step process as mentioned in Resource 2:\n\nRandomly initialize some parameters\nCalculate our predictions\nCalculate our loss\n\n\nIn Deep Learning frameworks, the idea of pandas cross tab to represent the latent factors - which represent the distinct of a matrix is not present. To represent our movies like in the below diagram. We need to calculate our particular movie and user combination. Then identify index of our movie in user latent factor matrix and index in movie latent factor matrix. Then you need to multiply the predictions dot product.\n\n\nThe trick is to replace our indices with one-hot encoded vectors to multiply by user factors as shown in code, to calucate latent factors:\n\none_hot_3 = one_hot(3, n_users).float()\nuser_factors.t() & one_hot_3\n\nSo in frameworks like Pytorch, multiplying by a one-hot encoded matrix using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you can multiply the one hot-encoded matrix by (or using a computational shortcut index into directly) is called Embedding matrix. [2]\n\n\n\n\nimage\n\n\nLetâ€™s look at a practical example of Embedding in real world using github copilot, which uses language modelling over Codex.\n\n\n\nimage\n\n\n\n\nReferences\n\nEmbedding Google ML Crash Course\nDeep Learning for Coders - Pytorch\nIllustrated Word2Vec"
  },
  {
    "objectID": "posts/2023/mbifl2023/index.html",
    "href": "posts/2023/mbifl2023/index.html",
    "title": "Attending MBIFL 2023, a literature festival",
    "section": "",
    "text": "Since I am a frequent book purchaser from Mathrubhumi book, in January I got a notification to buy early bird tickets for MBIFL 2023. I immediately bought 4 dayâ€™s tickets to attend the literature festival. When it came to February, I realised due to some commitments at home and work, I couldnâ€™t attend all the days of this literature festival despite having a ticket.\nMBIFL had almost 300+ speakers, with famous speakers like Nobel prize winners, Brooker prize winners, politicians etc.\nYet I had just one wish. Benyamin is an author who has written lot of thriller novels like à´…àµ½ - à´…à´±àµ‡à´¬àµà´¯àµ» à´¨àµ‹à´µàµ½ à´«à´¾à´•àµâ€Œà´Ÿà´±à´¿ and à´®àµà´²àµà´²à´ªàµà´ªàµ à´¨à´¿à´±à´®àµà´³àµà´³ à´ªà´•à´²àµà´•àµ¾(which is a dual novel) along with the famous à´†à´Ÿàµ à´œàµ€à´µà´¿à´¤à´‚(Goat Days). I just wanted to meet him in person and get an autograph on my copy of my favourite novel.\n\n\n\nMy favourite novel\n\n\nI travelled early morning from my house and reached the venue by almost 10:30 AM. During this time, there was a conversational session about à´ªàµà´°à´µà´¾à´¸à´¿à´¯àµà´Ÿàµ† à´…à´•àµà´·à´°à´•à´¾à´²à´‚ with P Sreeramkrishan, Benyamin, Shabini Vasudev and KV Mohan Kumar. After the session, there was a huge fan base to meet author Benyamin. I first went to get a signature from another author KV Mohan Kumar in the panel, and then waited in the queue to get a signature from Benyamin.\n\n\n\nSession on à´ªàµà´°à´µà´¾à´¸à´¿à´¯àµà´Ÿàµ† à´…à´•àµà´·à´°à´•à´¾à´²à´‚\n\n\nAfter lot of books were signed by Benyamin which was bought by his fans with some of his famous works like à´†à´Ÿàµ à´œàµ€à´µà´¿à´¤à´‚, à´®à´¾à´¨àµà´¤à´³à´¿à´°à´¿à´²àµ† à´‡à´°àµà´ªà´¤àµ à´•à´®àµà´®àµà´¯àµ‚à´£à´¿à´¸àµà´±à´±àµ à´µàµ¼à´·à´™àµà´™àµ¾, à´…àµ½ - à´…à´±àµ‡à´¬àµà´¯àµ» à´¨àµ‹à´µàµ½ à´«à´¾à´•àµâ€Œà´Ÿà´±à´¿. I got my copy also signed with a salutation of snehattode(with love). I just asked my favourite author does a person named Sameera Parveen who was a character in Jasmine Days really exist or not. Benyam answered that itâ€™s a mixture of multiple people who he had seen in his life, yet there is no one such person who doesnâ€™t exist as Sameera Parveen like Najeeb in à´†à´Ÿàµ à´œàµ€à´µà´¿à´¤à´‚.\nMy mission was just completed in one hour and I got a few more signatures like Shabini Vasudev who is an author from Bahrain and from Vinu V John, who is the star anchor of Asianet news(who kindly reminded me he is not famous to be giving autograph).\n\n\n\nAll the signature I got\n\n\nAfter that, I attended the talk with Aparna Balamurali and Captian GR Gopinath. Shashi Tharoor talked about his book Pride, Prejudice and Punditry. Seeing Tharoor answer a question to a student reminded me of Indiaâ€™s former peopleâ€™s president APJ. I wish him all the success as a politician.\nOne of the last sessions I attended that day was by Polish author Justi Guziak on Know yourself to express yourself. This session talked about improvising and was a hands-on workshop to learn this art. During the session, the speakers gave me a chit with the question what was the best gift you have ever received?. This question was to be asked to someone who was at the other end of the audience. I gave my question and Janaki who was from Thiruvananthapuram told me the best gift she ever received was a dog she received when she was in her 7th standard. This dog has been living with her for the past 15 years as a constant when she went through her school, then college and even after her marriage. Another situation which was given during the workshop was to convince Mr Karun to give something valuable in his hand. After a lot of convincing, I was able to share the lunch with Karun.\nThat winded up MBIFL 2023, a literature festival for me a data scientist who doesnâ€™t have anything related to it. It was cool to interact and meet with so many cool people. I am looking forward to MBIFL 2024 soonâ€¦\n\n\n\nSession by Shashi Tharoor\n\n\nUpdate 1(March 5, 2023)\nI found a very cool video in 360 degree which showed how the MBIFL 2023 venue atmosphere is. It was a very cool video and I thought of including this also with this blogpost."
  },
  {
    "objectID": "posts/2023/fitness_journey/index.html",
    "href": "posts/2023/fitness_journey/index.html",
    "title": "My daily fitness routine",
    "section": "",
    "text": "Stable diffusion image generated with prompt Very muscular man, photograph quality, ultra detailed\n\n\nHello everyone, I just thought of sharing my daily fitness routine which I have been following for the past 1 month. Previously I had written about How I lost almost 4Kgs in 3 weeks - my wegiht loss Journey. I want to continue this journey, even though just after writing that post I gained a lot of kilos and I couldnâ€™t follow everything I mentioned in that article.\nSo this time, I am with a new routine:\nDuring morning I go to gym and do High Intensity Interval Training (HIIT) for 30-45 minutes. In the gym I go, this usually consists of 8 stations of 3 minutes exercises followed by 30 seconds rest. The first four stations are vary day after day, followed by a challenge which changes every week. After this I do boxing, followed by two more stations of 3 minutes exercises which usually are floor exercises.\nIn evening I go for a walk for 45-60 minutes after my work. I usually try to walk almost 4 kilometers and monitor my progress in Strava app. I usually walk in the roads near my home till a paddy field and then return back. My usual pace is 4.5-6 km/h and I cover one kilometer in approximately 11-12 minutes.\nSome of you might be wondering why I am sharing this, well I am sharing this as I want to document my fitness progress. Looking back, I myself can look at things I did and see how I progressed. I also want to share this with others who are interested."
  },
  {
    "objectID": "posts/2023/gpt4/index.html",
    "href": "posts/2023/gpt4/index.html",
    "title": "Everything is about to be changed and launch of GPT-4",
    "section": "",
    "text": "There has been a lot of buzz about GPT and ChatGPT in specific. Tom Scott, a youtuber recently published a video on how he used ChatGPT to fix a problem in his email backup program. He said itâ€™s beginning of something new like how internet had literally changed everything 20-30 years back.\n\n\nAI is the new electricity - Andrew NG\n\nWhen I was writing this article, I forget the syntax of how to embed youtube videos in quarto markdown. I solved this issue by asking this question to Quarto Help Bot which was made by Hamel Husain.\nI have seen chatbots, yet the way ChatGPT interacts and communicates is insane to be honest. You can ask it literally anything and these Large Language models are becoming incredibly useful. Hamel had used LangChain which is like a framework for building applications with LLMs specifically for quarto.\n\n\n\nAn Image from gsb.stanford.edu article on Andrew NG remark"
  },
  {
    "objectID": "posts/2023/gpt4/index.html#how-has-the-ai-landscape-been-going-in-the-past-one-year",
    "href": "posts/2023/gpt4/index.html#how-has-the-ai-landscape-been-going-in-the-past-one-year",
    "title": "Everything is about to be changed and launch of GPT-4",
    "section": "How has the AI landscape been going in the past one year?",
    "text": "How has the AI landscape been going in the past one year?\nPersonally I have been following ML/AI for the past five years atleast. Last year really was hyped up. Initially there was this releases in DALL-E and itâ€™s open source alternative Craiyon(then named DALLE mini) being viral. During this time so many trending things like Stable Diffusion, Whisper etc came.\nOne month back, I started seeing my cousins, parents etc. discussing about ChatGPT and how awesome it is. Some of the application apps like Roamaround.ai build on top of ChatGPT was being used. The final nail was last Sunday, when my grandfather called me to read about ChatGPT article in news.\nI have seen people get hyped up about advances in ML. Yet I have never heard about the new technology being discussed by any of my family member like this. Hamel Hussein put this really well in his tweet about what is different in this wave of ML advances."
  },
  {
    "objectID": "posts/2023/gpt4/index.html#release-of-gpt-4",
    "href": "posts/2023/gpt4/index.html#release-of-gpt-4",
    "title": "Everything is about to be changed and launch of GPT-4",
    "section": "Release of GPT-4",
    "text": "Release of GPT-4\nThe GPT-4 model was released by OpenAI today IST. Itâ€™s been a hectic week already TBH, with so many releases already.\n\nThe initial reaction to this model has been awesome, and lot of people have been reacting about this.\n\nYou can read more about this release in below resources:\nResearch document\nProduct details\nCheck out the summary of what was highlights of GPT-4 and itâ€™s developer livestream in this tweet thread by FSDL."
  },
  {
    "objectID": "posts/2023/gpt4/index.html#how-can-we-try-gpt-4-now",
    "href": "posts/2023/gpt4/index.html#how-can-we-try-gpt-4-now",
    "title": "Everything is about to be changed and launch of GPT-4",
    "section": "How can we try GPT-4 now?",
    "text": "How can we try GPT-4 now?\nAccording to OpenAI, you can try the latest GPT-4 via Chat GPT Plus Membership.\nTwo more options seems to be available for normal users without mulimodal search:\n\nBingGPT\nPoe App from Quora"
  },
  {
    "objectID": "posts/2023/gpt4/index.html#is-binggpt-really-using-gpt-4",
    "href": "posts/2023/gpt4/index.html#is-binggpt-really-using-gpt-4",
    "title": "Everything is about to be changed and launch of GPT-4",
    "section": "Is BingGPT really using GPT-4?",
    "text": "Is BingGPT really using GPT-4?\nTo be honest, at the moment we canâ€™t be really sure about that. Even though as Microsoft Bing team confirmed today that they are using GPT-4.\nIn lot of user tests, it doesnâ€™t really seem to be the same. I tested a trick question:\n\nWhich is heavier, two pounds of brick or one pound of feathers?\n\nGPT-4 seems to give the correct answer at time of writing.\n\nBing-GPT seems to give wrong answer.\n\nChat-GPT seems to now give correct answer, it was previously giving wrong answer.\n\nCharles seems to have come with an explaination to this problem, at the time I was writing this article.\nâ€œGPT-4â€ is really a series of models, and the one live in chat now is likely the 0314 checkpoint available via the API. BingChat canâ€™tâ€™ve launched with that checkpoint and likely has branched off in the intervening weeks.â€"
  },
  {
    "objectID": "posts/2023/BirdCLEF2023/index.html",
    "href": "posts/2023/BirdCLEF2023/index.html",
    "title": "ðŸ¦Learning more about birds and bird calls with Merlin app",
    "section": "",
    "text": "I recently went for a bird watching adventure in Salim Ali Bird Sanctuary, Thattekad Kerala, India. During that time, my guide was very proficient in identifying birds by just hearing sounds. In Salim Ali bird sanctuary itself there are almost 300 species of land birds and another 100+ species of water birds.\nI was genuinely curious, how they learned about all these birds and memorized all these. This is when my guide talked about the Merlin app, which can help in learning more about various species of bird.\n\nVideo Explaining functionality of Merlin\nYou can download it from Apple App store or Google Playstore for your android phones. Learn more about Merlin app in this link\n\n\n\nA photo during my bird walking trek\n\n\nThe below image is Ceylon Frogmouth which is a rare bird in my region\n\n\n\nCeylon Frogmouth\n\n\nNote: This article was originally published in Kaggle discussion here"
  },
  {
    "objectID": "posts/2023/welcome/index.html",
    "href": "posts/2023/welcome/index.html",
    "title": "Welcome To My Quarto Website",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome everyone to my new blog.\nI have been wanting for a long time to unify my existing Machine learning blog based on fast-pages together with my main blog website.\nThe unification process is a bit challenging:\n\nAll my previous blog post in main website was written with beautiful jekyll format using markdown.\nNeed support for jupyter-notebook, which beautiful jekyll doesnâ€™t support\n\nThis is where Quarto comes as a saviour. It has wide support for various formats like markdown, jupyter notebook, rmarkdown. The goal, JJ and team is looking to have a single source of publication is really interesting. I was already familiar with nbdev with few of the projects I previously made.\nI converted all of my existing blog post in markdown to fastpages markdown format. Then I leveraged nbdev 1 migration scripts to move it to quarto."
  },
  {
    "objectID": "posts/2020/2020-11-22-weightloss.html",
    "href": "posts/2020/2020-11-22-weightloss.html",
    "title": "How I lost almost 4Kgs in 3 weeks - my wegiht loss Journey",
    "section": "",
    "text": "Stable diffusion image generated with prompt: two stickman, the left is very chubby, while n the right is very lean ,in full body display, smile.\n\n\nHavenâ€™t you all seen advertisements which say you can lose 6 kilograms per week. Today I am going to share my journey on how I lost nearly 4 kilograms in 3 weeks and lost another 3 kilograms in past 4 months. So in total, I lost about from 83 Kg to my current weight which usually falls in the range of 76.5-77.3 Kgs in 5 months.\nSo, first of all, I just want to say itâ€™s perfectly okay to be fat or slim. There is no need to be ashamed of your weight. If anyone comes says you are and says you look like a Hippo. They are just doing an act of body shaming. To know more about body shaming, I would recommend you to watch more about podcast episode about Body shaming in Ennodoppam Malayalam Podcast. To lose or not lose your weight is your choice.\nFor me, the motivation to lose weight was to go back to my old shape like when I was in my school days. And a lot of cousins advised losing some weight during my college days. Yet the real motivation for me was when I was watching an episode of Chai TIme Data Science show featuring Robert Bracco. Robert talked about the importance of taking gambles with his friends and reaching his goals. Hence, he stays inspired to continue the plans promised, and his friends act like an accountability partner.\nThis acted for me as an inspiration to get started on my weight loss journey. Coincidentally my cousin brother invited me to do some exercise with dumbbells. As an effort my part, I made a massive gamble by promising him to buy a biryani, if I didnâ€™t lose 6 kilograms by the end of the month(July 2020).\nAs an initial step, I started exercising by going to cousin brothers place daily and spend about 30-40 minutes daily in weight training. This was followed by doing 20 minutes of Yoga which I learned attending a class in February and March(before the lockdown started). The weight slowly started decreasing, and I was delighted. All the plans went well for the first 2-3 weeks, and I was in the range 80 kgs very quickly from about 83 kg. It was looking to reach that goal easily if the curve of growth was decreasing linearly.\nDuring this time, I picked up another good habit, that was to avoid eating Tea with Sugar. So I joined the club of folks having tea without sugar which usually only aged people drink. Regarding sugar, I now understand itâ€™s very injurious to health after watching The Sugar film. Sugar is a major cause of fat which is very hard to lose generally. Also, sugar is present literally almost every food item. I highly recommend you to watch the Sugar film and think about your sugar intake.\nSugar intake is a significant cause of obesity, and avoiding soft drink can help reduce your weight. I havenâ€™t followed it thoroughly, yet I am steadily reducing my intake a lot after this small change. I even followed the method of measuring individual food items calories with Healthify app. Yet, it didnâ€™t work out well for me.\nYet in the last 2 weeks, I hardly lost any weight, and I lost the challenge. I had to cook up a biriyani for my cousin brothers family. It was a experience, which made me pursue my goal of continuing my weight loss even further.\nI continued this journey further for the past 4 months, even though during this time there was not much weight loss, while I was able to be in almost 78-80 Kg weight range during this period. This was my time in the Plateau of Latent Potential but for me, I continued with the good habits I learned. This is a core principle from the book of Atomic Habits. I highly recommend you to check mjbraganza blogpost on atomic habits.\nThen in the past 3 weeks, I was continuing my weight loss journey further when Vellaunty challenged us. It was a three-weeks hectic diet to reduce my diet by avoiding Rice(daily food) + snacks(which includes in my daily evening routine). So in effect, I had to make my own food daily, i.e.Â every day I used to make chapatis and Oats. The first few days were a bit challenging for me personally, yet I kept going through all the challenging days. I slowly started loving the process of making my own food.\nDuring these days, Vellaunty and Nun used to continually check in each other. In one week, I lost about 1 Kgs. In the coming days, I tried to add a bit more rigour to my diet, by following a diet where I was not having any food between 8AM to 4PM. This idea was inspired from the below video by Goal Guys on how they finally managed to stop gaining weight.\n\n\nAt the end of three weeks, I emerged as the winner of the challenge. During these three-weeks, I reduced my weight from 81.6 range to my current weight of 76.5 weight at the lowest degree, which means I lost about 5.1 Kgs. Folks, please donâ€™t fret with these numbers, it doesnâ€™t mean anything.\nAt the end of my weight loss, for me, itâ€™s all about changing the habits. I am planning to continue the following practices even after the end of the challenge:\n\nDaily walk 5000 steps\nStrictly avoid eating any meals from 8AM to 4PM\nAvoid having snacks as much as possible\nContinue the habit of drinking without tea\nInclude Chapathi and Oats in the diet for five days\n\nI wish you all good luck on whatever journey you want to embark :)"
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html",
    "href": "posts/2020/2020-07-09-TWTW_week1.html",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "",
    "text": "I started to become interest in TWTW after reading more than a year of blogpost from my friend Joel the week that was posts, which he picked up originally inspired from Sijo Kurivilla."
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-3",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-3",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 3",
    "text": "July 3\n\nMy pull request with contribution to Swathanthra Malayalam Computingâ€™s project MSC was merged.\nMy contribution was a exploratory notebook to analyse dataset to find users who contributed to dataset most, category of sound voices, no of good quality sounds recorded so far(with more than 3 upvotes)."
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-4",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-4",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 4",
    "text": "July 4\n\nI attended another session of Adithya Mitra mandal course to improve my communication and leadership skills. Aditya reviewed my resume and further shared a lot of points to improve.\nSome of the insights after program are:\n\n\nImproving my resume and filing my gaps in Linkedin & twitter bio.\nAditya recommends a lot of things on how to interact in industry, hiring manager etc.\nHe gave tonnes of resources regarding books to buy, insights from some important books, apps to use and even a weekly workout schedule etc. He tells to get better every day\nThe community and mentoring will remain even after the course is completed."
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-5",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-5",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 5",
    "text": "July 5\n\nCreate a twitter list to add Mecians and XMecians together in one single list\nStarted unfollowing some of my friends in Twitter\nAttended 2 meetups virtually ie Paper we love Kochi and TFUG Kolkata.\nInteresting talk on Practical NLP by Dipanjan Sarkar"
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-6",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-6",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 6",
    "text": "July 6\n\nOff day from exercise and doing work\nExercise tips from Adithya"
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-7",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-7",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 7",
    "text": "July 7\n\nHad a video call with Shrey Keny from Goa, I taught him basics of Analytical reporting.\nHad a delicious Kuzhimandhi and realised about 2-3 restaurants in my locality had stopped functioning due to Covid-19 situation"
  },
  {
    "objectID": "posts/2020/2020-07-09-TWTW_week1.html#july-8",
    "href": "posts/2020/2020-07-09-TWTW_week1.html#july-8",
    "title": "TwTw-1(July 3 - July 8)",
    "section": "July 8",
    "text": "July 8\n\nMy Kaggle ranks are soaring high continuosly \nWorking on a new notebook for CTDS comps"
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html",
    "href": "posts/2020/2020-12-20-Aoc-1.html",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "",
    "text": "Advent of Code is a yearly contest for programming puzzles which is being held every year. I would like to thank Eric Wastl for making Advent of Code quite awesoeme.\nPS: Spoilers ahead, please donâ€™t read ahead if havenâ€™t solved puzzles Day 1-5 already ðŸ¤ž.\nI would be trying to write and optimise my code for readability, and avoid use of any external libraries(except python standard library)."
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html#day-1",
    "href": "posts/2020/2020-12-20-Aoc-1.html#day-1",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "Day 1",
    "text": "Day 1\n## Part1\ninput = []\nwith open('input1.txt') as f:\n    for k in f.readlines():\n        input.append(int(k.strip()))\n\nfor i in input:\n    for j in input:\n            print(i,j)\n            if (i+j)==2020:\n                print(i*j)\n                exit()  \n\n## Part 2\n\nfor i in input:\n    for j in input:\n        for k in input:\n            print(i,j,k)\n            if (i+j+k)==2020:\n                print(i*j*k)\nThe part 2 is super slow and takes a lot of time to run"
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html#day2",
    "href": "posts/2020/2020-12-20-Aoc-1.html#day2",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "day2",
    "text": "day2\nfrom collections import Counter\n\ntexts = []\nvalid = 0\nwith open('input/input2.txt') as f:\n    for k in f.readlines():\n        text = (k.split(':')[1]).strip()\n        texts.append(text)\n        a = k.split(':')[0]\n        key = a.split()[1]\n        ans = a.split()[0]\n        start = a.split('-')[0]\n        end = ans.split('-')[1]\n\n        ans_counter = Counter(text)\n\n        if int(start)<=ans_counter[key]<=int(end):\n            # if (text[int(start)-1]==key)^(text[int(end-1]==key):\n                valid += 1\n\n    print(f\"No of valid passwords: {valid}\")"
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html#day3",
    "href": "posts/2020/2020-12-20-Aoc-1.html#day3",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "day3",
    "text": "day3\nlines = []\n\nwith open('input/input3.txt') as f:\n    for data in f.readlines():\n        lines.append(data)\n\nprint(len(lines), len(lines[0]))\nSLOPES = [3, 1]\ntrees = 0\ni,j = 0,0\nl = 0\nfor _ in range(len(lines)-1):\n    i = i +1\n    j = j+3\n    print(j%len(lines[i]))\n    if lines[i][j%len(lines[i])-1]==\"#\":\n        trees += 1\n\nprint(\"No of trees\", trees)\n## Part 2\nimport fileinput\nfrom utils import aoc_timer\n\n# def solution():\nslopes = [(1,1), (3,1), (5,1), (7,1), (1,2)]\n\npattern = []\nfor line in fileinput.input(\"input/input3.txt\"):\n    pattern.append(list(line.strip()))\n\nans =1 \nfor (dc, dr) in slopes:\n    r = 0\n    c = 0\n    score = 0\n    while r< len(pattern):\n        c += dc\n        r += dr\n\n        if r<len(pattern) and pattern[r][c%len(pattern[r])] == \"#\":\n            score +=1 \n\n    ans *= score\n\n    print(f\"Tree with slope:{dc,dr} with {score}\")\n\nprint(f\"part2 answer: {ans}\")"
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html#day4",
    "href": "posts/2020/2020-12-20-Aoc-1.html#day4",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "day4",
    "text": "day4\nimport re\nfrom utils import aoc_timer\n\ndef textinput():\n    with open(\"input/input4.txt\") as file:\n        line = [x.strip() for x in file.readlines()]\n        return line\n\n@aoc_timer(4,1,2020)\ndef solve_valid_passport(passport_dump):\n    p1 = 0\n    p2 = 0\n    passport = {}\n    count = 0\n    \n    for x in passport_dump:\n        if x != \"\":\n            for p in x.split():\n                key, value = p.split(\":\")\n                passport[key] = value\n        else:\n            count += 1\n            valid = True\n\n            for field in ['byr', 'iyr', 'eyr', 'hgt', 'hcl', 'ecl', 'pid']:\n                if field not in passport:\n                    valid = False\n\n            if valid:\n                p1 +=1\n            \n            # print(f\"Pass1: all fields {valid}\")\n            if not (1920<=int(passport['byr'])<=2002):\n                valid = False\n            \n            if not (2010<=int(passport['iyr'])<=2020):\n                valid = False\n\n            if not (2020<=(int(passport['eyr']))<=2030):\n                valid = False\n             \n            # print(passport['hgt']) # check for cm/inch\n\n            if passport['hgt']:\n                if passport['hgt'].endswith('cm'):\n                    val = int(passport['hgt'][0:passport['hgt'].find('cm')])\n                    if not (150<=val<=193):\n                        valid = False\n\n                elif passport['hgt'].endswith('in'):\n                    val = int(passport['hgt'][0:passport['hgt'].find('in')])\n                    if (val<59) and (val>76):\n                        valid = False\n\n            # print(f\"Pass2 - height & eyr&byr&iry: {valid}\")\n\n            if(passport['ecl'] not in ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']):\n                valid = False\n\n            if(passport['hcl'].startswith(\"#\")==False) or (len(passport['hcl'])!=7):\n                valid = False\n\n            elif(passport['hcl'].startswith(\"#\")==True):\n                p = re.compile(\"[0-9a-f]\")\n                if p.match(passport['hcl'].strip(\"#\"))==None:\n                    valid =False\n\n            # print(f\"Pass3 - after Eye colour & Hair colour:{valid}\")\n            pid = passport['pid']\n            if len(pid) != 9 or any([c not in '0123456789' for c in pid]):\n                valid = False\n            # print(f\"Pass4 - after pid:{valid}\")\n            if valid:\n                p2 += 1\n    print(\"Answer:1\", p1)\n    print(\"Answer: 2\", p2)\n    \n    return p1,p2\n\n\nif __name__ == \"__main__\":\n    passport_dump = textinput()\n    print(solve_valid_passport(passport_dump))"
  },
  {
    "objectID": "posts/2020/2020-12-20-Aoc-1.html#day-5",
    "href": "posts/2020/2020-12-20-Aoc-1.html#day-5",
    "title": "Advent of Code - Day 1 to Day 5",
    "section": "Day 5",
    "text": "Day 5\nimport fileinput\nfrom utils import aoc_timer\n\n\ndef seat_matrix_transform(seat_matrix):\n    seat_ids = []\n    for seat in seat_matrix:\n        b1 = str.replace(seat[:7], \"F\", \"0\")\n        b2 = str.replace(b1, \"B\", \"1\")\n        row = int(b2, 2)\n\n        b3 = str.replace(seat[7:], \"R\", \"1\")\n        b4 = str.replace(b3, \"L\", \"0\")\n        col = int(b4, 2)\n\n        seatid = row * 8 + col\n        seat_ids.append(seatid)\n\n    return seat_ids\n\n\n@aoc_timer(day=5, part=1, year=2020)\ndef solve1():\n    seat_matrix = list(x.strip(\"\\n\") for x in fileinput.input(\"input/input5.txt\"))\n    seat_ids = seat_matrix_transform(seat_matrix)\n    print(\"Maximum seat id:\", max(seat_ids))\n\n\n@aoc_timer(day=5, part=2, year=2020)\ndef solve_part2():\n    seat_matrix = list(x.strip(\"\\n\") for x in fileinput.input(\"input/input5.txt\"))\n    seat_ids = seat_matrix_transform(seat_matrix)\n    # print(seat_ids)\n    seat_ids.sort()\n\n    for i, d in enumerate(seat_ids):\n        if(seat_ids[i]-seat_ids[i-1]==2):\n            print(\"Our seat\", d-1)\n\n\n\n\nif __name__ == \"__main__\":\n    solve1()\n    solve_part2()"
  },
  {
    "objectID": "posts/2020/2020-05-21-deoldify.html",
    "href": "posts/2020/2020-05-21-deoldify.html",
    "title": "Colorising old black and white images with Deoldify",
    "section": "",
    "text": "A few dayâ€™s back, bunch of old photos was posted. It was a perfect opportunity for me to explore about Deoldify, a software used to colorizing and restoring images & Videos build by fastai student Jason Antic.\nCheck out some of the images which I colourized for my family:"
  },
  {
    "objectID": "posts/2020/2020-10-13-MyVSCODE_setup.html",
    "href": "posts/2020/2020-10-13-MyVSCODE_setup.html",
    "title": "My Visual Studio Setup",
    "section": "",
    "text": "Itâ€™s 1AM in the morning, and I am about to call it for the night. I realize if I donâ€™t write this piece, itâ€™s never going to be in my blog ever. We all know Inspiration dies!\nI have used a lot of Editors from trying out Atom, sublime text, then moving away from these IDEs to text based terminal based IDEs in linux like first vim, then to emacs world, then back again to VIM. Yet when writing code, I felt I was missing code completion which is something very important. Thatâ€™s when I turned to visual studio code. So far my usage experience is so far great and I am planning to continue only using VSCODE for atleast the next 6 months.\nMy current VSCode setup looks like this:\n\n\n\nMy VSCODE\n\n\nI use a Default Light colour scheme. I use the following extensions to make my life easier:\n\nPylance\nPython\nRemote WSl\nYAML\nAnaconda extension pack(cause I now use conda as my package manager)\n\nI always love to have my line highlighted with the below settings change:\n    \"workbench.colorCustomizations\": {\n\n        \"editor.lineHighlightBackground\": \"#add8e6\"\n      }\n}\nThatâ€™s all for now. I will update this article if I make any more changes"
  },
  {
    "objectID": "posts/2020/2020-11-16-Various_python_distributions.html",
    "href": "posts/2020/2020-11-16-Various_python_distributions.html",
    "title": "Various Python Distributions",
    "section": "",
    "text": "Python is infamous for being slow usualy. A lot of people with other language backgrounds complain Python is notoriously slow. And to be honest there is some merit in there argument too and some may have heard about GIL being the issue causing this lock mechanism. There is always a convenience vs speed drift being discussed at the core of issue. Yet most of this issue is always talked about CPython, which is a variant of Python developed by PSF and maintained by various Python core developers.\nYet itâ€™s not just CPython alone. There are various other distribution implementations of Python. Even though for beginners and almost 90% percentage of folks out there might be using CPython for their use case. I created a twitter poll recently and almost 74% have never used any other Python distribution other than CPython.\n\n\nHave you used any Python Distribution other than CPython(normal Python)?\n\nâ€” Kurian Benoy (@kurianbenoy2) November 11, 2020\n\n\nSo letâ€™s look at some of distributions out there in the Python world like which are build for specific purposes like:\n\n\nIron Python is a python written for .NET platform. It was used in lot of .NET libraries libraries where we need a glue language without the boiler plate then IronPython is a good fit. Also since it did not have a GIL, it could multithread quite effectively. Used in Tibco Statistica as default Python version.Thanks @Arocks"
  },
  {
    "objectID": "posts/2020/2020-11-16-Various_python_distributions.html#cython",
    "href": "posts/2020/2020-11-16-Various_python_distributions.html#cython",
    "title": "Various Python Distributions",
    "section": "Cython",
    "text": "Cython\nItâ€™s a version which is responsible for all weird Python errors like missing headers file if you have received it. Itâ€™s widely used and mature with numpy support. It requires thorough knowledge of C.\nYou need to write your operators and types in C like fashion with cint and others as example. The obvious advantage on writing your code in Cython is it runs faster in almost 90% of the cases."
  },
  {
    "objectID": "posts/2020/2020-11-16-Various_python_distributions.html#pypy",
    "href": "posts/2020/2020-11-16-Various_python_distributions.html#pypy",
    "title": "Various Python Distributions",
    "section": "PyPy",
    "text": "PyPy\nA Python based interpreter, which is being used in competitive coding a lot. It also more faster and supports all the features of Python standard. Guido recommends using PyPy for speeding python code\nAccording to PyPy docs:\nPyPy has been used to mean two things. The first is the RPython translation toolchain for generating interpreters for dynamic programming languages. And the second is one particular implementation of Python produced with it. Because RPython uses the same syntax as Python, this generated version became known as Python interpreter written in Python. It is designed to be flexible and easy to experiment with.\nThere are even more which Victor Stenner mentioned in his Pycon India talk. The most commonly used are Cython which cause C like errors in python usually.\nI can promise to update this article soon."
  },
  {
    "objectID": "posts/2020/2020-11-30-aireading.html",
    "href": "posts/2020/2020-11-30-aireading.html",
    "title": "Introducing Reading like AI",
    "section": "",
    "text": "Reading like AI is a python package inspired from the below meme:\nWhat if AI systems could count numbers?\nSo then the numbers will be pronounced as\n1999 - Nineteen Ninty Nine\n1888 - Eighteen Eighty-Eight\n1111 - Eleven Hundred Eleven,\nNo folks, AI is cooler. Itâ€™s => Eleventeen Onety One"
  },
  {
    "objectID": "posts/2020/2020-11-30-aireading.html#installation",
    "href": "posts/2020/2020-11-30-aireading.html#installation",
    "title": "Introducing Reading like AI",
    "section": "Installation",
    "text": "Installation\npython -m pip install numbersai==0.0.1"
  },
  {
    "objectID": "posts/2020/2020-11-30-aireading.html#usage",
    "href": "posts/2020/2020-11-30-aireading.html#usage",
    "title": "Introducing Reading like AI",
    "section": "Usage",
    "text": "Usage\n>> from nosai.numbers import spell\n>> spell(1111)\n'Eleventeen Onety one'"
  },
  {
    "objectID": "posts/2020/2020-11-30-aireading.html#technology-used",
    "href": "posts/2020/2020-11-30-aireading.html#technology-used",
    "title": "Introducing Reading like AI",
    "section": "Technology used",
    "text": "Technology used\nThis package is build based one of the Rules of Machine Learning\nI.e. Rule #7: Turn heuristics into features, or handle them externally.\nWe didnâ€™t require any complex models which was trained for ten thousand hours, all it needed was a few if loops."
  },
  {
    "objectID": "posts/2020/2020-10-05-Pycon_India.html",
    "href": "posts/2020/2020-10-05-Pycon_India.html",
    "title": "Pycon India 2020 Highlights",
    "section": "",
    "text": "Pycon India is one of the conferences which I like to attend every year. The reason is something deeply emotional because Python is the language which hooked me into programming, my first major talk was in Pycon India 2019 and I enjoy the pycon vibe every year. This was my third year attending Pycon and this time it was fully remote.\nThe sessions were well organised virtually using Hoppin Platform. Attending the session was very seamless in my opinion. I attended a few interesting Keynotes, talk sessions from reputed speakers. Every year, I still understand there is a lot for me to continue learning and keep hustling. Also, like every year, I attended the staircase meetings of Dgplug in Pycon India.\nThis year, I attended the workshop sessions as well for the first time. I attended workshops about Python for Computation Social science by Bhargav Srinivasa and Animating Data in PowerPoint by S Anand. The sessions were great and learned some interesting ideas.\nI would like to sign off by thanking all the organisers of Pycon India and a special shout out to Sayan Chaudhary(Chair) for making it extra special like every year."
  },
  {
    "objectID": "posts/2020/2020-06-01-Books_read.html",
    "href": "posts/2020/2020-06-01-Books_read.html",
    "title": "Books read in 2020",
    "section": "",
    "text": "The books I have read so far in 2020 are:\n\nAlchemist(reread again in December)\nThe C programming language\nDharmarajyam by Bhasheer\nConfessions - Leo Tolstoy\nThe 7 habits of highly effective people - Stephen R. Covey\nSix days of War\nNo Exuse guide to blPythgging\nà´•àµ‡à´°à´³à´¯à´¿à´¸à´‚by Santhosh George\nà´¬à´¾àµ¾à´Ÿà´¿à´•àµ à´¡à´¯à´±à´¿\nPython Trick - Dan Bader\nMind Master - Vishy Anand\nSurely youâ€™re joking - Richard Feyman\nML engineering rules\nRich Dad Poor Dad\nApproaching almost any machine learning problem - Abhishek Thakur\nà´…à´±à´¬à´¿ à´•à´¥à´•àµàµ¾\nIllustrated Biography of Bhagat Singh\nHigh performance python programming\n\nBooks currently reading:\n\nDeep Work\n\nOn the shelf:\n\nCracking the coding interview\nAtomic Habits\nDL4CV starter pack"
  },
  {
    "objectID": "posts/2020/2020-06-01-ideasforMLcomputing.html",
    "href": "posts/2020/2020-06-01-ideasforMLcomputing.html",
    "title": "Ideas for ML computing",
    "section": "",
    "text": "Disclaimer: ML stands not for machine learning, but for Malayalam\nSwathanthra Malayalam in association with Tinkerhub conducted a special program called People behind SMC. SMC is preimier organisation working in the area of Malayalam computing. It was interesting to listen to experiences of various folks like Anivar, Santhosh thottingal sir, Balasankar C, Jishnu Mohan, Kavya Manohar etc. The sessions was hosted by Hrishi Chettai in Tinkerhubs instagram pages. The recorded sessions can be found here.\nAfter listening to these talks, I want to share some ideas which came in my mind:\n\n1. Teaching Language computing with Python\nSanthosh thottingal sir usually starts most of his beginner talks, by asking a cliche example like has anyone tried to code in Malayalam? He usually shows a C sample code for Hello world, and first time I saw this I was really fascinated which lead to [artcle link]. Seeing this Adithya and Subin even coded with old style malayalam letters, and this was the first time I was seeing writings in Malayalam digits.\nThere is a lot of difference, when you start thinking of programming with Malayalam instead of English. There will 4 bytes for Malayalm instead of 1 bytes. It may be good to create a progrmming guide on how to program in Python for Language computing for various applications.\nI personally feel this can be a good topic to talk in any Pycon conference, as this area is relatively being under-utilised.\n\n\n2. Covid-19 dataset\nKerala has put up a commendable performance so far in facing Covid-19 panademic. KHA has been writing daily health bulletins written using Manjeri font. One of common criticism Kerala model has been seen is the lack of scientific papers being published. Another idea in my mind is to create a dataset of KHA bulletins so people can study these Kerala model and dataset can be placed in SMC projectâ€™s text corpus. The initial data has been collected and uploaded . Yet one of the challenges is how to convert these PDFs to a useful format for data analysis. In Kaggle they converted the CORD dataset into a no of JSon files."
  },
  {
    "objectID": "posts/2020/2020-01-08-Aws-Reinvent_recap.html",
    "href": "posts/2020/2020-01-08-Aws-Reinvent_recap.html",
    "title": "Attending an Aws meetup",
    "section": "",
    "text": "I recently attended AWS re: Invent reCap organised by AWS User Group Kochi. One of the exciting announcements during the program was about AWS Inferentia chips which is being developed by Annapurna Labs(which unlike itâ€™s name is an Israel based company).\nFor more and more machine learning models now high performance and low bandwidth is a necessary requirement. With the support of having neural cores, support for INT8, BF16 datatypes itâ€™s able to lower the computation cost needed for deploying these models. Almost 90% of ML compute resources is used for interference, with this new chip and release of Amazon EC2 Inf1 instances. Using EC2 Inf1 instances you can have the fastest and lowest inferencing for all ML models in Applications like Speech recognition, Image processing, object detection etc.\nRight Amazon EC2 Inf1 is used by Alexa team to considerably reduce their computation for answering all kinds of voice queries which was earlier used by GPU systems.\nI recently attended AWS re:Invent reCap session by Suman Debnath at AWS User Group Kochi. One of the exciting announcements during the program was about AWS Inferentia chips which is being developed by Annapurna Labs(an Israel based company).\nFor more and more machine learning applications now high performance and low bandwidth is a necessary hardware requirement. Inferentia chips with neural cores, support for INT8, BF16 datatypes is able to lower the computation cost needed for deploying these models(which are used inference). Almost 90% of ML compute resources is used for interference(that is squeezing the result for your input from ML model already build). Amazon EC2 Inf1 instances which leverage the power AWS Inferentia chips you can have the fastest and lowest inferencing for all Applications like Speech recognition, Image processing, object detection etc.\nRight now Amazon EC2 Inf1 is already being used by Alexa team to considerably reduce their computation for answering all kinds of voice queries which was earlier using GPU powered systems.\n#machinelearning #aws #datascience #reinvent2019\nGot some AWS goodies and got connected with AWS speaker Sumanth Debnath"
  },
  {
    "objectID": "posts/2020/2020-03-07-Books_readFebruary.html",
    "href": "posts/2020/2020-03-07-Books_readFebruary.html",
    "title": "Books- January and February",
    "section": "",
    "text": "The seven habits are: - Be Proactive - Begin with the End in the Mind - Put first thinks first - Think Win/Win - Seek first to understand than to be understood - Synergize - Sharpen the saw\nMust read book. The one thing I like about this book is stressing about Character Ethics and Private victories. Private victories are way more important than public victories. Itâ€™s good to sow these habits, and improve yourself 1% per day. I have read this book 2 times so far."
  },
  {
    "objectID": "posts/2020/2020-03-07-Books_readFebruary.html#the-c-programming-language---brian-w-kernighan-denis-m-ritchie",
    "href": "posts/2020/2020-03-07-Books_readFebruary.html#the-c-programming-language---brian-w-kernighan-denis-m-ritchie",
    "title": "Books- January and February",
    "section": "The C Programming Language - Brian W Kernighan, Denis M Ritchie",
    "text": "The C Programming Language - Brian W Kernighan, Denis M Ritchie\nThis book gives an excellent introduction to C, datastructures. I read this book for a quick revision and this books talks about things like Control Flow, Function and Program structure, arrays,pointers, structures, Input&output and UNIX system interfaces. Algorithms like Shell sort, itoa, atoai are mentioned in this book. One thing I really loved about this book is implements things from scratch(like itoa, strcpy) which is like implementing things from scratch like we do in the ML world. Exercises in this book are really good and you must do it. Reading this book made me to rant all first years need to do is Read this book and no hackathons/websites/ML etc."
  },
  {
    "objectID": "posts/2020/2020-03-07-Books_readFebruary.html#confessions---leo-tolstoy",
    "href": "posts/2020/2020-03-07-Books_readFebruary.html#confessions---leo-tolstoy",
    "title": "Books- January and February",
    "section": "Confessions - Leo Tolstoy",
    "text": "Confessions - Leo Tolstoy\nLeo Tolstoy is a renowned Russian author with many books like Anna Kareena, War and Peace, etc. Confessions mentions about some of sins Leo did, and moral dilemma Tolstoy faced and he mentions about his mid-life crisis. He faces this question What is the point of life? in his life and he doesnâ€™t try to evade from this question. He finally finds these answers finally by understanding there is a god exsisting, that too from the peasants. He is highly critical of Russian orthodox church, but he finds the true christanity and mentions what it is in this book. History of Leo Tolstoyâ€™s life in documented more in this wikipedia article."
  },
  {
    "objectID": "posts/2020/2020-03-07-Books_readFebruary.html#book-in-my-shelf",
    "href": "posts/2020/2020-03-07-Books_readFebruary.html#book-in-my-shelf",
    "title": "Books- January and February",
    "section": "Book in my shelf",
    "text": "Book in my shelf\n\nFluent Python\nSwift in Depth\nA no excuse guide to blogging\nPermanent Record, Snowden\n\nThis format is inspired from Jishnuâ€™s blog. Thanks for reading"
  },
  {
    "objectID": "posts/2020/2020-06-09-CS229_lessonnotes.html",
    "href": "posts/2020/2020-06-09-CS229_lessonnotes.html",
    "title": "CS229 - Lesson Notes",
    "section": "",
    "text": "This is just a post for myself to write notes while watching videos, so it may contain lot of typos and some mistakes.\nPre-requisities:\nAim: To do an awesome project by the end of project and gain basics useful forever.\nI am already familar with basics of programming, in case of probability, I know about bayes theoreum, yet there are more topics which I donâ€™t, so cheatsheets for now. I am not familar with linear algebra much now, yet I remeber taking Rachel lectures to learn about SVD, NMF etc.\nIn the less Andrew covered following topics: - Difference b/w CS229, CS230, CS229A? - Classification - Regression - History(Arthur Samuel, chess program) - Deep learning(automated car moving)\neg: given age, tumour size - 2 feature input, predict if tumour is malignant(classif)"
  },
  {
    "objectID": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson2",
    "href": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson2",
    "title": "CS229 - Lesson Notes",
    "section": "Lesson2",
    "text": "Lesson2\nAndrew started off by theoratically representing, linear regression. He explained about gradient descent, and talked about the ambiguity of big enough data now(~1-10 million datapoints now). In case of a batch gradient descent, we use entire training data fully, even for 1 parameter update.\nSome mathematical properites:\nâˆ‡A means derivate of a trace AB = trace BA tr ABC = tr CAB cost = 0.5*np.sum(A[i] - y[i])**2\nProofs are clearly explained in the lecture notes. In end, he showed derivation of Newton equation which reaches the optimal convergence point in a single update(only for linear reg). The contour and loss curver visualisation to show effect of learning was awesome."
  },
  {
    "objectID": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson3",
    "href": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson3",
    "title": "CS229 - Lesson Notes",
    "section": "Lesson3",
    "text": "Lesson3\nAndrew talks about locally weighted regression method where, each points is fitted in a straight depending on itâ€™s curve and dependance. It used for low-dim data with less features and is an example of non-parametric learing algorithm.\nTopics like: - Likelihood - why least square formula? - maximum likelehod"
  },
  {
    "objectID": "posts/2020/2020-06-09-CS229_lessonnotes.html#read-why-understanding-backprop-is-necessary",
    "href": "posts/2020/2020-06-09-CS229_lessonnotes.html#read-why-understanding-backprop-is-necessary",
    "title": "CS229 - Lesson Notes",
    "section": "Read: Why understanding backprop is necessary?",
    "text": "Read: Why understanding backprop is necessary?"
  },
  {
    "objectID": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson7-kernels-in-svm",
    "href": "posts/2020/2020-06-09-CS229_lessonnotes.html#lesson7-kernels-in-svm",
    "title": "CS229 - Lesson Notes",
    "section": "Lesson7 (Kernels in SVM)",
    "text": "Lesson7 (Kernels in SVM)\n\nuse optimisation problem\n\nRepresentation theoreum proof, ie\n||wb||^2-> denoting kernels with objective function\ncanâ€™t understand: dual optimization, convex optimization, etc\nie Kernel trick: Kernel function(x,z) = Phi(x)Transpose* Phi(z)\nie K(z,z) = (x T z)^2"
  },
  {
    "objectID": "posts/2020/2020-03-21-BengaliAI_experience.html",
    "href": "posts/2020/2020-03-21-BengaliAI_experience.html",
    "title": "Bengali AI Competition Experience",
    "section": "",
    "text": "Thanks to Kaggle and Bengali AI for organising this competition. I would like to congratulate all the winners of this competition.\nPublic LB:  0.9749(Rank 297)\nPrivate LB: 1473\nI joined this competition when it was started, yet only got actively involved in this competition after @seesee released his public notebooks on using TPUs. At that time, I was participating in Kaggleâ€™s flower recognition competition(Playground) and was able to understand what @seeâ€” Notebook did.\nAfter a bit of fine-tuning, with hyperparameters(changing to EfficientNet B4) and training for 30 epochs, I was able to get a score of 0.9729. I was all excited because it was the first time I got a higher score than the best available public kernels available(in a live competition) at that time + I didnâ€™t make many changes to original kernel.\nLast 7 days of the competition was full of excitement for me(as my college was shut down due to Covid-19 situation in Kerala, India). After getting this initial result, I read through almost all the posts in discussions.I tried a variety of ideas and organised them as github issues(suggestion from my teammate). I tried to implement augmentations like Mixup, Cutmix, Gridmask etc(Thanks to @cdeotte, @xiejialun Notebook from Flower classification with TPUs notebooks). I tried training on all the architectures of EfficientNet from B3 to B7. I started retraining my models with weights from already available model, yet for me always retraining on weights made my model perform worst.\nFor me this competition was like @init27 saying, kaggle competition felt like a 100 Mile sprint where you are competing against people on Supercars (GrandMasters with a LOT of experience) while I was running barefoot.\nI tried changing with other architectures like Densenet 169, 121. Yet single models based on this approach was not so effective. After doing all these experiments for the last 4-6 days, I was not able to improve my model, any further. It gave me a feel when all the Kaggle grandmasters and masters were able to implement ideas and do things quickly, I was not able to perform so well. I saw a lot of failed ideas, and even after reading lots of papers I was not able to transform certain augmentations into Tensorflow for BengaliAI competition from Flower Classification with TPUs competition.\nOn the final day of competition, I trained my model with EfficientNetB4 for 30 epechs, with step learning rate and decay. And I was able to obtain my highest score of 0.9749 in public LB(which was able to have a better score than the best public kernel available then). I tried ensembling weights with Densenet, but it didnâ€™t work out any good.\nFor me, this competition was a huge learning experience for me and I was able to spend about 100+ hours for this competition and learned a lot of new things from experienced folks here. I would like to thank @hengck23 for encouraging to share the solution.\nObviously, after the competition, I got a lot of new insights which I am trying to ponder and experiment more in the coming days(both in Tensorflow and Pytorch)."
  },
  {
    "objectID": "posts/2020/2020-01-13-newbeginnings.html",
    "href": "posts/2020/2020-01-13-newbeginnings.html",
    "title": "My 2020 resolutions",
    "section": "",
    "text": "My new year resolution is to work more harder and be more organised. In this post I will be sharing some of my goals for new year.\n\nParticpate in 3 Kaggle competitions and write more kernels. Get 2 medals\nBecome a Kaggle 1x master, 4x Kaggle Expert, participate in 5 competitions actively\nStrengthen the fundamental knowledge of CS subjects\nRead 12 books - 7 technical books\nComplete reading Cracking CTCI, Read EPI\ncomplete euler 100 challenge(Python, Swift)\nComplete making Malayalam text-to-speech system\nWrite two research papers\nSpeak in two conferences and conduct a ML study group\n\nShowerthoughts\nParents have done so much for us. Like literally they have spent their entire life living for us. Just feels sad when you canâ€™t give back anything substantial to them"
  },
  {
    "objectID": "posts/2020/2020-08-09-Mindmaster.html",
    "href": "posts/2020/2020-08-09-Mindmaster.html",
    "title": "Notable quotes from Mind master - Viswanathan Anand",
    "section": "",
    "text": "Hello all, I am back with a few notable quotes from Mind Master by Viswanathan Anand. I am not a chess expert and know some of the basic moves. I recently picked up chess, after seeing Pranav Shridhar and Joel playing chess after In out Hackathon, Bangalore - 2019. The only chess player, most of Indians, know is Viswanathan Anand and maybe Magnus Carlsen too after bitter 2013 World Championship loss in Chennai.\nThis book is not merely a chess book. In my opinion, the best thing in this book was how a 5-time world champion approaches games and winning mentality. Anand talks about the state of AI in chess, attitude needed for any winners in any field, how to grab your advantage and lot of life advice are there. I am sharing a few exciting parts in this book\n\nThe way I see it, talent is a lot like a plant. When itâ€™s watered with hard work, it grows and blooms. Deprived of nourishment, the plant simply withers away. With hard work, talent gains in depth and scope, and uncovers newer abilities that were earlier unexplored. And hard work is not just about plugging away at something. It involves thinking intelligently about what you want to achieve, the goals youâ€™re setting, how youâ€™re improving on your innate skills or talents, and how you can incorporate all of this into the list of things that will help you scale that peak.\n\nState of AI in chess:\n- Gary Kasprov moment when IBM computers beat was not a key advancement.\n- There are more opportunities to learn and get itself used in the board.\n- Understand the potential moves, like vishy did in 2007 World chess championship.\n- Adapting to change is necessary.\n- Practise moves which required like weeks of preparation by team for four in 1990s can be done\nby computer in like few  minutes.\n\nIn any situation in life, being adaptable is the only way to grow and succeed. You may have skills that youâ€™ve perfected, a certain worldview that worked for you at a particular stage â€“ but the reality is that circumstances change, and you canâ€™t be prepared for everything.\n\n\nEvery advantage you have can be chipped away slowly, and if youâ€™re not mindful of your present and are busy visualizing future celebrations, then you may not eventually quite be a part of it.\n\n\nThe lead this game gave me was a luxury, but I restrained myself from celebrating too quickly. Iâ€™d suffered from the consequences of doing that earlier â€“ relaxing when I should have been focused, or letting the excitement of an anticipated win take over. There is a difference between having an overwhelming lead and actually finishing a game on top. In these moments, itâ€™s important to be acutely mindful of your present, find calm and keep yourself grounded. If youâ€™re too busy visualizing future success it may eventually give you the slip.\n\n\nNo matter what youâ€™re up against, you have to give yourself some odds of success. If youâ€™ve done the right preparation following the best methods, you have to go out there and believe in what you know. If you talk yourself out of everything, youâ€™re undermining the advantages you have right there.\n\n\nNo matter what your odds are, you should never gamble too much. Thereâ€™s a difference between gambling and preparing to take a risk. What Topalov did on the board was a spur-of-the-moment act, which almost brought me to a winning position in three moves.\n\n\nThe effort you put into a winning match and a losing match is, in fact, exactly the same. But when you win you feel every bit of your work has paid off, and when you lose you feel you neednâ€™t have bothered putting in any effort at all. The results of this game for me may not have been as convincing as in Bonn, but it was expected that my opponent would base his preparations on that match, walk himself through the surprises Iâ€™d employed there and anticipate my moves more closely. What worked most effectively for us was the teamâ€™s ability and willingness to be flexible. They responded to the sudden turn of events â€“ from a volcanic ash cloud and the disruption of travel plans to the rumours of the â€˜superâ€™ weapon in the opponentâ€™s hands â€“ with minimum fuss and maximum practicability.\n\n(So true, So true â€¦)\n\nResilience is the only answer to adversity. When tough situations arise â€“ and they sometimes arrive like a hailstorm â€“ your primary focus should be on accepting that although it is not the way you would want things to be, it is what you have to deal with, and then tackle it with practicality. Itâ€™s also important to remember that no matter what youâ€™re up against.\n\n\nSome degree of visualization at the emotional level might reduce this feeling of hopelessness. For instance, in difficult times, some people like to focus on life events or situations that are pleasant. They think about winning, climbing on to a podium, picture themselves with a medal around their necks or holding up a trophy, and that makes them feel better. The method I tend to favour is imagining everything going disastrously for me. I think of the mistakes I could make and I try to think of what it will be like if I lose a match. I find that it calms me down when I imagine that I can engage in activities other than chess â€“ bury myself in a book, peer at the star-studded sky through a powerful pair of binoculars that log on to iTelescope.net offering astro-imaging and access to telescopes across both the northern and the southern hemispheres â€“ and life will go on. This kind of visualization, which looks at the bigger picture, has always helped me deal with my fears and hopes. In a sense, I try to diminish the importance of the scenario in\n\n\nSuccess can often lull you into believing in what is non-existent â€“ that you have no chink in your armour; that your occasional wins make you invincible; that there is nothing for you to improve upon. Life does not raise red flags unprompted. Look for the cues â€“ they will ask you to identify and work on your weaknesses, disallow passivity in your attitude, thought and preparation for success. Even when you hit the lowest point, they will offer a handy start to hitting the road to recovery.\n\nHope this pieces from the book which I jotted down when I was reading the book will be helpful for you in whatever pursuits you are aiming for. I am planning to be consistently write a blogpost every week.\n:wq"
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html",
    "href": "posts/2020/2020-03-14-swift0.html",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "",
    "text": "I usually pick up a new language, every 2 months and explore new things - Kushal Das\nInspired by this I decided to pickup a new language after sometime and blog my experiences with that. The inspiration for me to learn Swift was not IOS development, but for machine learning/DL application. Last year fast.ai guru Jeremy Howards conducted a Deep learning from Foundations course copresented with Chris Lattner-creator of Swift, guy who made LLVM compiler. Inspired by them, I start journey with Swift with a hope to contribute to Tensorflow pretty and learn Tensorflow4 Swift."
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html#promise-of-swift",
    "href": "posts/2020/2020-03-14-swift0.html#promise-of-swift",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "Promise of Swift",
    "text": "Promise of Swift\n\nInfinetely Hackable language - we can go and change even the lowest level in the language, and use these above the hood to get best performance\nSimilar to Python (print(), no semi colons)\nCan reach C level speed for programming\nSwift for TensorFlowâ€™s advantages include seamless integration with a modern general-purpose language, allowing for more dynamic and sophisticated models. Fast abstractions can be developed in â€œuser-spaceâ€ (as opposed to in C/C++, aka â€œframework-spaceâ€), resulting in modular APIs that can be easily customized.\nCan change anything even at lowest level, for TPUâ€™s new represntation of float called bfloat16 is needed. You can change the base level implementation of float with Swift using LLVM primitives."
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html#installation",
    "href": "posts/2020/2020-03-14-swift0.html#installation",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "Installation",
    "text": "Installation\nBefore hacking a new language, itâ€™s always necessary to setup your working environment for the language. The installation instructions are covered in Swift docs. Swift can be installed in MacOS and Linux.\nSince my operating system is Debian10, as I found the local installation with linux a bit tricky, I installed with Docker.\nDocker installation is pretty straightword:\ndocker pull swift\ndocker run --security-opt seccomp=unconfined -it swift"
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html#semantics",
    "href": "posts/2020/2020-03-14-swift0.html#semantics",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "Semantics",
    "text": "Semantics\nWe use let to make a constant and var to make a variable. The value of a constant doesnâ€™t need to be known at compile time, but you must assign it a value exactly once. This means you can use constants to name a value that you determine once but use in many places.\nvar myVar = 42\nlet myconstant = 42\n\nlet var:Double = 70 (specifying type to double)\nStrings can be added with values in Swift in itâ€™s own special syntax sugar. The value is written in parentheses, and write a backslash (\\) before the parentheses. For example:\nlet apples = 3\nlet oranges = 5\nlet appleSummary = \"I have \\(apples) apples.\"\nlet fruitSummary = \"I have \\(apples + oranges) pieces of fruit.\""
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html#arrays",
    "href": "posts/2020/2020-03-14-swift0.html#arrays",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "Arrays",
    "text": "Arrays\nWe can easily create arrays and dictionaries using brackets([]), and access their elements by writing the index or key in brackets.\nvar shoppingList = [\"catfish\", \"water\", \"tulips\"]\nshoppingList[1] = \"bottle of water\"\n\nvar occupations = [\n    \"Malcolm\": \"Captain\",\n     \"Kaylee\": \"Mechanic\",\n]\noccupations[\"Jayne\"] = \"Public Relations\"\nYou can add new elements to arrays using append:\nshoppingList.append(\"blue paint\")"
  },
  {
    "objectID": "posts/2020/2020-03-14-swift0.html#control-flows",
    "href": "posts/2020/2020-03-14-swift0.html#control-flows",
    "title": "Exploring a new language - Swift(Part 1)",
    "section": "Control flows",
    "text": "Control flows\nWe use for-in to iterate over items in a dictionary by providing a pair of names to use for each key-value pair. Dictionaries are an unordered collection, so their keys and values are iterated over in an arbitrary order.\nlet interestingNumbers = [\n    \"Prime\": [2, 3, 5, 7, 11, 13],\n    \"Fibonacci\": [1, 1, 2, 3, 5, 8],\n    \"Square\": [1, 4, 9, 16, 25],\n]\nvar largest = 0\nfor (kind, numbers) in interestingNumbers {\n    for number in numbers {\n        if number > largest {\n            largest = number\n        }\n    }\n}\nprint(largest)\nUse while to repeat a block of code until a condition changes. The condition of a loop can be at the end instead, ensuring that the loop is run at least once.\nvar n = 2\nwhile n < 100 {\n    n *= 2\n}\nprint(n)\nYou can keep an index in a loop by using ..< to make a range of indexes. Very interesting syntax\nvar total=0\nfor i in 0..<4 {\n    total += i\n    }\nprint(total)\nCurrently I am reading a book called Swift from Depth to learn more about the language.\nThanks for reading!\nAlso do check out my first thoughts about Swift4Tensorflow and how to train a NN with Swift article. Till next time C-x s!"
  },
  {
    "objectID": "posts/2020/2020-02-26-replaceurl.html",
    "href": "posts/2020/2020-02-26-replaceurl.html",
    "title": "Replace one host name with another host name in Javascript",
    "section": "",
    "text": "Problem: You are given a host name, and from it you want to replace host name of one website with another which follows the exact Url pattern\neg: Url = https://github.com/kurianbenoy-aot/Learning\nIn the above url we need to replace github with gitlab because I have a clone repo in gitlab also with same domain name.\nI am going to show how to do it with Javascript.\nUrl = \"https://github.com/kurianbenoy-aot/Learning\"\nreplaceUrl = \"https://gitlab.com\"\n\n Urlextract= (Url.split('://')[1]).split('/')[0]\n replaceUrlextract = replaceUrl.split(\"//\")[1]\n \n//answer is: \"https://gitlab.com/kurianbenoy-aot/Learning\"\nconsole.log(Url.replace(Urlextract, replaceUrlextract))\nWhat if we just wanted to extract the domain name from Url:\nans = Url.replace(\"https://\", \"\").replace(\"http://\", \"\").split('/')[0]\nconsole.log(ans)"
  },
  {
    "objectID": "posts/2022/2022-03-13-hfenml-translate.html",
    "href": "posts/2022/2022-03-13-hfenml-translate.html",
    "title": "Building a fine-tuned translation system for English-Malayalam",
    "section": "",
    "text": "Hey, everyone. We all are familiar with translation systems like using google translate. So today, letâ€™s build a fine tuned translation system for converting text from english to malayalam. Itâ€™s built using Blurr library - built on top of Hugging face and fast.ai made by Wayde Gilliam. Also our translation system is going to be fine tuned on top of KDE specific dataset. You can find the trained model here.\n\n\n\nimage\n\n\nInstallation\n\n\nCode\n! python3 -m pip install -Uqq datasets== fastai==2.6.3\n! python3 -m pip install -Uqq transformers[sentencepiece]\n! python3 -m pip install -Uqq ohmeow-blurr==1.0.5\n! python3 -m pip install -Uqq nltk\n! python3 -m pip install -Uqq sacrebleu\n! python3 -m pip install -Uqq  git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[\"fastai\"]\n\n\nERROR: Could not find a version that satisfies the requirement datasets== (from versions: 0.0.9, 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.2, 1.13.3, 1.14.0, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.17.0, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 2.0.0, 2.1.0, 2.2.0)\nERROR: No matching distribution found for datasets==\n\n\n\n\nA translation system is an example of sequence to sequence models, which is usually used for tasks which involves generating new data. Translation usually needs datasets in both the source language and target language (the language to which it needs to be translated).\nWe are using KDE4 datasets, and choose both source language and translation language as english and malayalam respectively. Usually these datasets are curated by community volunteers to their native language, and this was probably done by KDE community volunteers in Kerala. When someone is localizing these texts into there in local languague, usually computer science specific terms are still written in english.\n\nimport pandas\nfrom datasets import load_dataset\n\n\nraw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"ml\", split=\"train[:1000]\")\n\nUsing custom data configuration en-ml-lang1=en,lang2=ml\nReusing dataset kde4 (/home/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n\n\nMost of translation dataset is in form of id and translation json output - with both en and ml as objects.\n\nraw_datasets[0]\n\n{'id': '0',\n 'translation': {'en': 'Add Feed to Akregator',\n  'ml': 'à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµ\\u200d à´«àµ€à´¡àµ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµ\\u200dà´•àµà´•àµà´•'}}\n\n\n\n\n\n\n\n\nfrom blurr.text.data.all import *\nfrom blurr.text.modeling.all import *\nfrom blurr.text.utils import *\n\nfrom fastai.data.all import *\nfrom fastai.callback.all import *\nfrom fastai.learner import load_learner, Learner\nfrom fastai.optimizer import *\nfrom transformers import *\n\n\npretrained_model_name = \"Helsinki-NLP/opus-mt-en-ml\"\nmodel_cls = AutoModelForSeq2SeqLM\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n    pretrained_model_name, model_cls=model_cls\n)\n\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n\n\ntranslation_df = pd.DataFrame(raw_datasets[\"translation\"], columns=[\"en\", \"ml\"])\ntranslation_df.head()\n\n\n\n\n\n  \n    \n      \n      en\n      ml\n    \n  \n  \n    \n      0\n      Add Feed to Akregator\n      à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´«àµ€à´¡àµ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´•\n    \n    \n      1\n      Add Feeds to Akregator\n      à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´«àµ€à´¡àµà´•à´³àµâ€ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´•\n    \n    \n      2\n      Add All Found Feeds to Akregator\n      à´Žà´²àµà´²à´¾ à´«àµ€à´¡àµà´•à´³àµà´‚ à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´•\n    \n    \n      3\n      Subscribe to site updates (using news feed)\n      à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµâ€ à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµâ€à´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµâ€ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ)\n    \n    \n      4\n      Imported Feeds\n      à´Žà´Ÿàµà´¤àµà´¤ à´«àµ€à´¡àµà´•à´³àµâ€\n    \n  \n\n\n\n\n\nblocks = (Seq2SeqTextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), noop)\ndblock = DataBlock(\n    blocks=blocks,\n    get_x=ColReader(\"en\"),\n    get_y=ColReader(\"ml\"),\n    splitter=RandomSplitter(),\n)\n\n\ndls = dblock.dataloaders(translation_df, bs=16)\ndls.show_batch(dataloaders=dls, max_n=2, input_trunc_at=100, target_trunc_at=250)\n\n\n\n  \n    \n      \n      text\n      target\n    \n  \n  \n    \n      0\n      Aâ–versionâ–controlâ–historyâ–entryâ–consists ofâ–severalâ–lines.â–Specify theâ–regularâ–expressionâ–toâ–detect\n      à´’à´°àµ à´­à´¾à´·à´¾à´¨àµà´¤à´° à´¨à´¿à´¯à´¨àµà´¤àµà´°à´£à´¤àµà´¤à´¿à´¨àµà´±àµ† à´¨à´¾à´³àµà´µà´´à´¿ à´šàµ‡à´°àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´²àµ à´ªà´² à´µà´°à´¿à´•à´³àµà´£àµà´Ÿà´¾à´•àµà´‚. à´†à´¦àµà´¯à´¤àµà´¤àµ† à´µà´°à´¿ à´•à´£àµà´Ÿàµà´ªà´¿à´Ÿà´¿à´•àµà´•à´¾à´¨àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ à´¨à´¿à´°àµà´¦àµà´¦àµ‡à´¶à´¿à´•àµà´•àµà´• (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´‚ à´•àµ‚à´Ÿà´¾à´¤àµ†). à´‡à´¨à´‚ à´¤à´¿à´°à´¿à´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´•àµ€à´•à´³àµ† à´’à´¨àµà´¨à´¿à´šàµà´šà´¾à´•àµà´•à´¾à´¨àµ à´¬àµà´°à´¾à´•àµà´•à´±àµà´±àµà´•à´³àµ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´•. à´’à´´à´¿à´šàµà´šàµ à´µà´¿à´Ÿàµà´Ÿ\n    \n    \n      1\n      â–Mailodyâ–canâ–storeâ–allâ–attchements ofâ–allâ–messages inâ–aâ–certainâ–folder.â–Thenâ–youâ–neverâ–haveâ–toâ–saveâ–\n      à´Žà´²àµà´²à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³àµà´Ÿàµ‡à´¯àµà´‚ à´Žà´²àµà´²à´¾ à´…à´¨àµà´¬à´¨àµà´§à´™àµà´™à´³àµà´‚ à´’à´°àµ à´ªàµà´°à´¤àµà´¯àµ‡à´• à´…à´±à´¯à´¿à´²àµ à´¸àµ‚à´•àµà´·à´¿à´•àµà´•à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´¨à´¿à´™àµà´™à´³àµà´•àµà´•à´µà´¯àµ† à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³à´¿à´²àµà´¨à´¿à´¨àµà´¨àµ à´ªàµà´°à´¤àµà´¯àµ‡à´•à´‚ à´¸àµ‚à´•àµà´·à´¿à´•àµà´•àµ‡à´£àµà´Ÿà´¤à´¿à´²àµà´². à´…à´µ à´…à´±à´¯à´¿à´²àµ à´‰à´£àµà´Ÿà´¾à´¯à´¿à´°à´¿à´•àµà´•àµà´‚. à´ªàµà´°à´¤àµà´¯àµ‡à´•à´‚ à´¶àµà´°à´¦àµà´§à´¿à´•àµà´•àµà´•, à´ˆ à´…à´± à´‡à´Ÿà´•àµà´•à´¿à´Ÿà´•àµà´•àµ à´•à´¾à´²à´¿à´¯à´¾à´•àµà´•à´¿à´•àµà´•àµŠà´£àµà´Ÿà´¿à´°à´¿à´•àµà´•\n    \n  \n\n\n\n\n\n\n\n\n\nBugs in ohmeow v1.0.4 has been fixed by the open-source maintainer.\n\nfrom blurr.text.utils import BlurrText\n\nNLP = BlurrText()\n\n\nlearn = BlearnerForTranslation.from_data(\n    translation_df,\n    pretrained_model_name,\n    src_lang_name=\"English\",\n    src_lang_attr=\"en\",\n    trg_lang_name=\"Malayalam\",\n    trg_lang_attr=\"ml\",\n    dl_kwargs={\"bs\": 16},\n)\n\n\nmetrics_cb = BlearnerForTranslation.get_metrics_cb()\nlearn.fit_one_cycle(1, lr_max=4e-5, cbs=[metrics_cb])\n\n[nltk_data] Downloading package wordnet to /home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      bleu\n      meteor\n      sacrebleu\n      time\n    \n  \n  \n    \n      0\n      5.512897\n      4.821253\n      0.023251\n      0.158193\n      4.086147\n      00:19\n    \n  \n\n\n\n\nlearn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=250)\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      target\n      prediction\n    \n  \n  \n    \n      0\n      â–Mailodyâ–isâ–ableâ–toâ–convertâ–yourâ–plainâ–messageâ–toâ–aâ–htmlâ–messageâ–andâ–includeâ–that in theâ–outgoingâ–message.â–Thisâ–means theâ–receiverâ–willâ–alsoâ–haveâ–clickableâ–linksâ–andâ–coloredâ–quoteâ–levels.\n      à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´¾à´¦à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´Žà´šàµà´šàµà´Ÿà´¿à´Žà´‚à´Žà´²àµ à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±à´¿ à´…à´¤àµ à´ªàµà´±à´¤àµà´¤àµ‡à´•àµà´•àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿à´²àµ à´‰à´³àµà´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´…à´¤à´¾à´¯à´¤àµ à´žàµŠà´Ÿàµà´Ÿà´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´‚ à´µà´°àµà´£àµà´£ à´‰à´¦àµà´§à´°à´£à´¿ à´¤à´²à´µàµà´‚ à´¸àµà´µàµ€à´•à´°àµà´¤àµà´¤à´¾à´µà´¿à´¨àµà´•àµ‚à´Ÿà´¿ à´²à´­àµà´¯à´®à´¾à´µàµà´‚\n      [à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´®àµà´ªà´¾à´¦à´¨ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´’à´°àµ html à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±àµà´µà´¾à´¨àµà´‚ à´ªàµà´±à´¤àµà´¤àµà´³àµà´³ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿àµ½ à´‰àµ¾ à´•àµà´•àµŠà´³àµà´³àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿àµ½ à´‰àµ¾à´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤àµà´µà´¾à´¨àµà´‚ Middià´¯àµà´•àµà´•àµàµ à´•à´´à´¿à´¯àµà´‚. à´‡à´¤à´¿à´¨àµ¼à´¤àµà´¥à´‚ à´±à´¿à´•àµà´•àµ‹àµ¼à´¡àµ à´šàµ†à´¯àµà´¯à´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´•àµà´•àµà´‚ à´¨à´¿à´±à´™àµà´™à´³àµà´•àµà´•àµà´‚ à´µà´²à´•àµà´•àµ†à´Ÿàµà´Ÿàµà´•à´³àµà´‚ à´¨à´¿à´±à´™àµà´™à´³àµà´•àµà´•àµàµ., à´ªà´¤à´¿à´ªàµà´ªàµ à´¨à´¿à´¯à´¨àµà´¤àµà´°à´¿à´¤ à´šà´°à´¿à´¤àµà´°à´¤àµà´¤à´¿à´¨àµà´±àµ† à´ªàµà´°à´¾à´°à´‚à´­à´®à´¾à´¯à´¤à´¿à´¨àµà´±àµ† à´¸à´¾à´§à´¾à´°à´£ à´ªàµà´°à´¯àµ‹à´—à´‚. à´¸à´¾à´§à´¾à´°à´£à´¯à´¾à´¯à´¿ à´ˆ à´µà´°à´¿à´¯à´¿àµ½ \"$2Loux\" à´•àµ€à´µà´¾à´¤à´•à´‚ à´‰à´£àµà´Ÿàµ. à´¸àµà´µà´¤à´µàµ‡à´¯àµà´³àµà´³ à´®àµ‚à´²àµà´²àµà´¯à´‚: description from play played for play play for play for play play play for filme cout fillulume for for courtyourtime fume time ck., à´¤àµà´Ÿà´™àµà´™àµà´¨àµà´¨à´¤à´¿à´¨à´¾à´¯à´¿, \"à´ªàµà´¤à´¿à´¯\" à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´¤àµà´¤àµ à´†à´¦àµà´¯à´‚ à´’à´°àµ à´ªàµà´¤à´¿à´¯ à´’à´ªàµà´ªàµ à´‰à´£àµà´Ÿà´¾à´•àµà´•àµà´•. à´…à´ªàµà´ªàµ‹àµ¾ à´¨à´¿à´™àµà´™àµ¾à´•àµà´•àµàµ à´¤à´¿à´°àµà´¤àµà´¤à´¾à´¨àµà´‚ à´’à´ªàµà´ªàµà´•à´³àµà´Ÿàµ† à´¶àµ‡à´–à´°à´‚ à´¸à´‚à´°à´•àµà´·à´¿à´•àµà´•à´¾à´¨àµà´‚ à´¸à´¾à´§à´¿à´•àµà´•àµà´‚., à´¤à´¿à´°àµà´¤àµà´¤àµ à´šàµ†à´¯àµà´¤ à´«à´¯àµ½ à´¸àµ‚à´•àµà´·à´¿à´¯àµà´•àµà´•àµà´®àµà´ªàµ‹àµ¾ à´µà´°à´¿à´¯àµà´Ÿàµ† à´…à´µà´¸à´¾à´¨à´™àµà´™àµ¾ à´¸à´œàµà´œàµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´¨àµà´¨àµ. à´¡àµ‹à´Žà´¸àµ/ à´œà´¾à´²à´•à´™àµà´™àµ¾: CRS+LLF; à´¯àµà´Žà´«àµ: LRIFX; à´’à´ªàµà´ªà´‚ CRL++D=0, LRD=0A, LRFAA +0A, à´®àµàµ»à´•à´¾à´´àµà´šà´•àµ¾ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµ½ à´ªà´°à´¾à´œà´¯à´‚. à´ˆ à´•à´®à´¾àµ»à´¡àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´¯àµà´•àµà´•àµà´•:% 1 à´Žà´¨àµà´¨ à´†à´œàµà´ž à´‡à´ªàµà´ªàµ‹àµ¾ à´ªàµà´°à´µàµ¼à´¤àµà´¤à´¨à´°à´¹à´¿à´¤à´®à´¾à´¯à´¿à´°à´¿à´•àµà´•àµà´‚., \"Subject\" à´…à´²àµà´²àµ†à´™àµà´•à´¿àµ½ 'suck' à´’à´°àµ à´†à´´à´¤àµà´¤à´¿à´²àµà´³àµà´³ à´…à´µà´¸àµà´¥à´¯à´¾à´£àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´ªàµ‚àµ¼à´£àµà´£à´®à´¾à´¯àµà´‚ à´…à´§à´¿à´•à´¾à´°à´¤àµà´¤à´¿àµ½ à´•àµŠà´£àµà´Ÿàµà´µà´°àµà´¨àµà´¨àµ, à´¸à´¸àµà´ªàµ†àµ»à´¡àµ à´’à´°àµ à´¨à´¿à´¦àµà´°à´¾ à´¸à´‚à´¸àµà´¥à´¾à´¨à´®à´¾à´£àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´Šàµ¼à´œàµà´œà´‚ à´•àµà´±à´¯àµà´•àµà´•àµà´®àµà´ªàµ‹à´³àµ à´®à´¾à´¤àµà´°à´‚ à´Šàµ¼à´œàµà´œà´‚ à´¸à´‚à´­à´°à´¿à´•àµà´•àµà´• à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´³àµà´³àµ‚., à´ˆ à´®àµàµ»à´•à´°àµà´¤àµ½ à´²àµˆà´¨à´¿à´‚à´—àµ à´²àµˆà´¨à´¿à´‚à´—àµ à´²àµˆà´¨à´¿à´‚à´—àµ à´µàµ‡à´³à´¯à´¿àµ½ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´®àµà´³àµà´³àµ. ( à´µà´¿à´¶à´¦à´¾à´‚à´¶à´™àµà´™àµ¾à´•àµà´•àµ à´¡àµ‹à´•àµà´¸àµ à´•à´¾à´£àµà´•.), à´«à´¯à´²à´¿à´¨àµà´±àµ† à´ªà´•àµ¼à´ªàµà´ªàµ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯à´¿à´²àµ à´ªà´¿à´¶à´•àµ: à´µà´¾à´¯à´¨à´¯àµà´•àµà´•àµà´³àµà´³ à´«à´¯àµ½ à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤à´¿àµ½ à´ªà´°à´¾à´œà´¯à´‚:% 1, â–ª à´®à´¾à´®àµ‹à´¦àµ€à´¸ à´¤àµà´±à´¨àµà´¨àµ à´¨àµ‹à´•àµà´•àµà´•à´¯àµà´‚ à´…à´Ÿà´¯àµ à´•àµà´•àµà´•à´¯àµà´‚ à´šàµ†à´¯àµà´¯àµà´¨àµà´¨à´¤àµ à´•àµà´°à´®à´®à´¾à´¯ à´ªàµà´°à´¯àµ‹à´—à´¤àµà´¤à´¿àµ½ à´šàµ‡à´°àµà´•à´¯à´¿à´²àµà´²., à´Žà´²àµà´²à´¾ à´‰à´ªà´¯àµ‹à´•àµà´¤à´¾à´•àµà´•àµ¾à´•àµà´•àµà´‚ à´†à´ªàµà´ªà´¿àµ¾à´Ÿàµà´Ÿàµà´•àµ¾ à´‡àµ»à´¸àµà´±àµà´±àµ‹àµ¾ à´šàµ†à´¯àµà´¯àµà´•à´¯àµ‹ à´¨àµ€à´•àµà´•à´‚ à´šàµ†à´¯àµà´¯àµà´•à´¯àµ‹ à´šàµ†à´¯àµà´¯àµà´•., à´«à´¯à´²à´¿à´¨àµà´±àµ† à´ªà´•àµ¼à´ªàµà´ªàµ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯à´¿à´²àµ à´ªà´¿à´¶à´•àµ: à´µà´¾à´¯à´¨ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ:% 1, à´µàµˆà´°àµà´¦àµà´§àµà´¯à´™àµà´™à´³àµà´Ÿàµ† à´Žà´£àµà´£à´‚ à´¸à´‚à´¬à´¨àµà´§à´¿à´šàµà´šàµ à´’à´°àµ à´¸à´‚à´µà´¾à´¦à´‚ à´•à´¾à´£à´¿à´•àµà´•àµà´•., à´¨à´¿à´™àµà´™àµ¾ à´¬à´¾à´±àµà´±à´±à´¿ à´…à´§à´¿à´•à´¾à´°à´¤àµà´¤à´¿àµ½ à´¨à´¿à´¨àµà´¨àµà´‚ à´“à´Ÿà´¿ à´°à´•àµà´·à´ªàµ†à´Ÿà´¾àµ» à´ªàµ‹à´µàµà´•à´¯à´¾à´£àµ, à´‡à´ªàµà´ªàµ‹àµ¾ à´’à´¨àµà´¨àµà´‚ à´šàµ†à´¯àµà´¯à´¾à´¨à´¿à´²àµà´²., à´µà´¿à´²à´¾à´¸à´™àµà´™à´³àµ à´šàµ‡àµ¼à´¤àµà´¤à´¿à´Ÿàµà´Ÿà´¿à´²àµà´². à´…à´¯à´¯àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´¨àµ à´®àµà´®àµà´ªàµ à´•àµà´±à´žàµà´žà´¤àµ à´’à´¨àµà´¨àµ†à´™àµà´•à´¿à´²àµà´‚ à´šàµ‡àµ¼à´•àµà´•àµ‚., à´•àµà´·à´®à´¿à´•àµà´•à´£à´‚, à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´«àµ‹à´£àµº à´ªà´¤à´¿à´ªàµà´ªàµ à´ªà´¿à´¨àµà´¤àµà´£à´¯àµà´•àµà´•àµà´¨àµà´¨à´¿à´²àµà´².]\n    \n  \n\n\n\n\n\n\n\nb = dls.one_batch()\n\n\nlen(b), b[0][\"input_ids\"].shape, b[1].shape\n\n(2, torch.Size([16, 72]), torch.Size([16, 114]))\n\n\n\ndls.show_batch(dataloaders=dls, input_trunc_at=250, target_trunc_at=250)\n\n\n\n  \n    \n      \n      text\n      target\n    \n  \n  \n    \n      0\n      Aâ–versionâ–controlâ–historyâ–entryâ–consists ofâ–severalâ–lines.â–Specify theâ–regularâ–expressionâ–toâ–detect theâ–firstâ–line (without theâ–leadingâ–comment).â–Useâ–parenthesesâ–toâ–group theâ–keysâ–youâ–wantâ–toâ–useâ–forâ–sorting.â–Ifâ–leftâ–empty,â–thenâ–KDiff3â–assumesâ–thatâ–e\n      à´’à´°àµ à´­à´¾à´·à´¾à´¨àµà´¤à´° à´¨à´¿à´¯à´¨àµà´¤àµà´°à´£à´¤àµà´¤à´¿à´¨àµà´±àµ† à´¨à´¾à´³àµà´µà´´à´¿ à´šàµ‡à´°àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´²àµ à´ªà´² à´µà´°à´¿à´•à´³àµà´£àµà´Ÿà´¾à´•àµà´‚. à´†à´¦àµà´¯à´¤àµà´¤àµ† à´µà´°à´¿ à´•à´£àµà´Ÿàµà´ªà´¿à´Ÿà´¿à´•àµà´•à´¾à´¨àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ à´¨à´¿à´°àµà´¦àµà´¦àµ‡à´¶à´¿à´•àµà´•àµà´• (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´‚ à´•àµ‚à´Ÿà´¾à´¤àµ†). à´‡à´¨à´‚ à´¤à´¿à´°à´¿à´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´•àµ€à´•à´³àµ† à´’à´¨àµà´¨à´¿à´šàµà´šà´¾à´•àµà´•à´¾à´¨àµ à´¬àµà´°à´¾à´•àµà´•à´±àµà´±àµà´•à´³àµ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´•. à´’à´´à´¿à´šàµà´šàµ à´µà´¿à´Ÿàµà´Ÿ\n    \n    \n      1\n      â–Thereâ–isâ–noâ–Inboxâ–found inâ–anyâ–resource.â–Startingâ–aâ–newâ–messageâ–willâ–cause theâ–messageâ–toâ–beâ–lostâ–afterâ–youâ–haveâ–sentâ–it.â–Youâ–willâ–notâ–haveâ–aâ–localâ–copyâ–anymore.â–Ifâ–youâ–wantâ–aâ–copy,â–oneâ–wayâ–toâ–doâ–thisâ–isâ–toâ–addâ–yourselfâ–asâ–a CCâ–to theâ–message.<pad><\n      à´µà´¿à´­à´µà´™àµà´™à´³à´¿à´²àµŠà´¨àµà´¨àµà´‚ à´’à´°àµ à´‡à´¨àµà´¬àµ‹à´•àµà´¸àµ à´•à´¾à´£àµà´¨àµà´¨à´¿à´²àµà´². à´’à´°àµ à´ªàµà´¤à´¿à´¯ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´¤àµà´Ÿà´™àµà´™àµà´¨àµà´¨à´¤àµ à´…à´¤àµ à´…à´¯à´šàµà´š à´¶àµ‡à´·à´‚ à´¨à´·àµà´Ÿà´ªàµà´ªàµ†à´Ÿà´¾à´¨àµ à´•à´¾à´°à´£à´®à´¾à´•àµà´‚. à´ªàµà´°à´¾à´¦àµ‡à´¶à´¿à´• à´ªà´•à´°àµà´ªàµà´ªàµà´•à´³àµŠà´¨àµà´¨àµà´‚ à´’à´°à´¿à´•àµà´•à´²àµà´‚ à´²à´­àµà´¯à´®à´²àµà´²à´¾à´¤à´¾à´µàµà´‚. à´‰à´¦à´¾à´¹à´°à´£à´®à´¾à´¯à´¿ à´’à´°àµ à´ªà´•à´°àµà´ªàµà´ªàµ à´†à´µà´¶àµà´¯à´®àµà´£àµà´Ÿàµ†à´™àµà´•à´¿à´²àµ à´’à´°àµ à´•à´¾à´°àµà´¬à´£àµ à´ªà´¤à´¿à´ªàµà´ªàµà´•àµ‚à´Ÿà´¿ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµà´¤àµà´¤à´¤à´¾\n    \n    \n      2\n      â–Toâ–preventâ–dataâ–lossâ–orâ–otherâ–damage,â–youâ–canâ–have theâ–system suspendâ–orâ–hibernate,â–soâ–youâ–doâ–notâ–accidentallyâ–runâ–out ofâ–batteryâ–power.â–Configure theâ–number ofâ–minutesâ–belowâ–which theâ–machineâ–willâ–run theâ–configuredâ–action.<pad><pad><pad><pad><pad>\n      à´µà´¿à´µà´°à´¨à´·àµà´Ÿà´®àµ‹ à´¹à´¾à´¨à´¿à´¯àµ‹ à´¤à´Ÿà´¯à´¾à´¨àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´®à´¯à´™àµà´™àµà´•à´¯àµ‹ à´¶à´¿à´¶à´¿à´°à´¨à´¿à´¦àµà´°à´¯à´¿à´²à´¾à´•àµà´•à´¯àµ‹à´šàµ†à´¯àµà´¯à´¾à´µàµà´¨àµà´¨à´¤à´¾à´£àµ, à´…à´™àµà´™à´¨àµ† à´†à´•à´¸àµà´®à´¿à´•à´®à´¾à´¯à´¿à´Ÿàµà´Ÿàµà´ªàµ‹à´²àµà´‚ à´¬à´¾à´±àµà´±à´±à´¿ à´Šà´°àµà´œàµà´œà´‚ à´¤àµ€à´°à´¾à´¤à´¿à´°à´¿à´¯àµà´•àµà´•àµà´‚. à´•àµà´°à´®àµ€à´•à´°à´¿à´šàµà´š à´¨à´Ÿà´ªà´Ÿà´¿à´¯àµà´®à´¾à´¯à´¿ à´®àµà´¨àµà´¨àµ‹à´Ÿàµà´Ÿàµàµ à´ªàµ‹à´•àµ‡à´£àµà´Ÿà´¤àµàµ à´Žà´¤àµà´° à´®à´¿à´¨à´¿à´±àµà´±àµà´•à´³à´¿à´²àµ à´¤à´¾à´´àµ†à´¯à´¾à´•àµà´®àµà´ªàµ‹à´´à´¾à´£àµ†à´¨àµà´¨àµàµ à´¤à´¾à´´àµ† à´•àµà´°à´®àµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´•\n    \n    \n      3\n      â–AutoSyncâ–isâ–aâ–featureâ–from MP3tunesâ–whichâ–allowsâ–youâ–toâ–automaticallyâ–moveâ–yourâ–musicâ–betweenâ–computersâ–andâ–devices.â–Youâ–canâ–uploadâ–musicâ–fromâ–oneâ–locationâ–andâ–haveâ–itâ–downloadâ–instantlyâ–toâ–otherâ–locations.<pad><pad><pad><pad><pad><pad><pad><pad><pa\n      Enable harmony\n    \n    \n      4\n      â–Regularâ–expressionâ–forâ–linesâ–whereâ–KDiff3â–shouldâ–automaticallyâ–chooseâ–oneâ–source.â–Whenâ–aâ–lineâ–withâ–aâ–conflictâ–matches theâ–regularâ–expressionâ–then -â–ifâ–available - C,â–otherwise Bâ–willâ–beâ–chosen.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n      à´•àµ†à´¡à´¿à´«àµ3 à´¸àµà´µà´¤à´¨àµà´¤àµà´°à´®à´¾à´¯à´¿ à´’à´°àµ à´¸àµà´°àµ‹à´¤à´¸àµà´¸àµ à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•àµà´¨àµà´¨à´¿à´Ÿà´¤àµà´¤àµ à´µà´°à´¿à´•à´³àµà´•àµà´•àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚. à´¸à´‚à´˜à´Ÿàµà´Ÿà´¨à´®àµà´³àµà´³ à´µà´°à´¿ à´šàµ‡à´°àµà´¨àµà´¨àµà´µà´°àµà´®àµà´ªàµ‹à´³àµ à´…à´¤à´¿à´¨àµà´±àµ† à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ - à´¸à´¿ à´‰à´£àµà´Ÿàµ†à´™àµà´•à´¿à´²àµ à´…à´¤àµ, à´…à´²àµà´²àµ†à´™àµà´•à´¿à´²àµ à´¬à´¿ à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•à´ªàµà´ªàµ†à´Ÿàµà´‚.\n    \n    \n      5\n      â–Loadingâ–externalâ–imagesâ–givesâ–spammers theâ–acknowledgementâ–thatâ–youâ–receivedâ–thisâ–messageâ–soâ–theyâ–willâ–useâ–yourâ–emailâ–addressâ–toâ–spamâ–you.â–Soâ–youâ–shouldâ–onlyâ–continueâ–forâ–veryâ–trustedâ–messages.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n      à´ªàµà´±à´®àµ†à´¨à´¿à´¨àµà´¨àµŠà´°àµ à´šà´¿à´¤àµà´°à´‚ à´•à´¯à´±àµà´±àµà´¨àµà´¨à´¤àµ à´¨à´¿à´™àµà´™à´³àµà´•àµà´•àµ à´ˆ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´²à´­à´¿à´šàµà´šàµ†à´¨àµà´¨ à´®à´Ÿà´•àµà´•à´°à´¶àµ€à´¤à´¿ à´šà´µà´±à´¯à´¯àµà´•àµà´•àµà´¨àµà´¨à´µà´°àµà´•àµà´•àµ à´²à´­à´¿à´šàµà´šàµ‡à´•àµà´•à´¾à´‚. à´…à´µà´°àµ à´¨à´¿à´™àµà´™à´³àµà´Ÿ à´‡à´¤à´ªà´¾à´²àµ à´µà´¿à´²à´¾à´¸à´‚ à´¨à´¿à´™àµà´™à´³àµà´•àµà´•àµàµ à´¨àµ‡à´°àµ‡à´¯àµà´‚ à´šà´µà´±à´¯à´¯àµà´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´šàµà´šàµ‡à´•àµà´•à´¾à´‚. à´…à´¤àµà´•àµŠà´£àµà´Ÿàµ à´µà´³à´°àµ† à´µà´¿à´¶àµà´µà´¸àµà´¤ à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³àµ à´®à´¾à´¤àµà´°à´‚ à´¤àµà´Ÿà´°àµà´¨àµà´¨à´¾à´²àµ à´®à´¤à´¿.\n    \n    \n      6\n      â–Mailodyâ–isâ–ableâ–toâ–convertâ–yourâ–plainâ–messageâ–toâ–aâ–htmlâ–messageâ–andâ–includeâ–that in theâ–outgoingâ–message.â–Thisâ–means theâ–receiverâ–willâ–alsoâ–haveâ–clickableâ–linksâ–andâ–coloredâ–quoteâ–levels.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pa\n      à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´¾à´¦à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´Žà´šàµà´šàµà´Ÿà´¿à´Žà´‚à´Žà´²àµ à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±à´¿ à´…à´¤àµ à´ªàµà´±à´¤àµà´¤àµ‡à´•àµà´•àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿à´²àµ à´‰à´³àµà´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´…à´¤à´¾à´¯à´¤àµ à´žàµŠà´Ÿàµà´Ÿà´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´‚ à´µà´°àµà´£àµà´£ à´‰à´¦àµà´§à´°à´£à´¿ à´¤à´²à´µàµà´‚ à´¸àµà´µàµ€à´•à´°àµà´¤àµà´¤à´¾à´µà´¿à´¨àµà´•àµ‚à´Ÿà´¿ à´²à´­àµà´¯à´®à´¾à´µàµà´‚\n    \n    \n      7\n      â–Youâ–haveâ–clickedâ–onâ–aâ–linkâ–whichâ–mightâ–notâ–indicateâ–correctlyâ–whereâ–youâ–areâ–reallyâ–goingâ–to.â–Pleaseâ–checkâ–ifâ–youâ–reallyâ–wantâ–toâ–viewâ–aâ–pageâ–onâ–thisâ–server:â–%1â–Doâ–youâ–wantâ–toâ–goâ–there?<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n      à´¨à´¿à´™àµà´™à´³àµ à´’à´°àµ à´•à´£àµà´£à´¿à´¯à´¿à´²àµ à´žàµŠà´Ÿàµà´Ÿà´¿à´¯à´¤àµ à´¨à´¿à´™àµà´™à´³àµ†à´µà´¿à´Ÿàµ‡à´•àµà´•à´¾à´£àµ à´µà´¾à´¸àµà´¤à´µà´¤àµà´¤à´¿à´²àµ à´ªàµ‹à´•àµà´¨àµà´¨à´¤àµ à´Žà´¨àµà´¨àµ à´•àµƒà´¤àµà´¯à´®à´¾à´¯à´¿ à´¸àµ‚à´šà´¿à´ªàµà´ªà´¿à´•àµà´•àµà´¨àµà´¨à´¿à´²àµà´². à´‡ à´¸àµ‡à´µà´•à´¨àµà´±àµ† à´¤à´¾à´³à´¿à´²àµ‡à´•àµà´•àµ à´¤à´¨àµà´¨àµ† à´¯à´¾à´£àµ‹ à´ªàµ‹à´•àµ‡à´£àµà´Ÿà´¤àµ†à´¨àµà´¨àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµà´•:% 1 à´¨à´¿à´™àµà´™à´³àµà´•àµà´•à´¿à´µà´¿à´Ÿàµ† à´ªàµ‹à´•à´£àµ‹?\n    \n    \n      8\n      â–Tryâ–toâ–align Bâ–and Câ–whenâ–comparingâ–orâ–mergingâ–three inputâ–files.â–Notâ–recommendedâ–forâ–mergingâ–becauseâ–mergeâ–mightâ–getâ–moreâ–complicated. (Defaultâ–isâ–off.)<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n      à´…à´•à´¤àµà´¤àµà´µà´¿à´Ÿà´¾à´¨àµà´³àµà´³ 3à´«à´¯à´²àµà´•à´³àµ à´¤à´¾à´°à´¤à´®àµà´¯à´®àµ à´šàµ†à´¯àµà´¯àµà´®àµà´ªàµ‹à´´àµ‹ à´²à´¯à´¨à´‚ à´¨à´Ÿà´¤àµà´¤àµà´®àµà´ªàµ‹à´´àµ‹à´¬à´¿à´¯àµà´‚ à´¸à´¿à´¯àµà´‚ à´¨à´¿à´°à´’à´ªàµà´ªà´¿à´•àµà´•à´¾à´¨àµ à´¶àµà´°à´®à´¿à´•àµà´•à´£à´‚. à´•àµ‚à´Ÿàµà´¤à´²àµ à´•àµà´´à´ªàµà´ªà´‚ à´ªà´¿à´Ÿà´¿à´šàµà´šà´¤à´¾à´¯à´¤àµà´•àµŠà´£àµà´Ÿàµ à´²à´¯à´¨à´¤àµà´¤à´¿à´¨àµà´±àµ† à´•à´¾à´°àµà´¯à´¤àµà´¤à´¿à´²àµ à´…à´¤àµ à´¶àµà´ªà´¾à´°àµà´¶ à´šàµ†à´¯àµà´¤à´¿à´Ÿàµà´Ÿà´¿à´²àµà´². (à´“à´«àµ à´†à´£àµ à´¸à´¹à´œà´‚.)\n    \n  \n\n\n\n\nseq2seq_metrics = {\n    \"bleu\": {\"returns\": \"bleu\"},\n    \"meteor\": {\"returns\": \"meteor\"},\n    \"sacrebleu\": {\"returns\": \"score\"},\n}\n\nmodel = BaseModelWrapper(hf_model)\nlearn_cbs = [BaseModelCallback]\nfit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n\nlearn = Learner(\n    dls,\n    model,\n    opt_func=partial(Adam),\n    loss_func=PreCalculatedCrossEntropyLoss(),  # CrossEntropyLossFlat()\n    cbs=learn_cbs,\n    splitter=partial(blurr_seq2seq_splitter, arch=hf_arch),\n)\n\nlearn.freeze()\n\n[nltk_data] Downloading package wordnet to /home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\nlearn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n\n\n\n\n\n\n\n\nSuggestedLRs(minimum=0.00010000000474974513, steep=2.75422871709452e-06, valley=0.00013182566908653826, slide=0.2089296132326126)\n\n\n\n\n\n\nlearn.fit_one_cycle(15, lr_max=5e-4, cbs=fit_cbs)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      bleu\n      meteor\n      sacrebleu\n      time\n    \n  \n  \n    \n      0\n      5.345715\n      4.811335\n      0.038182\n      0.183787\n      5.607312\n      00:15\n    \n    \n      1\n      4.658613\n      4.272337\n      0.058925\n      0.206963\n      3.932045\n      00:32\n    \n    \n      2\n      4.196794\n      4.031921\n      0.069354\n      0.167185\n      4.876618\n      00:25\n    \n    \n      3\n      3.826715\n      4.102871\n      0.071109\n      0.122637\n      4.639123\n      00:18\n    \n    \n      4\n      3.551352\n      4.046614\n      0.080512\n      0.202187\n      6.996449\n      00:26\n    \n    \n      5\n      3.329875\n      3.928597\n      0.054780\n      0.179460\n      3.403195\n      00:53\n    \n    \n      6\n      3.197556\n      3.818610\n      0.109263\n      0.212960\n      9.055532\n      00:21\n    \n    \n      7\n      3.034615\n      3.821332\n      0.100355\n      0.200819\n      8.208149\n      00:20\n    \n    \n      8\n      2.900438\n      3.807550\n      0.101014\n      0.222323\n      7.221607\n      00:35\n    \n    \n      9\n      2.820454\n      3.813579\n      0.111548\n      0.219794\n      9.026264\n      00:29\n    \n    \n      10\n      2.756891\n      3.791952\n      0.107236\n      0.223636\n      9.965611\n      00:31\n    \n    \n      11\n      2.740331\n      3.809624\n      0.115999\n      0.239258\n      10.261043\n      00:24\n    \n    \n      12\n      2.685602\n      3.804891\n      0.126693\n      0.234446\n      11.131677\n      00:22\n    \n    \n      13\n      2.653799\n      3.800799\n      0.119988\n      0.227742\n      10.985718\n      00:23\n    \n    \n      14\n      2.636473\n      3.801427\n      0.124357\n      0.229134\n      10.967688\n      00:25\n    \n  \n\n\n\n\nlearn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=500)\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      target\n      prediction\n    \n  \n  \n    \n      0\n      â–Withâ–aâ–dynamicâ–playlist, Amarokâ–becomesâ–yourâ–ownâ–personalâ–dj,â–automaticallyâ–selectingâ–tracksâ–forâ–you,â–basedâ–onâ–aâ–number ofâ–parametersâ–thatâ–youâ–select.\n      Turn dynamic mode on\n      [Username for logins to disabled, format: Artist - Track (Album), from the currently playlist column name and token for playlist layouts, à´µà´°à´¿à´•à´³à´¿à´²àµ† à´…à´•àµà´·à´°à´™àµà´™à´³àµà´Ÿàµ† à´…à´µà´¸àµà´¥ à´µà´¿à´—à´£à´¿à´•àµà´•àµà´•. (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´µà´°à´™àµà´™à´³àµà´®à´¾à´¯à´¿ à´¤à´¾à´°à´¤à´®àµà´¯à´‚ à´šàµ†à´¯àµà´¯à´¾à´¨àµà´‚ à´¸à´¿à´®àµà´²àµ‡à´±àµà´±à´±à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚.), à´’à´°àµ à´Ÿà´¾à´¬àµ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚. à´’à´°àµ à´šàµ†à´±à´¿à´¯ à´Ÿà´¾à´¬àµ à´¬à´¾à´±àµà´±à´±à´¿à´¯à´¿à´²àµà´²à´¾à´¤àµ† à´¸àµà´µà´¯à´‚ à´Ÿà´¾à´¬àµ à´¬à´¾à´±àµà´±à´±à´¿ à´¯à´¾à´¨àµà´¤àµà´°à´¿à´•à´®à´¾à´¯à´¿ à´®à´±à´¯àµà´•àµà´•àµà´¨àµà´¨àµ. à´’à´°àµ à´Ÿà´¾à´¬àµ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚., à´‡à´¤àµà´¤à´°à´‚ à´ªàµà´°à´•à´Ÿà´¨à´‚ à´¤àµà´Ÿà´™àµà´™àµà´®àµà´ªàµ‹à´³àµê ±, à´®àµ†à´¯à´¿à´²à´¡à´¿à´•à´³à´¿àµ½ à´šà´¿à´°à´¿à´šàµà´šà´¤à´¾à´¯à´¿ à´¸àµà´µà´¯à´®àµ‡à´µ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´•. à´‰à´¦à´¾à´¹à´°à´£à´®à´¾à´¯à´¿ à´’à´°àµ à´…à´•àµà´·à´°à´°àµ‚à´ªà´‚ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµà´¨àµà´¨àµà´£àµà´Ÿàµ‹à´¯àµ†à´¨àµà´¨àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•à´¾à´¤àµ† à´¸àµà´µà´¯à´‚ à´šà´¿à´°à´¿à´šàµà´šàµà´•àµŠà´£àµà´Ÿà´¾à´µàµà´‚., à´®àµà´¨àµê ±à´œàµà´œàµ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµà´•àµŠà´£àµà´Ÿàµ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ. à´ˆ à´†à´œàµà´ž à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµ‚:% 1 à´®àµà´¨àµê ±à´¨à´Ÿà´ªà´Ÿà´¿ à´†à´œàµà´ž à´‡à´ªàµà´ªàµ‹àµ¾ à´ªàµà´°à´µà´°àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´¤àµà´¤à´¨à´°à´¹à´¿à´¤à´®à´¾à´•àµà´•àµà´‚., à´ªàµà´±à´®àµ†à´¨à´¿à´¨àµà´¨àµ- à´®à´¿à´šàµà´šà´®àµà´³àµà´³ à´®à´Ÿàµà´Ÿàµà´‚ à´ªàµà´°à´¾à´µàµ¼à´¤àµà´¤à´¿à´•à´®à´¾à´•àµà´•àµà´¨àµà´¨àµ. à´µà´²à´¿à´¯ à´«à´¯à´²àµà´•à´³àµ†à´•àµà´•àµà´±à´¿à´šàµà´šàµà´³àµà´³ à´ªà´°à´¿à´¶àµ‹à´§à´•à´¨àµà´±àµ† à´ªà´°à´¿à´¶àµ‹à´§à´•à´¨àµà´­à´µà´‚ à´µà´³à´°àµ† à´ªà´¤àµà´•àµà´•àµ†à´¯à´¾à´µàµà´‚., à´ˆ à´¸àµà´²àµˆà´¡à´¨àµà´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´¸àµ‹à´•àµà´•à´±àµà´±àµ à´ªàµà´±à´¤àµà´¤àµ‡à´¯àµà´•àµà´•àµàµ à´ªà´¤à´¿à´•àµà´•àµà´¨àµà´¨àµ, à´ªàµà´±à´¤àµà´¤à´¾à´•àµà´•àµà´µà´¾à´¨àµà´³àµà´³ à´®àµ†à´¯à´¿à´²à´¡à´¿à´­à´¾à´—à´‚ à´²à´­àµà´¯à´®à´²àµà´², à´¦à´¯à´µà´¾à´¯à´¿ à´…à´¤àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨à´¤à´¿à´¨àµ à´®àµà´®àµà´ªà´¾à´¯à´¿ à´•àµà´°à´®àµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´•, à´ˆ à´®àµà´¨àµê ± à´ªàµ‚à´°àµê ±à´µàµà´µ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯àµà´•àµà´•àµà´³àµà´³ à´¸à´®à´¯à´¤àµà´¤àµ à´µà´°à´¿à´¯à´¿à´²àµê ± à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚. (à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´™àµà´™à´³àµà´•àµà´•àµ à´ªàµà´°à´®à´¾à´£à´ªà´¤àµà´°à´‚ à´¨àµ‹à´•àµà´•àµà´•.), à´«à´¯à´²àµê ± à´ªà´•à´°àµê ±à´ªàµà´ªàµ†à´Ÿàµà´•àµà´•àµà´®àµà´ªàµ‹à´³àµê ± à´ªà´¿à´¶à´•àµ: à´«à´¯à´²à´¿à´²àµâ™¬ à´µà´¾à´¯à´¨à´¯àµà´•àµà´•àµà´³àµà´³ à´«à´¯à´²àµê ± à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤àµ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ. à´«à´¯à´²àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´• à´¨à´¾à´®à´‚:% 1, à´¨à´¿à´™àµà´™à´³àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´¬àµ€à´—à´¿à´³àµê ± à´¬à´¾à´•àµà´•àµ†à´¨àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´¡àµà´•à´³àµê ± à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•àµà´•., à´‰à´ªà´¯àµ‹à´•àµà´¤à´¾à´µàµà´¨à´¾à´®à´‚ à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´žàµà´žà´¿à´Ÿàµà´Ÿà´¿à´²àµà´², à´…à´²àµà´²àµ†à´™àµà´•à´¿àµ½ à´ªà´¾à´±àµà´±àµ‡à´£àµà´•à´³à´¿à´²àµ† à´…à´Ÿà´¯à´¾à´³à´®à´¿à´Ÿàµà´•., à´…à´¨àµà´¬à´¨àµà´§à´™àµà´™à´³àµà´Ÿàµ† à´…à´³à´µàµ% 1 is new name for translate is the filter, representing the playlist column name and token for playlist layouts, à´¬à´¾à´±àµà´±à´±à´¿à´•à´³à´¿à´²àµ†à´¤àµà´¤à´¿à´šàµà´šàµ‡à´°àµà´®àµà´ªàµ‹à´³àµê ± à´¶àµ‹à´­ à´¨à´¿à´¯à´¨àµà´¤àµà´°à´¿à´•àµà´•àµà´¨àµà´¨àµ, (\"à´’à´´à´¿à´žàµà´ž à´‡à´Ÿà´™àµà´™à´³àµê ± à´•à´¾à´£à´¿à´¯àµà´•àµà´•àµà´•\" à´ªàµà´°à´µà´°àµê ±à´¤àµà´¤à´®à´°à´¹à´¿à´¤à´®à´¾à´•àµà´•à´¿à´¯à´¾à´²àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´• à´’à´´à´¿à´žàµà´ž à´‡à´Ÿà´™àµà´™à´³àµà´Ÿàµ† à´µàµà´¯ à´…à´²àµà´²à´¾à´¹àµà´•àµà´•à´³àµê ± à´’à´´à´¿à´µà´¾à´•àµà´•àµà´‚.), à´®à´¾à´¤à´¾à´ªà´¿à´¤à´¾à´•àµà´•àµ¾ à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´…à´Ÿà´¯àµà´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´•.]\n    \n  \n\n\n\n\n\n\n\n\n\n\n\ntest_text = \"How are you doing\"\n\n\noutputs = learn.blurr_generate(\n    test_text, key=\"translation_texts\", num_return_sequences=3\n)\noutputs\n\n[{'translation_texts': ['à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ?',\n   'à´Žà´™àµà´™à´¨àµ†à´¯àµà´£àµà´Ÿàµ?',\n   'à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ.']}]\n\n\n\n\n\n\nexport_fname = \"saved_model\"\n\n\nlearn.metrics = None\nlearn.export(fname=f\"{export_fname}.pkl\")\n\n\nfrom huggingface_hub import push_to_hub_fastai\n\npush_to_hub_fastai(\n    learn,\n    \"kurianbenoy/kde_en_ml_translation_model\",\n    commit_message=\"New version with 15 epoch of training\",\n)\n\n/home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`.\nW0511 16:57:02.992604 140162781013824 repository.py:685] /home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`.\n\n\n\n\n\nremote: Enforcing permissions...        \nremote: Allowed refs: all        \nTo https://huggingface.co/kurianbenoy/kde_en_ml_translation_model\n   62f36f9..766c8a0  main -> main\n\nW0511 16:58:04.917035 140162781013824 repository.py:1144] remote: Enforcing permissions...        \nremote: Allowed refs: all        \nTo https://huggingface.co/kurianbenoy/kde_en_ml_translation_model\n   62f36f9..766c8a0  main -> main\n\n\n\n'https://huggingface.co/kurianbenoy/kde_en_ml_translation_model/commit/766c8a06c07bb6352d0537ac1972d3c70360fd53'\n\n\n\n\n\n\n\n\ntest_text = \"How are you doing\"\n\ninf_learn = load_learner(fname=f\"{export_fname}.pkl\")\ninf_learn.blurr_translate(test_text)\n\n[{'translation_texts': 'à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ?'}]\n\n\n\ntest_text1 = \"Add All Found Feeds to Akregator.\"\n\ninf_learn = load_learner(fname=f\"{export_fname}.pkl\")\ninf_learn.blurr_translate(test_text1)\n\n[{'translation_texts': 'à´Žà´²àµà´²à´¾ à´«àµ€à´¡àµà´•à´³àµà´‚ à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµê ± à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´•àµà´•àµà´•.'}]\n\n\n\n\n\n\ntest_text2 = \"Subscribe to site updates (using news feed).\"\n\ninf_learn = load_learner(fname=f\"{export_fname}.pkl\")\ninf_learn.blurr_translate(test_text2)\n\n[{'translation_texts': 'à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµê ± à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµê ±à´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµâ™¬ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ).'}]\n\n\nExpected: â€™à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµ00d à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµ00dà´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµ00d à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ\n\n\n\n\n\n\nWayde Gilliam - for creating blurr, and helping with doubts in translation bits\nKevin Bird - for helping in editing the article.\nAshwin Jayaprakash - for trying out notebook and reporting issues which was later fixed by Wayde in blurr.\n\nfin."
  },
  {
    "objectID": "posts/2022/2022-05-28-fastai-walthrus1.html",
    "href": "posts/2022/2022-05-28-fastai-walthrus1.html",
    "title": "Installing Python Packages & setting up libraries for Data Science - the right way",
    "section": "",
    "text": "Jeremy has been conducting these official course walk-thrus which started on May 27, 2022 for students of fastaiv5 course. The idea of these course walk-thrus were â€œgoing to explain exactly how to do every step, and why we do things the way we doâ€.\nWe in the sense fastai approach of development. It was very useful to me and it covered answers to some of things which I was searching for a long time. I will use it again and again, so thought of writing it down to refer later. The first walk thrus was on the topic: Introduction to the terminal. How to install Python and libraries and I felt I got more value out of these walk-thrus than some of lessons in course\n\n\n\n\n\n\nImportant\n\n\n\nThe right way here means the way Jeremy does stuff. Obiviously there as thousand of way to do thing, yet going through each ways has itâ€™s own pros/cons."
  },
  {
    "objectID": "posts/2022/2022-05-28-fastai-walthrus1.html#whats-terminal-how-to-work-with-it",
    "href": "posts/2022/2022-05-28-fastai-walthrus1.html#whats-terminal-how-to-work-with-it",
    "title": "Installing Python Packages & setting up libraries for Data Science - the right way",
    "section": "ðŸš– Whatâ€™s terminal, how to work with it?",
    "text": "ðŸš– Whatâ€™s terminal, how to work with it?\n\nTerminal vs Shell\nA terminal is a program which can display console window to run program. Yet thing inside is not strictly a terminal, but called a shell. The black coloured stuff, which you see in movies used by hackers is Terminal as shown in image below.\n\n\n\nimage\n\n\nUsually in a terminal there can be multiple shells, which can have different colours, shells etc. So terminal and shell are totally different in meaning. You can think shell as the a ship which does main things itâ€™s supposed to do like running program, while terminal is like the a group of ships which is usually controlled by a parent like a corporal.\nIn a windows terminal it can start no of shells like PowerShell, Command Prompt, Ubuntu etcâ€¦ Most of the time we use both terminal and shell interchangebly which is ok.\n\n\nInstalling Terminal & Shell\nYou can install terminal in Windows by downloading Windows terminal. In case of linux/MacOS, you can just search for terminal as it will come with a terminal pre-installed.\nIn Windows, Jeremy recommends to use WSL, which install a linux distribution within windows and then install Ubuntu from Microsoft Store.\n\n\nHandy Tips\nItâ€™s a good idea to change the default shell from Power shell to Ubuntu for easy usage. Also learn some keyboard shortcuts.\n\n\n\n\n\n\nImportant\n\n\n\nLearning Keyboard shortcuts can be immensely valuable.\n\n\n\n\nKeyboard shortcuts\nSome of the useful keyboard shortcuts shared during lesson and in forums are as follows:\nShared by Jeremy\nCtrl+Shift+1 - Open shell set for default profile.\nCtrl+Shift+3 - Open shell listed as number 3 in WSL.\nAlt+Enter - Enter terminal in full screen.\n Ctrl + r  - Recursvie back search to search the previous typed commands.\nCtrl-a - Move to the start of the current line.\nCtrl-e - Move to the end of the line.\nTab Autocomplete.\nShared by miwojc\nCtrl-f - Move forward a character.\nCtrl-b - Move back a character.\nAlt-f - Move forward to the end of the next word. Words are alphanumeric.\nAlt-b - Move back to the start of the current or previous word. Words are alphanumeric.\nCtrl-l - Clear the screen.\nCtrl-p - Fetch the previous command from the history list (same as up but easier to reach).\nCtrl-n - Fetch the next command from the history list (same as down).\nCtrl-r - Search backward through history.\nCtrl-d - Delete the character under the cursor.\nCtrl-k  - Kill (cut) forwards to the end of the line.\nCtrl-u - Kill (cut) backwards to the start of the line.\nAlt-d - Kill (cut) forwards to the end of the current word.\nCtrl-w - Kill(cut) backwards to the start of the current word.\nShared by Kurian\nAlt Shift +  - Open a new vertical pane\nAlt Shift - - Open a new horizontal pane\nCtrl Shift w - Closing a Pane"
  },
  {
    "objectID": "posts/2022/2022-05-28-fastai-walthrus1.html#how-to-install-python-packages-for-datascience",
    "href": "posts/2022/2022-05-28-fastai-walthrus1.html#how-to-install-python-packages-for-datascience",
    "title": "Installing Python Packages & setting up libraries for Data Science - the right way",
    "section": "ðŸ”° How to install Python Packages for datascience",
    "text": "ðŸ”° How to install Python Packages for datascience\n\n\n\n\n\n\nImportant\n\n\n\nNever use the python which comes by default with operating system, always use a differnt python to work on stuff you want, else it will become really messyâ€¦ âš ï¸.\n\n\n\nInstallation with mamba\nTo follow this advice, this letâ€™s go ahead and install python seperately with packaging manager called mamba which is a Fast Cross-Platform Package Manager. Itâ€™s aka. faster version of conda.\nSo go to the mambaforge installer. Based on your operating system, download the shell script to install with:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\nbash Mambaforge-Linux-x86_64.sh\nAfter downloading and installation is complete. To refresh terminal either close and reopen terminal or typing below command:\n. ~/.bashrc\n\n\n\n\n\n\nImportant\n\n\n\nJeremy recommends to install popular libraries which are supported in conda from mamba. If itâ€™s not there in conda, or something which requires editable install use pip.\n\n\n\n\nðŸ› Uninstalling mamba/conda\n\n\n\n\n\n\nNote\n\n\n\nIf you are a beginner, working with virtual environments is intimidating. So jeremy recommends whenever you face any issues as a beginner, itâ€™s important to know how to delete your setup.\n\n\nThese are steps to see if conda/mamba is properly uninstalled in a linux environment:\n\nCheck if ipython, jupyter is installed. If yes, first uninstall it.\n\nNote: You can uninstall via:\npip uninstall jupyter\npip uninstall ipython3\nYou can check the path of program by typing the command, to see if was installed by system python or mamba\n(base) kurianbenoy@Lap-34:~/blog/ml-blog$ which jupyter\n/home/kurianbenoy/mambaforge/bin/jupyter\n\nThen remove mamabaforge folder. In linux usually, mambaforge is installed in /home/username.\n\nSo to delete the mambaforge package:\n(base) kurianbenoy@Lap-34:~$ ls\nblog  downloads  mambaforge\n(base) kurianbenoy@Lap-34:~$ rm -rf mambaforge/\n\nThen delete conda package also.\n\n\n\nUsing fastsetup\nGoto github repository and download the setup-conda.sh file.\nwget https://raw.githubusercontent.com/fastai/fastsetup/master/setup-conda.sh\nThis is a normal shell script, so just run the shellscript to install mamba as follows:\nsource setup-conda.sh\n. ~/.bashrc\nconda install -yq mamba\n\n\nMy question to Jeremy\nâ“I asked to Jeremy about: which version of python is using and how to use for a different python version\nJeremy on checking Mambaforge repository, realised that python version which we are using now is python 3.9. Itâ€™s always a good idea to know, which kind of python versions are near their end of life and can be here. At the time of writing any version above python 3.7 to python3.10 is recommended to use. Yet according to Jeremy, he usually prefers to use latest python only 1 year after itâ€™s released, but fastai do support the latest version.\nRadek who is a engineer at Nvidia, told he usually prefer mamba because with just one line of code you can switch to any python version you want. Afer the session in forums, Radek shared how to do this as shown in below screenshot.\n\n\n\nimage\n\n\n\n\nInstalling datascience packages - pytorch and jupyter\nSince for fastai course, we are using pytorch. Letâ€™s install pytorch based on official page instructions:\n\n\n\nimage\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne of the advantages of installing libraries like pytorch in Anaconda is that if you are using conda, it install all the packages and drivers for GPU setup as well. In a normal pip based installation, there are lot more steps required for installing correctly in GPU.\n\n\nGo ahead and install python packages in this manner, by just replacing conda with mamba.\nmamba install pytorch torchvision torchaudio cpuonly -c pytorch\n\n\n\n\n\n\nNote\n\n\n\nItâ€™s always a good idea to google and find the correct conda packages before installation, as conda requires some specifications like setting the correct channel to install.\n\n\nNow letâ€™s install jupyter lab to do our experiments quickly:\nmamba install jupyterlab\nThen run the jupyter lab, with the following command:\njupyter lab --no-browser\nThis opens up jupyterlab in localhost:8888\n\n\n\nimage\n\n\n\n\nInstalling packages with fastchan\nEven though Jeremy didnâ€™t cover this topic during walk-thrus. I came to know about fastchan when I wanted to install pytorch and huggingface transformers in a gpu based system. What is fastchan and the problem it solves is covered by detailed blogpost by the Aman Arora.\nTo install both pytorch and huggingface transformers in a GPU, I used the following command:\nmamba install pytorch transformers cudatoolkit=11.4 -c fastchan"
  },
  {
    "objectID": "posts/2022/2022-05-28-fastai-walthrus1.html#conclusion",
    "href": "posts/2022/2022-05-28-fastai-walthrus1.html#conclusion",
    "title": "Installing Python Packages & setting up libraries for Data Science - the right way",
    "section": "Conclusion",
    "text": "Conclusion\nOne of fastai students, during the start of lesson talked about the need of a quick setup, which just works as expected. One of biggest takeaways for me, personally is a quick and fastsetup to do my experiments in DataScience. Sometimes installing packages in data science can take hours of effort, and thatâ€™s why I really loved this setup.\nThanks to Jeremy Howard for creating this quick setup and for starting the project fastchan."
  },
  {
    "objectID": "posts/2022/2022-07-06-gradio_spaces.html",
    "href": "posts/2022/2022-07-06-gradio_spaces.html",
    "title": "Getting featured in Spaces of the week and my latest two gradio spaces",
    "section": "",
    "text": "I recently created two gradio based webapps, and one of my spaces - Paddy Doctor got featured in list of hugging face Spaces of the week.\n\n\n\nimage\n\n\nBoth gradio apps based on two kaggle competitions which I have been participating in. Both are on computer vision models with one to identify the type of disease in the paddy crop and another to identify the name of flowers(which I am terribly bad at remembering names).\nDo checkout the links below for the spaces\nPaddy Doctor\nIdentify which flower"
  },
  {
    "objectID": "posts/2022/2022-07-26-wandbbug.html",
    "href": "posts/2022/2022-07-26-wandbbug.html",
    "title": "A strange bug when using fastai library with Weights & Biases",
    "section": "",
    "text": "After attending an introduction to using weights & biases along with fastai session conducted by Thomas Capaballe. I was excited to use weights and biases library along with a few of my hobby projects. I was working on training an Image classification models on the Kaggle competition dataset Petal to Metal.\n\n\n\nimage info\n\n\nIn general, whenever I am passing a code to any fastai learner objects with callback. I usually directly pass it along with vision_learner as shown in below code.\n\narch = \"convnext_tiny_in22k\"\nlearn = vision_learner(\n    data,\n    arch,\n    metrics=error_rate,\n    cbs=[\n        WandbCallback(log_preds=False, log_model=True),\n        SaveModelCallback(monitor=\"accuracy\"),\n    ],\n)\nlearn.fine_tune(5)\n\nI exported this model, as I was trying to create a hugging face spaces to identify various flowers species.\n\nlearn.export()\n\nNow I went ahead creating the inference code with requirements for this model. This is when I noticed that the model exported requires wandb library to run the inference code. I was totally surprised, why it was happening at first.\nWhy this annoying behaviour?\nItâ€™s because when passing the callbacks to Learner class or itâ€™s variants like in case of computer vision fastai uses vision_learner class makes it stick around. In my case, I donâ€™t want the callback to hang around the Learner class forever, as itâ€™s just for training job monitoring only.\nAfter a bit of googling, I found this solution from one of the forum posts written by Wayde Gilliam.\n\n\n\n\n\n\nImportant\n\n\n\nInstead of adding your callback to Learner â€¦ if it is simply used for training, just include it in your call(s) to fit or fit_one_cycle. As the callback is no longer associated to your Learner, they wonâ€™t interfere with your call to get_preds().\n\n\nOriginal answer.\nSo inorder to fix it, I just passed the callbacks I am using directly with fine_tune method directly. Letâ€™s check the code to pass callbacks this way.\n\narch = \"convnext_tiny_in22k\"\nlearn = vision_learner(data, arch, metrics=[accuracy, error_rate])\nlearn.fine_tune(\n    5,\n    cbs=[\n        WandbCallback(log_preds=False, log_model=True),\n        SaveModelCallback(monitor=\"accuracy\"),\n    ],\n)\n\nHence I learned this valuable lesson, which fixed my bug in inferencing code for huggingface spaces which I was creating.\n\nZach Mueller also confirmed this is the case."
  },
  {
    "objectID": "posts/2022/2022-04-26-fastai-50.html",
    "href": "posts/2022/2022-04-26-fastai-50.html",
    "title": "Practical Deep Learning for Coders Course - Lesson 0",
    "section": "",
    "text": "Last few weeks, I enrolled for the live cohort of Deep Learning For Coders with fastai Couse which is going to be taken by Jeremy Howard. Itâ€™sâ€™ a previlege at the same time, a dream come true moment for me, as a fastai student who took some part of fast.ai from the year 2018.\nWhatever I have done in ML can be partly attributed to the way and the Jeremy motivated students to be world class researchers. The fastai course has seen lot of success stories from folks like Aman Arora, Deoldify creator, Sanyam Bhutani, Even Oldrige, Jason Antic Radek, Zach Mueller, Wayde Gilliam and much more. Jeremy has inspired and enabled multiple people to be practitioners in Deep Learning. {bhuvani an example}\nI still feel I am a late boomer, and due to pursuing a work along with academics. I am in a position to be extremely careful with what I spend time with and things I do.\nI will do following:\n\nTry out every notebook given by Jeremy during class and writes in Kaggle\nbreath and look through pytorch tutorials\nWrite blogpost every week with lesson summary and occasionally on new things I learning during course.\nParticipate in NLP competition provided by jeremy link\n\nI wonâ€™t do:\n\nWonâ€™t read Pytorch book(even though itâ€™s very tempting to do)\nBe active in twitter during course\nRead EDA notebooks\nWonâ€™t write any notebooks in tensorflow :)"
  },
  {
    "objectID": "posts/2022/2022-11-07-clip_experiments.html",
    "href": "posts/2022/2022-11-07-clip_experiments.html",
    "title": "How well does CLIP models classify corn seeds?",
    "section": "",
    "text": "The OpenAI CLIP model are really impressive and how itâ€™s a foundation for stuff like stable diffusion is awesome. The thing about CLIP models which I am most impressed by is the wide range of applications it be used for like Semantic Video Search, Zero shot image classification, searching images in your gallery etc.\nI recently started reading CLIP paper and paper claims to have very high accuracy in image clssification accuracy. To test that claim, I thought of trying it out that in a kaggle competition I had recently participated.\nThe kaggle competition is a Corn image classification competition and is asking to classify images of corn seeds into following categories:\n\npure\nbroken\ndiscolored\nsilkcut\n\nI used open_clip, an open source implementation of CLIP which is having higher accuracy compared to model weights released by OpenAI.\nEven after using one of the best accuracy CLIP models available( ViT-H-14), it got me a classification accuracy score of 27.95% in private LB whereas Resnet or Convnext models could have given easily above 75% score.\n\n\n\nModel\nPublic LB\nPrivate LB\nNotebook link\n\n\n\n\nViT-B-32-quickgelu\n0.16666\n0.18397\nlink\n\n\nViT-H-14\n0.28591\n0.27955\nlink\n\n\nConvnext model\n0.76149\n0.75386\nlink\n\n\n\nUPDATE\nWhen I shared this results in twitter, YiYi Xu suggested to try out linear probing in CLIP. She mentioned that, I was not comparing apples to apples, as I was using a zero-shot model with CLIP to compared with a fine tuned model of convnext. In order to level up, I should use linear probing which is using training data to kind of fine tune with a logistic regression model leveraging features in CLIP model.\nBased on this, I leveraged using linear probing on the dataset. As a result my updated result are the following:\n\n\n\nModel\nPublic LB\nPrivate LB\nNotebook link\n\n\n\n\nZero Shot ViT-B-32-quickgelu\n0.16666\n0.18397\nlink\n\n\nZero Shot ViT-H-14\n0.28591\n0.27955\nlink\n\n\nLinear probing w/ ViT-H-14\n0.71982\n0.72583\nlink\n\n\nConvnext model\n0.76149\n0.75386\nlink\n\n\n\nNote: This article was originally published in Kaggle discussion here"
  },
  {
    "objectID": "posts/2022/2022-05-23-nlpkagglecomp.html",
    "href": "posts/2022/2022-05-23-nlpkagglecomp.html",
    "title": "Quickly trying out a NLP model for Kaggle Competition",
    "section": "",
    "text": "This is my attempt to see how well we can build a NLP model for Natural Language Processing with Disaster Tweets.\nAccording to competition you are required to :"
  },
  {
    "objectID": "posts/2022/2022-05-23-nlpkagglecomp.html#downloading-data",
    "href": "posts/2022/2022-05-23-nlpkagglecomp.html#downloading-data",
    "title": "Quickly trying out a NLP model for Kaggle Competition",
    "section": "Downloading Data",
    "text": "Downloading Data\n\ncreds = \"\"\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\n! kaggle competitions download -c nlp-getting-started\n\nnlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\n! unzip nlp-getting-started.zip\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"train.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      keyword\n      location\n      text\n      target\n    \n  \n  \n    \n      0\n      1\n      NaN\n      NaN\n      Our Deeds are the Reason of this #earthquake M...\n      1\n    \n    \n      1\n      4\n      NaN\n      NaN\n      Forest fire near La Ronge Sask. Canada\n      1\n    \n    \n      2\n      5\n      NaN\n      NaN\n      All residents asked to 'shelter in place' are ...\n      1\n    \n    \n      3\n      6\n      NaN\n      NaN\n      13,000 people receive #wildfires evacuation or...\n      1\n    \n    \n      4\n      7\n      NaN\n      NaN\n      Just got sent this photo from Ruby #Alaska as ...\n      1\n    \n  \n\n\n\n\n\ndf.describe(include=\"object\")\n\n\n\n\n\n  \n    \n      \n      keyword\n      location\n      text\n    \n  \n  \n    \n      count\n      7552\n      5080\n      7613\n    \n    \n      unique\n      221\n      3341\n      7503\n    \n    \n      top\n      fatalities\n      USA\n      11-Year-Old Boy Charged With Manslaughter of T...\n    \n    \n      freq\n      45\n      104\n      10\n    \n  \n\n\n\n\n\ndf[\"input\"] = df[\"text\"]"
  },
  {
    "objectID": "posts/2022/2022-05-23-nlpkagglecomp.html#tokenization",
    "href": "posts/2022/2022-05-23-nlpkagglecomp.html#tokenization",
    "title": "Quickly trying out a NLP model for Kaggle Competition",
    "section": "Tokenization",
    "text": "Tokenization\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\n\n\nds\n\nDataset({\n    features: ['id', 'keyword', 'location', 'text', 'target', 'input'],\n    num_rows: 7613\n})\n\n\n\nmodel_nm = \"microsoft/deberta-v3-small\"\n\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\ndef tok_func(x):\n    return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\nParameter 'function'=<function tok_func at 0x7f28da60b8b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n\n\n\n\n# collapse_output\nrow = tok_ds[0]\nrow[\"input\"], row[\"input_ids\"]\n\n('Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n [1,\n  581,\n  65453,\n  281,\n  262,\n  18037,\n  265,\n  291,\n  953,\n  117831,\n  903,\n  4924,\n  17018,\n  43632,\n  381,\n  305,\n  2])\n\n\n\ntok_ds = tok_ds.rename_columns({\"target\": \"labels\"})\n\n\ntok_ds\n\nDataset({\n    features: ['id', 'keyword', 'location', 'text', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 7613\n})\n\n\n\n# collapse_output\ntok_ds[0]\n\n{'id': 1,\n 'keyword': None,\n 'location': None,\n 'text': 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n 'labels': 1,\n 'input': 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n 'input_ids': [1,\n  581,\n  65453,\n  281,\n  262,\n  18037,\n  265,\n  291,\n  953,\n  117831,\n  903,\n  4924,\n  17018,\n  43632,\n  381,\n  305,\n  2],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
  },
  {
    "objectID": "posts/2022/2022-05-23-nlpkagglecomp.html#validation-traning-testing",
    "href": "posts/2022/2022-05-23-nlpkagglecomp.html#validation-traning-testing",
    "title": "Quickly trying out a NLP model for Kaggle Competition",
    "section": "Validation, Traning, Testing",
    "text": "Validation, Traning, Testing\n\neval_df = pd.read_csv(\"test.csv\")\neval_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      keyword\n      location\n      text\n    \n  \n  \n    \n      0\n      0\n      NaN\n      NaN\n      Just happened a terrible car crash\n    \n    \n      1\n      2\n      NaN\n      NaN\n      Heard about #earthquake is different cities, s...\n    \n    \n      2\n      3\n      NaN\n      NaN\n      there is a forest fire at spot pond, geese are...\n    \n    \n      3\n      9\n      NaN\n      NaN\n      Apocalypse lighting. #Spokane #wildfires\n    \n    \n      4\n      11\n      NaN\n      NaN\n      Typhoon Soudelor kills 28 in China and Taiwan\n    \n  \n\n\n\n\n\neval_df.describe(include=\"object\")\n\n\n\n\n\n  \n    \n      \n      keyword\n      location\n      text\n    \n  \n  \n    \n      count\n      3237\n      2158\n      3263\n    \n    \n      unique\n      221\n      1602\n      3243\n    \n    \n      top\n      deluged\n      New York\n      11-Year-Old Boy Charged With Manslaughter of T...\n    \n    \n      freq\n      23\n      38\n      3\n    \n  \n\n\n\n\n\nmodel_dataset = tok_ds.train_test_split(0.25, seed=34)\nmodel_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5709\n    })\n    test: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1904\n    })\n})\n\n\n\neval_df[\"input\"] = eval_df[\"text\"]\neval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
  },
  {
    "objectID": "posts/2022/2022-05-23-nlpkagglecomp.html#training-models",
    "href": "posts/2022/2022-05-23-nlpkagglecomp.html#training-models",
    "title": "Quickly trying out a NLP model for Kaggle Competition",
    "section": "Training Models",
    "text": "Training Models\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n\n\nbs = 128\nepochs = 4\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokz)\n\n\ntraining_args = TrainingArguments(\"test-trainer\")\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n\nSome weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=model_dataset[\"train\"],\n    eval_dataset=model_dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokz,\n)\n\n\ntrainer.train()\n\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: location, text, id, input, keyword. If location, text, id, input, keyword are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 5709\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2142\n\n\n\n\n    \n      \n      \n      [2142/2142 03:04, Epoch 3/3]\n    \n    \n  \n \n      Step\n      Training Loss\n    \n  \n  \n    \n      500\n      0.491000\n    \n    \n      1000\n      0.406300\n    \n    \n      1500\n      0.323600\n    \n    \n      2000\n      0.265800\n    \n  \n\n\n\nSaving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1500\nConfiguration saved in test-trainer/checkpoint-1500/config.json\nModel weights saved in test-trainer/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-2000\nConfiguration saved in test-trainer/checkpoint-2000/config.json\nModel weights saved in test-trainer/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-2000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=2142, training_loss=0.3674473464210717, metrics={'train_runtime': 184.9649, 'train_samples_per_second': 92.596, 'train_steps_per_second': 11.581, 'total_flos': 222000241127892.0, 'train_loss': 0.3674473464210717, 'epoch': 3.0})\n\n\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds\n\nThe following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: location, text, id, input, keyword. If location, text, id, input, keyword are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 3263\n  Batch size = 8\n\n\n\n    \n      \n      \n      [408/408 00:05]\n    \n    \n\n\narray([[-2.78964901,  3.02934074],\n       [-2.77013326,  3.00309706],\n       [-2.74731326,  2.972296  ],\n       ...,\n       [-2.8556931 ,  3.08512282],\n       [-2.7085278 ,  2.88177919],\n       [-2.7887187 ,  3.00746083]])\n\n\n1. Just happened a terrible car crash\n2. Heard about #earthquake is different cities, stay safe everyone.\n3. There is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all.\nThe above are samples from our Test set, looks all disaster tweets which seems to have been predicted correctly. This is my first iteration in which I tried mostly editing from Jeremyâ€™s notebook on getting started with NLP in about 1 hour."
  },
  {
    "objectID": "posts/2022/2022-08-04-fastai56.html",
    "href": "posts/2022/2022-08-04-fastai56.html",
    "title": "Practical Deep Learning for Coders Course - Paddy Disease Classification competition",
    "section": "",
    "text": "There are four notebook covering Paddy Disease Competition, and I feel each of these notebooks are excellent:\n\n\n\nImage of Paddy crop\n\n\n\nRoad to Top Part 1\nRoad to Top Part 2\nRoad to Top Part 3\nRoad to Top Part 4"
  },
  {
    "objectID": "posts/2022/2022-08-04-fastai56.html#how-was-paddy-competiton-for-me",
    "href": "posts/2022/2022-08-04-fastai56.html#how-was-paddy-competiton-for-me",
    "title": "Practical Deep Learning for Coders Course - Paddy Disease Classification competition",
    "section": "How was Paddy competiton for me?",
    "text": "How was Paddy competiton for me?\nThis competition is still ongoing at the time of writing. Yet during one month break during which Jeremyâ€™s live-coding sessions were held. I got some time to play with Paddy disease classification competition with this competition with other fastai friends also.\nAt the start of lesson 7, Jeremy said some kind words to Nick and me which can be found in below youtube video. I am extremely lucky to learn from such a great teacher. Without these four notebooks and Jeremyâ€™s live-coding sessions, it wouldnâ€™t have been possible.\n\n\n\nJeremy showing leaderboard in class\n\n\nA clip from original video\nI am not going in detail on this lesson, as I feel the introduction notebooks have covered everything so well. So signing off early this time."
  },
  {
    "objectID": "posts/2022/2022-06-27-tipstolearnvim.html",
    "href": "posts/2022/2022-06-27-tipstolearnvim.html",
    "title": "How to approach learning Vim - tips from two monks",
    "section": "",
    "text": "Early morning, the disciple woke up to learn from a monkâ€™s session at 6:30 AM to advance his skills. Today the monk was teaching about how to use vim, which is a great text editor.\nMonk said Vim is powerful and specifically data scientists should learn Vim because interactive text munching is what we do with input data files and output data files all the time."
  },
  {
    "objectID": "posts/2022/2022-06-27-tipstolearnvim.html#showing-the-power-of-vim",
    "href": "posts/2022/2022-06-27-tipstolearnvim.html#showing-the-power-of-vim",
    "title": "How to approach learning Vim - tips from two monks",
    "section": "ðŸ’ªShowing the power of Vim",
    "text": "ðŸ’ªShowing the power of Vim\nThe monk then went ahead and showed how he made Youtube chapter markers from the posts as shown below. The monk wanted to convert this comment to a format which is suitable for youtube to add chapter markers, so folks can easily watch it in future.\n00:00 Create a total empty notebook\n- How to be lazy and a great programmer? 03:02\n\n\n04:13 Create an empty notebook and symlink from persistence storage\n-  Why open a new window to use jupyter lab? to keep paperspace interface for shutting down when finished\n-  Why should you read paperspace docs? What did Jeremy find out? 05:16\n-  What are the interesting folders inside the root directory?\n-  How is storage folder different from notebooks directory? Why both of them exist for good reason? 06:32\n-  Should we worry about using pip install when paperspace uses conda a lot? 08:02\n-  How to pip upgrade packages into the home directory with --user? pip install -U --user fastcore 13:14\n-  What folder we want to be there next time when we open notebook? .local/\n-  How to save this .local/ into persistence storage? mv .local /storage/\n\n19:24 Create pre-run.sh from scratch to automate .local symlink from storage\n\n\n-  How to create a python file to setup the symlink first before running jupyter lab \n-  Does Jeremy think paperspace is the way to have easy to use GPU for fastai in \n-  How to make a symlink from /storage/ back to this notebookâ€™s /notebooks director\n-  What does this step above do? to link the /storage/ folder back into /notebook\n-  Why to access the /storage/ folder inside notebooks is useful?\n-  How to create a text file, edit it and save it inside /storage/ with jupyter lab?\n\nThe monk opened his Vim editor and showed a bunch of ways he can easily do this task very quickly with ex commands as shown in the gif below.\nFinally, the output will be like in this format:\n00:00 Create an notebook\n04:13 Symlink from persistence storage\n19:24 Create pre-run.sh from scratch to automate .local symlink from storage\nIf you are curious to learn the trick, follow the below three steps:\nStep 1: To delete and move to the start of the next digit type d /^tep 2: Then move down a row using j (or down arrow) and type dot â€˜.â€™ to repeat the movement: j.\nStep3: When itâ€™s almost over with no more chapter markers, type dG to delete till the end of the file."
  },
  {
    "objectID": "posts/2022/2022-06-27-tipstolearnvim.html#disciple-asks-how-to-approach-learning-vim",
    "href": "posts/2022/2022-06-27-tipstolearnvim.html#disciple-asks-how-to-approach-learning-vim",
    "title": "How to approach learning Vim - tips from two monks",
    "section": "Disciple asks how to approach learning Vimâ“",
    "text": "Disciple asks how to approach learning Vimâ“\nThis disciple was really impressed, yet intimidated at the same time seeing this and asked to the monk how can someone learn Vim and lotâ€™s of keyboard shortcuts he showed when someone is a beginner at this topic."
  },
  {
    "objectID": "posts/2022/2022-06-27-tipstolearnvim.html#first-monk-speaks",
    "href": "posts/2022/2022-06-27-tipstolearnvim.html#first-monk-speaks",
    "title": "How to approach learning Vim - tips from two monks",
    "section": "First Monk speaks",
    "text": "First Monk speaks\nThe trained monk replied as following:\nThe trick with learning something new is try and like learn in small chunks. So donâ€™t expect to learn all of Vim. At this point:\nStart by learning i to start inserting text, arrow keys to move around, escape to go back to command mode, and :wq to close and save. At that point you can use vim to edit your shell scripts and stuff. Then try and learn maybe one or two new commands each day like motion commands w and b are useful to move forward and backward a word.\nThere are lot of tutorials out there in internet like openvim.com is very helpful to learn Vim. You can work through tutorial like this.\nYeah honestly we all get intimidated, when we see an expert working with something that we donâ€™t know yet and at first itâ€™s like WOW thatâ€™s powerful I wish i could do that. You will be like my god you know how would i ever get to that point â€¦ the goal is not to be an expert at Vim the goal is to like be able to use Vim to like slowly do something that you want to be able to do.\nThis is one of the things the I really had to practice for myself in my late teens and early 20s was to repeatedly put myself in a position where I was intentionally doing things slowly by using a tool that I wanted to know and I was pretty sure at some point would be useful but I didnâ€™t know it well enough to do it faster than with other than some other tools.\nSo Iâ€™ve always you know since like 16 been pretty good at using Lotus 1-2-3 and Excel spreadsheets. I tended to turn to them for everything and then I wanted to learn SQL databases. So I kind of forced myself to do things involving lists with databases for a while even though I got slower and then I was like â€¦ Iâ€™m going to start doing more stuff with VBA macros and stop doing stuff manually and again it was kind of slower for a while and then became faster.\nParticularly like you know things like cleaning up that Youtube timestamp thing to create chapter markers. I could have done that manually you know and and the first ten times it would be faster to do it manually but donâ€™t do it manually right because each time you do it manually you know youâ€™re missing out on the opportunity to get better at the thing thatâ€™s going to make you faster.\nThe thing about practicing what you think might eventually be the fast way is that those fast ways accumulate together in kind of these multiplicative ways. So you know Iâ€™ve been kind of using this approach of always trying to do things the way I suspect would be the fastest if i was an expert at it. I have been doing that for like 30 years now.\nNow most people who watch me work go WOW youâ€™re very fast at doing stuff. You must be really smart you know. I am like Oh no Iâ€™m not really smart like you should have seen me when I started, I was terribleâ€¦ Yet now these things have all accumulated right.\nIf anybody finds you know good tutorials let me know honestly itâ€™s been a long time since Iâ€™ve run a vim tutorial. So I donâ€™t know any good tutorials and donâ€™t know the first one that came up in google is good or bad. I am sure though openvim.com is pretty good though.\n\nThoughts of Vim plugins\nI recommend donâ€™t install lots of plugins thereâ€™s lots of plugins you can install. After using vim for well over 20 years, I donâ€™t use any plugins at all. Itâ€™s not to say that there are none that are of any use â€¦ but like theyâ€™re not that useful honestly and you can get lost in that whole like customizing things thing. I just wanted to make it clear to say that actually out of the box Vim works fine and you customize with vimrc configurations."
  },
  {
    "objectID": "posts/2022/2022-06-27-tipstolearnvim.html#second-monk-speaks",
    "href": "posts/2022/2022-06-27-tipstolearnvim.html#second-monk-speaks",
    "title": "How to approach learning Vim - tips from two monks",
    "section": "Second monk speaks",
    "text": "Second monk speaks\nThis when another monk who is working in NVIDIA chimed in with tips to learn vim.\nAccording to me the best tutorial for learning Vim is in terminal. The vimtutor is a excellent tutorial. The first time I looked vimtutor is intimidating, it took me only on like third or fourth try does it start to make sense and was able to complete the entire thing.\nWhen i was learning the basics of them I realized Vim has a steep learning curve. I like to make things appealing, attractive and simple for me. So there is this game called vim-adventures.com.\nWhen I had my corporate job and instead of whatever people do on calls which is just you know browse Reddit. I would do play with website vim-adventures to learn more.\nAnother resource, which is really great is the Vim book: Practical Vim, Edit Text at the Speed of Thought. It is really well written and thereâ€™s a similar book for tmux so thatâ€™s thatâ€™s two book which made the really big difference for me personally."
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html",
    "href": "posts/2022/2022-05-01-audiocnndemo.html",
    "title": "Music genre classifier using fast.ai",
    "section": "",
    "text": "UPDATE - This project got featured in some of the cool project Jeremy Howard shared in lesson3 of fastaiv5 course.\nDuring first lesson of Practical Deep Learning for Coders course, Jeremy had mentioned how using simple computer vision model we can build even a model to classify audio with image classification model itself.\nRecently Kaggle grandmaster Rob Mulla conducted a challenge to classify music according to what genre it was. At stakes there was a RTX 3080 Ti GPU. Letâ€™s look how we can classify music genres using a simple computer vision model which was taught in the first lesson of fast.ai."
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#downloading-packages-and-importing-libraries",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#downloading-packages-and-importing-libraries",
    "title": "Music genre classifier using fast.ai",
    "section": "Downloading packages and importing libraries",
    "text": "Downloading packages and importing libraries\n\n\n\n\n\n\nNote\n\n\n\nI had already installed fastai, pytorch for training this model before hand.\n\n\n\n\nCode\n! pip install -Uqq kaggle git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[\"fastai\"]\n\n\n\nfrom fastai.data.all import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\nfrom huggingface_hub import push_to_hub_fastai"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#collecting-data",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#collecting-data",
    "title": "Music genre classifier using fast.ai",
    "section": "Collecting Data",
    "text": "Collecting Data\nIn this piece of code, I will show you how you can download datasets from Kaggle in general and the datasets I had used for training model. Inorder to train models in audio, first convert the audio to a spectogram and throw an image model. Check this tweet from Dien Hoa Truong who won a NVIDIA RTX 3080 Ti GPU in this competition.\n\ntwitter: https://twitter.com/DienhoaT/status/1519785308715462656\n\nFor this competition you need two datasets:\n\nThe competition data\nImage data generated from converting audio to melspectograms in form of images\n\nThe data provided here are over 20,000 royalty free song samples (30 second clips) and their musical genres. Your task is to create a machine learning algorithm capable of predicting the genres of unlabeled music files. Create features, design architectures, do whatever it takes to predict them the best.\nThe code for downloading data from kaggle has been adopted from Jeremyâ€™s notebook\n\ncreds = \"\"\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\npath = Path(\"../input/kaggle-pog-series-s01e02\")\npath.ls()\n\n(#6) [Path('input/kaggle-pog-series-s01e02/genres.csv'),Path('input/kaggle-pog-series-s01e02/sample_submission.csv'),Path('input/kaggle-pog-series-s01e02/test.csv'),Path('input/kaggle-pog-series-s01e02/test'),Path('input/kaggle-pog-series-s01e02/train.csv'),Path('input/kaggle-pog-series-s01e02/train')]\n\n\n\nfrom zipfile import ZipFile\nfrom kaggle import api\n\nif not path.exists():\n    api.competition_download_cli(str(path))\n    ZipFile(f\"{path}.zip\").extractall(path)\n\nDownloading kaggle-pog-series-s01e02.zip to /home\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.05G/9.05G [07:54<00:00, 20.5MB/s]\n\n\n\n\n\n\n\n\n\n! kaggle datasets download -d dienhoa/music-genre-spectrogram-pogchamps\n\nDownloading music-genre-spectrogram-pogchamps.zip to /home\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6.80G/6.80G [07:00<00:00, 14.7MB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.80G/6.80G [07:00<00:00, 17.4MB/s]"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#quick-eda-and-data-cleaning",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#quick-eda-and-data-cleaning",
    "title": "Music genre classifier using fast.ai",
    "section": "Quick EDA and Data Cleaning",
    "text": "Quick EDA and Data Cleaning\n\ndf_train = pd.read_csv(\"../input/kaggle-pog-series-s01e02/train.csv\")\ndf_train.head()\n\n\n\n\n\n  \n    \n      \n      song_id\n      filename\n      filepath\n      genre_id\n      genre\n    \n  \n  \n    \n      0\n      10150\n      010150.ogg\n      train/010150.ogg\n      7\n      Instrumental\n    \n    \n      1\n      7358\n      007358.ogg\n      train/007358.ogg\n      2\n      Punk\n    \n    \n      2\n      20573\n      020573.ogg\n      train/020573.ogg\n      5\n      Folk\n    \n    \n      3\n      11170\n      011170.ogg\n      train/011170.ogg\n      12\n      Old-Time / Historic\n    \n    \n      4\n      16662\n      016662.ogg\n      train/016662.ogg\n      1\n      Rock\n    \n  \n\n\n\n\n\ndf_train[\"filepath\"] = df_train[\"filepath\"].str.replace(\"ogg\", \"png\")\n\nShows a highly imbalanced dataset\n\ndf_train[\"genre\"].value_counts()\n\nRock                   3097\nElectronic             3073\nPunk                   2584\nExperimental           1801\nHip-Hop                1761\nFolk                   1215\nChiptune / Glitch      1181\nInstrumental           1045\nPop                     945\nInternational           814\nAmbient Electronic      796\nClassical               495\nOld-Time / Historic     408\nJazz                    306\nCountry                 142\nSoul-RnB                 94\nSpoken                   94\nBlues                    58\nEasy Listening           13\nName: genre, dtype: int64\n\n\n\ndf_train.head()\n\n\n\n\n\n  \n    \n      \n      song_id\n      filename\n      filepath\n      genre_id\n      genre\n    \n  \n  \n    \n      0\n      10150\n      010150.ogg\n      train/010150.png\n      7\n      Instrumental\n    \n    \n      1\n      7358\n      007358.ogg\n      train/007358.png\n      2\n      Punk\n    \n    \n      2\n      20573\n      020573.ogg\n      train/020573.png\n      5\n      Folk\n    \n    \n      3\n      11170\n      011170.ogg\n      train/011170.png\n      12\n      Old-Time / Historic\n    \n    \n      4\n      16662\n      016662.ogg\n      train/016662.png\n      1\n      Rock\n    \n  \n\n\n\n\n\ndf_train = df_train.set_index(\"song_id\")\n\n\n# I noticed some of images are missing music-genre-spectrogram-pogchamps images which are in training data. So removed it\ndf_train = df_train.drop(\n    [\n        23078,\n        3137,\n        4040,\n        15980,\n        11088,\n        9963,\n        24899,\n        16312,\n        22698,\n        17940,\n        22295,\n        3071,\n        13954,\n    ]\n)\n\n\ndf_train.shape\n\n(19909, 4)"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#loading-data-using-fastai-dataloaders",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#loading-data-using-fastai-dataloaders",
    "title": "Music genre classifier using fast.ai",
    "section": "Loading Data using fastai DataLoaders",
    "text": "Loading Data using fastai DataLoaders\nFor creating this notebook, I spend a major portion of my time in cleaning and sorting out appropriate datablocks/dataloaders for training image models using fast.ai. This is something which you as a practitioner experience, compared to learning all the theory and backpropogation algorithm.\nSo letâ€™s see how we load this data using fast.ai. There are two approaches which we will discuss below. Both the approaches of loading data works, but the first approach as a disadvantage, which I will tell in a moment.\nApproach 1. Using DataBlock and loading images\n\nCreate a data frame temp_train and create new column is_valid\nis_valid is default column named created for using ColSplitter\nNow set get_x which specifies the path of files for inputting data which is set as base_path+filename path: lambda o:fâ€™{path}/â€™+o.path\nNow set get_y which specifies the variable to predict, ie the genre of music\n\n\ntemp_train = df_train\ntemp_train.loc[:15000, \"is_valid\"] = True\ntemp_train.loc[15000:, \"is_valid\"] = False\n\n\npath = Path(\"../input/music-genre-spectrogram-pogchamps/spectograms/\")\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    splitter=ColSplitter(),\n    get_x=lambda o: f\"{path}/\" + o.path,\n    get_y=lambda o: o.genre,\n    item_tfms=Resize(224),\n    batch_tfms=aug_transforms(),\n)\n\n\ndls = dblock.dataloaders(temp_train)\n\n\ndls.show_batch()\n\n\n\n\n\n# Can be used for debugging\n# dblock.summary(df_train)\n\nThis worked really well, and with this approach I was even able to train a ML model which got 50% accuracy.\n\ntwitter: https://twitter.com/kurianbenoy2/status/1520470393760272384\n\nYet when it came to export models, due to usage of lamda method in DataBlock. I got Pickling error as the model was not able to be exported with learn.export() method.\n2. Using DataLoaders methods with loading from dataframe method\nThis issue got me into using approach that using ImageDataLoaders.from_df in fastai. Letâ€™s first take a look at our df_train dataframe:\n\ndf_train.head()\n\n\n\n\n\n  \n    \n      \n      filename\n      filepath\n      genre_id\n      genre\n    \n    \n      song_id\n      \n      \n      \n      \n    \n  \n  \n    \n      10150\n      010150.ogg\n      train/010150.png\n      7\n      Instrumental\n    \n    \n      7358\n      007358.ogg\n      train/007358.png\n      2\n      Punk\n    \n    \n      20573\n      020573.ogg\n      train/020573.png\n      5\n      Folk\n    \n    \n      11170\n      011170.ogg\n      train/011170.png\n      12\n      Old-Time / Historic\n    \n    \n      16662\n      016662.ogg\n      train/016662.png\n      1\n      Rock\n    \n  \n\n\n\n\nIf you look at the dataframe, we know that on appending to the path, the filepath column. - This is the exact value for get_x method in fastai fn_col = 1 which specifies the column name filepath at position 1. - label or get_y is specified by the column name genre at position 3. - valid_pct (ensure what percentage of data to be used for validation) - y_block=CategoryBlock to ensure itâ€™s used for normal classification only and not multi-label\n\ndls = ImageDataLoaders.from_df(\n    df_train,\n    path,\n    valid_pct=0.2,\n    seed=34,\n    y_block=CategoryBlock,\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(size=224),\n    fn_col=1,\n    label_col=3,\n)\ndls.show_batch()"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#training-fastai-model",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#training-fastai-model",
    "title": "Music genre classifier using fast.ai",
    "section": "Training fastai model",
    "text": "Training fastai model\nI trained using a resnet18 model at first, later we stepped up to use resnet50 model.\n\nlearn = vision_learner(dls, resnet50, metrics=error_rate)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\nlearn.fine_tune(10, 0.0008317637839354575)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.869285\n      2.171426\n      0.616428\n      01:43\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.312176\n      1.843815\n      0.558654\n      02:07\n    \n    \n      1\n      2.102361\n      1.719162\n      0.539061\n      02:08\n    \n    \n      2\n      1.867139\n      1.623988\n      0.527003\n      02:08\n    \n    \n      3\n      1.710557\n      1.527913\n      0.507661\n      02:07\n    \n    \n      4\n      1.629478\n      1.456836\n      0.479779\n      02:05\n    \n    \n      5\n      1.519305\n      1.433036\n      0.474253\n      02:05\n    \n    \n      6\n      1.457465\n      1.379757\n      0.464456\n      02:05\n    \n    \n      7\n      1.396283\n      1.369344\n      0.457925\n      02:05\n    \n    \n      8\n      1.359388\n      1.367973\n      0.453655\n      02:05\n    \n    \n      9\n      1.364363\n      1.368887\n      0.456167\n      02:04\n    \n  \n\n\n\n\nlearn.export(\"model.pkl\")"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#pushing-models-to-hugging-face",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#pushing-models-to-hugging-face",
    "title": "Music genre classifier using fast.ai",
    "section": "Pushing models to hugging face",
    "text": "Pushing models to hugging face\nhuggingface_hub has released two new functions to easily push fastai models to Huggingface Hub.\n\nUsing push_to_hub_fastai you can easily push the fastai Learner to huggingface.\nAdditionally, you can load any fastai Learner from the Hub using from_pretrained_fastai\n\nOmar Espejel had shared a fantastic notebook on these new functionalities in huggingface here.\n\n\n\n\n\n\nNote\n\n\n\nYou should install git-lfs and login to huggingface account with token before pushing\n\n\n\nfrom huggingface_hub import push_to_hub_fastai\n\npush_to_hub_fastai(\n    learn,\n    \"kurianbenoy/music_genre_classification_baseline\",\n    commit_message=\"Resnet50 with 10 epochs of training\",\n)\n\n/home/kurianbenoy/music_genre_classification_baseline is already a clone of https://huggingface.co/kurianbenoy/music_genre_classification_baseline. Make sure you pull the latest changes with `repo.git_pull()`.\n\n\n\n\n\nTo https://huggingface.co/kurianbenoy/music_genre_classification_baseline\n   390320d..3605083  main -> main\n\n\n\n'https://huggingface.co/kurianbenoy/music_genre_classification_baseline/commit/360508311005aefeb3ca29933f2173202afe4f30'\n\n\nIf you want to load this model in fastai and use it directly for inference just from_pretrain_fastai as shown in the below screenshot:\nfrom huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai(\"kurianbenoy/music_genre_classification_baseline\")"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#taking-a-look-at-results",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#taking-a-look-at-results",
    "title": "Music genre classifier using fast.ai",
    "section": "Taking a look at results",
    "text": "Taking a look at results\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\ninterp = Interpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(15, 10))"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#inference-function",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#inference-function",
    "title": "Music genre classifier using fast.ai",
    "section": "Inference function",
    "text": "Inference function\n\nfrom fastai.vision.all import *\nfrom huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai(\"kurianbenoy/music_genre_classification_baseline\")\n\n\ndef predict(img):\n    img = PILImage.create(img)\n    _pred, _pred_w_idx, probs = learn.predict(img)\n    labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)}\n    return labels_probs"
  },
  {
    "objectID": "posts/2022/2022-05-01-audiocnndemo.html#conclusion",
    "href": "posts/2022/2022-05-01-audiocnndemo.html#conclusion",
    "title": "Music genre classifier using fast.ai",
    "section": "Conclusion",
    "text": "Conclusion\nWe trained a ML model which can identify with 54.4% accuracy to classify in a music file which genre it is. Itâ€™s not so bad for a baseline model. Dien Hoa Truong has shared some techniques which he learned during Kaggle competition with music genres.\n\nThanks for reading :pray:"
  },
  {
    "objectID": "posts/2022/2022-05-10-fastai-53.html",
    "href": "posts/2022/2022-05-10-fastai-53.html",
    "title": "Practical Deep Learning for Coders Course - Lesson 3",
    "section": "",
    "text": "There was a minor delay in streaming the lessons today, as today the sessions where being conducted in-person by Jeremy at University of Queensland. There were 130 people watching live in youtube.\nJeremy started the lesson by saying that usually lesson 1 and 2 are easy for everyone, while itâ€™s usually from lesson 3 things start getting hard. There is also a lesson 0 on how to do fast.ai?\n\nI had the previously written about fastai lesson 0, where Jeremy mentioned about How to do fast.ai lesson through the following five steps:\n\nWatching lecture/book (watching the video first without trying anything)\nRunning notebook and experimentation (going through lesson notebooks and experimenting stuff)\nReproduce results (try with fastai clean notebook version, see if you are able to understand and do things on your own)\nWorking on a different dataset (play with a different dataset, paraticipate in kaggle â€¦)\n\nAlways studying done with other people is the best way to retain your knowledge. So itâ€™s great to participate in study groups like Delft-fastai sessions.\nThis week, Jeremy showcased the various students projects based on those who got highest number of votes in share your work here topic in fastai forums. My work also got featured ðŸ™‚ in the lesson.\n\n\n\nimage"
  },
  {
    "objectID": "posts/2022/2022-05-10-fastai-53.html#dogs-vs-cat-notebooks--which-image-models-are-the-best",
    "href": "posts/2022/2022-05-10-fastai-53.html#dogs-vs-cat-notebooks--which-image-models-are-the-best",
    "title": "Practical Deep Learning for Coders Course - Lesson 3",
    "section": "Dogs vs Cat notebooks- which image models are the best?",
    "text": "Dogs vs Cat notebooks- which image models are the best?\nToday Jeremy featured, paperspace gradient platform. He has been using it for his development and itâ€™s totally amazing. He got something done by them to update fastbook regularly.\n\n\n\n\n\n\nImportant\n\n\n\nIn lesson2 the main things, is not about taking a particular platform and deploying them through javascript websites or online applications. But the key thing is to undestand the concept. There are two pieces:\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe Training piece by end of which you get a model.pkl file. Once you got that (train.ipynb)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThen part which takes inputs, spits out output â€¦ this separate step is deploying (app.ipynb)\n\n\n\nFinding good image models, by baselines results along with inference time will help us choose good architecture. He tried levit_models, which didnâ€™t work really great.\nFrom [13:52] in the video, he experiments with convnext tiny models from timm library. It got really good accuracy with almost 0.05 loss. At the moment for computer vision there are lot of good architectures, which beats resnets really well. In this case for predicting 37 breeds of dogs we can find categories in dataset using vocab of dataloaders in model.\nlabels = model.dls.vocab\nItâ€™s very important to understand whatâ€™s in a model? Using get_submodule in pytorch we can look at the various neural networks, what is their input and ouput and each layers. [21:24]\nLetâ€™s explore the architecture of a translation model which translates from their target form to english.\n\nfrom transformers import AutoModelForSeq2SeqLM\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n\nLooking at model architecture\n\n# collapse_output\nmodel\n\nMarianMTModel(\n  (model): MarianModel(\n    (shared): Embedding(64172, 512, padding_idx=64171)\n    (encoder): MarianEncoder(\n      (embed_tokens): Embedding(64172, 512, padding_idx=64171)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): MarianDecoder(\n      (embed_tokens): Embedding(64172, 512, padding_idx=64171)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=64172, bias=False)\n)\n\n\nLooking at layer 1 self_attn_layer_norm\n\n# collapse_output\nattention_layer = model.get_submodule(\"model.encoder.layers.0.self_attn_layer_norm\")\nlist(attention_layer.parameters())\n\n[Parameter containing:\n tensor([0.3865, 0.6348, 0.6938, 0.7140, 1.1017, 1.0888, 0.7801, 0.7572, 0.7402,\n         0.5655, 0.5940, 0.7477, 0.6920, 0.6781, 0.5128, 0.5862, 0.7173, 0.5140,\n         0.5940, 0.5998, 0.5002, 0.5931, 0.3720, 0.8686, 0.6557, 0.7436, 0.7564,\n         0.5402, 0.6773, 0.6831, 0.7060, 0.8484, 0.8874, 0.9380, 0.7360, 0.6073,\n         0.7911, 0.6247, 0.6225, 0.7281, 0.7470, 0.8066, 0.6336, 0.5607, 0.6914,\n         0.7630, 1.0365, 0.5133, 0.8260, 0.9167, 0.6362, 0.6375, 0.7296, 1.0838,\n         0.7916, 0.8332, 1.0474, 0.9655, 0.9446, 0.8361, 0.9928, 0.7550, 0.8335,\n         0.9597, 0.3449, 0.6119, 0.9266, 0.8208, 0.7301, 0.9969, 0.4639, 0.6579,\n         1.0493, 0.9808, 0.9181, 0.7736, 0.7346, 0.9642, 1.2211, 1.3974, 1.3712,\n         1.4836, 1.2050, 1.1015, 1.3986, 1.4113, 1.3771, 1.5623, 1.5389, 1.0727,\n         1.5310, 1.3641, 1.5365, 1.4774, 1.4893, 1.4168, 1.5904, 1.5720, 1.3812,\n         1.5914, 1.5096, 1.2807, 0.1877, 1.3947, 1.6565, 1.2572, 1.7532, 1.7136,\n         1.5001, 1.7059, 1.6033, 1.5448, 1.5357, 1.5565, 1.5366, 1.3784, 1.6677,\n         1.6570, 1.6885, 1.6925, 1.5795, 1.6837, 1.7601, 1.6240, 1.8309, 1.6668,\n         1.7021, 1.7827, 1.8194, 1.8531, 1.9633, 1.7518, 1.9518, 1.8846, 2.0106,\n         1.9608, 1.8964, 1.9245, 0.0996, 1.8191, 1.8534, 1.7096, 1.7831, 0.1533,\n         2.0808, 1.8960, 2.1153, 1.8570, 2.0739, 2.1022, 2.0319, 1.3613, 1.9232,\n         2.1441, 2.0704, 2.1557, 2.1526, 2.2401, 2.0910, 1.8356, 2.1069, 1.7451,\n         0.1487, 2.1800, 2.1589, 2.0273, 0.1957, 2.2119, 2.1048, 1.4881, 1.7567,\n         2.2064, 2.1753, 2.2111, 2.1907, 2.1288, 1.8702, 2.1218, 2.1744, 2.2581,\n         2.2565, 2.1913, 2.0952, 2.2975, 1.9853, 1.9851, 2.1758, 2.1094, 2.0666,\n         2.0578, 1.7592, 2.1246, 2.1616, 2.1781, 2.1823, 2.4415, 2.0122, 1.9394,\n         2.1719, 2.1455, 2.3547, 1.0006, 2.1169, 1.6765, 2.2037, 2.1994, 2.2939,\n         2.1233, 2.1261, 2.1542, 2.1301, 2.0364, 2.2253, 2.1832, 2.2080, 2.0617,\n         2.2758, 2.1373, 2.2573, 2.0367, 2.2055, 2.2531, 1.9362, 2.1346, 2.3110,\n         1.8304, 2.2435, 2.0757, 2.1346, 2.0784, 2.2972, 1.9981, 2.2595, 2.3887,\n         2.3544, 2.1077, 2.2306, 2.2086, 1.6925, 2.1120, 2.2147, 2.2832, 2.1880,\n         2.0909, 2.1869, 2.3249, 2.2425, 2.2322, 2.2695, 2.3331, 0.1346, 0.2001,\n         1.9555, 2.0758, 2.0961, 2.2567, 0.4750, 0.5842, 0.7058, 0.7570, 0.9744,\n         1.0287, 0.9519, 0.8539, 0.6670, 0.0686, 0.5976, 0.6930, 0.7278, 0.5867,\n         0.5813, 0.7097, 0.5000, 0.6474, 0.5425, 0.5578, 0.5803, 0.6271, 0.6408,\n         0.0821, 0.6325, 0.8464, 0.9188, 0.7320, 0.1289, 0.1581, 0.7063, 0.8729,\n         0.7022, 0.8077, 0.7002, 0.6772, 0.5950, 0.6649, 0.7646, 0.4813, 0.7579,\n         0.5831, 0.4914, 0.7263, 0.5337, 0.5253, 0.7073, 0.3907, 0.7041, 0.6702,\n         0.4874, 0.5163, 0.2580, 0.6476, 0.5674, 0.4555, 0.5476, 0.5859, 0.6279,\n         0.4089, 0.5099, 0.5995, 0.5399, 0.7964, 0.4036, 0.6919, 0.6908, 0.5914,\n         0.5730, 0.6122, 0.4277, 0.4590, 0.7666, 0.9008, 0.3882, 0.1257, 0.6154,\n         0.6206, 0.1595, 0.6308, 0.4924, 0.5181, 0.5823, 0.2778, 0.8624, 0.2661,\n         0.7717, 0.9022, 1.2887, 1.2015, 0.6473, 0.4860, 0.4110, 0.4339, 0.5128,\n         1.1724, 1.1852, 1.2922, 1.0709, 1.2392, 1.2499, 1.4100, 1.3137, 0.8466,\n         1.4344, 1.4693, 0.6968, 0.1751, 0.1710, 0.1834, 1.4736, 1.6201, 1.3277,\n         1.6475, 1.4915, 1.5697, 1.4164, 1.7855, 0.0784, 1.7240, 1.5680, 1.7145,\n         1.9040, 1.7964, 1.9526, 1.9328, 2.0737, 1.9253, 1.7730, 2.2707, 2.0602,\n         0.4566, 0.5279, 2.1403, 2.0589, 2.0557, 2.1391, 2.1761, 1.8147, 2.0583,\n         1.8788, 2.0470, 2.0793, 2.0560, 2.2968, 0.2280, 2.2384, 0.1449, 2.3148,\n         2.2568, 2.1043, 2.2506, 0.1906, 2.1942, 2.3548, 2.2405, 2.1008, 2.2179,\n         2.2754, 0.6110, 0.2974, 1.9307, 2.1931, 2.0484, 2.0105, 2.1261, 2.0659,\n         2.1462, 2.1739, 1.9466, 2.4105, 2.2565, 2.0342, 2.1688, 0.2608, 2.0383,\n         2.0664, 1.9995, 2.1393, 2.2680, 2.0550, 2.2346, 1.9870, 2.0796, 1.9112,\n         1.2930, 2.2390, 2.1678, 0.1801, 2.0002, 1.6783, 2.0918, 2.3177, 1.8342,\n         1.9244, 1.9471, 2.2717, 2.1227, 2.2932, 2.3473, 1.7774, 2.0945, 2.3712,\n         2.1550, 2.0802, 0.1087, 2.2277, 1.9290, 2.2212, 2.0705, 1.8797, 2.1542,\n         0.3608, 2.1922, 2.1362, 2.1825, 1.9593, 2.1429, 0.2623, 1.6499, 2.0807,\n         2.0261, 2.1480, 1.9283, 0.1497, 2.1901, 2.0398, 2.0140, 2.5195, 2.0685,\n         1.4206, 2.0745, 2.2225, 0.1621, 2.2012, 0.4932, 2.0481, 2.1097, 2.3599,\n         0.4743, 1.9034, 2.2135, 2.0947, 2.1751, 1.7660, 2.4012, 2.1536, 1.9608,\n         2.1268, 1.9698, 2.2014, 2.3058, 2.1618, 1.8719, 1.9626, 2.2343],\n        requires_grad=True),\n Parameter containing:\n tensor([ 7.7839e-02,  1.4282e-01, -6.7494e-02,  6.3598e-02, -1.2071e-01,\n          7.2978e-02, -1.3550e-01,  3.5607e-02, -4.4458e-02, -4.4257e-03,\n         -3.3140e-01, -8.8216e-02,  2.0695e-01, -1.7521e-01, -7.0075e-02,\n         -2.3476e-01, -3.5785e-01, -4.3914e-01,  1.4167e-01, -9.0072e-02,\n         -1.6590e-01, -2.4325e-02, -9.6055e-02, -3.2896e-01,  1.2258e-02,\n         -1.0973e-03,  2.2662e-01, -1.3086e-02, -2.1918e-01, -4.5178e-02,\n         -1.9418e-01, -1.8878e-02, -1.3459e-02, -2.9698e-01, -3.9941e-02,\n         -9.4998e-02, -1.9507e-01, -4.1943e-02,  1.6590e-01, -1.1282e-01,\n          1.1039e-01,  2.5711e-02, -1.5641e-01,  5.5295e-02, -1.1544e-01,\n         -1.7157e-01, -2.8929e-01,  2.3132e-01, -2.6698e-01, -2.9870e-02,\n         -1.4797e-02, -1.4169e-01, -4.8199e-03,  1.4835e-02, -8.9909e-02,\n         -4.6198e-02, -2.8071e-01, -4.3290e-01, -1.6699e-01, -2.0422e-01,\n         -5.8818e-03, -2.2520e-01,  6.2375e-03,  3.9504e-02,  7.5439e-02,\n         -1.4287e-01, -5.1881e-01, -5.5721e-02, -5.5866e-02, -5.3829e-01,\n         -1.4044e-02, -9.2953e-02, -1.1587e-01, -3.8476e-02, -2.5480e-01,\n         -5.7539e-02, -2.8871e-01,  3.5020e-02, -3.1672e-02, -5.8393e-02,\n         -3.2713e-01, -1.8932e-01,  8.0913e-02, -4.6087e-01, -6.5291e-02,\n         -4.0539e-01,  6.4874e-02, -1.7552e-01,  4.5883e-02,  9.9371e-03,\n          1.4575e-02, -1.4779e-01,  3.0300e-01, -7.1591e-02, -3.0603e-02,\n         -5.1550e-02,  3.3196e-01, -2.6409e-01, -1.0252e-01, -9.0839e-02,\n         -6.5229e-02,  4.6278e-03,  6.9909e-01, -3.8764e-01, -1.8178e-01,\n         -1.6395e-01, -4.2978e-01, -9.3517e-02, -2.7543e-02, -1.2259e-01,\n         -2.8473e-01,  2.5956e-01, -2.6014e-01,  5.4886e-02, -2.7227e-02,\n         -1.3363e-01, -1.5168e-01,  8.5377e-02,  2.9195e-01,  2.1162e-02,\n         -3.9784e-02,  4.4097e-02,  9.6993e-02,  1.4139e-01,  2.4818e-01,\n          1.8267e-02, -1.1592e-01,  1.0816e-01,  7.5200e-02, -1.3003e-01,\n         -8.0244e-03,  5.8670e-02, -3.7428e-01,  2.2588e-01, -5.0269e-01,\n         -2.3895e-01,  8.2600e-02, -6.8347e-02, -1.0482e+00, -1.3551e-01,\n          1.1412e-02, -2.1185e-01, -2.4042e-01,  2.4737e-02, -2.5176e-01,\n          1.5020e-01, -2.3560e-01,  1.1241e-01, -6.4413e-02, -3.5118e-01,\n         -1.2333e-01,  1.9045e-01,  4.3384e-02, -2.4544e-01, -4.2071e-01,\n         -7.8986e-02, -4.2295e-02,  4.0794e-01, -3.2176e-01, -5.9337e-01,\n         -6.2764e-02,  1.0759e-01, -4.0607e-01, -1.3816e-01, -3.3327e-01,\n         -2.0288e-01, -3.6235e-01, -4.0601e-01, -3.3251e-01,  1.0679e-01,\n         -3.2651e-01, -4.5523e-01,  9.8463e-03, -1.7090e-01,  5.6157e-02,\n         -2.0125e-01, -1.0815e-01, -1.1430e-01, -4.0327e-02, -3.9167e-01,\n         -3.6428e-01, -4.4570e-01, -8.9959e-02, -4.9760e-01, -1.0579e-01,\n          1.3707e-01, -7.0252e-02,  2.7966e-02, -2.4773e-01, -5.1971e-04,\n          8.3816e-02,  2.1685e-02, -6.9780e-01,  2.2206e-02,  3.3752e-01,\n         -3.2891e-01, -7.8279e-02,  3.3331e-03, -1.5812e-01, -7.3529e-02,\n         -2.4885e-01,  7.1563e-03, -1.0669e-01, -9.1697e-02,  3.7219e-02,\n          2.2590e-01, -3.7476e-01,  8.3716e-02,  5.5841e-02, -3.0678e-01,\n         -3.4485e-01, -4.4003e-01,  1.9830e-01, -4.7639e-01, -6.4421e-02,\n         -2.7313e-01, -1.4385e-01, -1.4548e-01, -3.6821e-01,  2.6972e-01,\n         -3.0483e-01,  4.1683e-02, -2.3375e-02, -2.3032e-01, -4.5438e-01,\n         -2.6145e-01, -2.2000e-01, -5.7517e-02,  4.7594e-02, -9.9610e-03,\n         -4.2952e-01,  1.8124e-01, -1.1407e-01, -2.7262e-01, -1.1815e-01,\n         -2.3155e-01, -4.2597e-01, -4.4960e-01, -1.8752e-01, -3.0844e-01,\n          3.5617e-02, -3.7852e-01, -3.3136e-01, -1.9491e-01, -2.1862e-01,\n         -3.3167e-01,  2.6676e-01, -1.9840e-01, -3.3605e-01, -1.6330e-01,\n         -6.2717e-02, -8.3715e-01, -2.5243e-01, -1.3302e-01, -3.6257e-01,\n          5.8300e-01, -1.1160e-01, -1.1229e-01, -4.1968e-01, -1.0799e-01,\n         -1.9890e-01, -6.1067e-02, -2.9817e-01, -6.8028e-02, -1.3047e-01,\n         -8.3282e-01, -2.1888e-01, -1.1378e-01, -1.4994e-02, -3.3752e-01,\n          1.4736e-01, -2.0098e-01, -3.8907e-01,  1.4387e-01, -1.3784e-01,\n          1.6391e-02, -1.7244e-01,  7.5800e-02, -2.3648e-01, -3.8036e-01,\n          1.9662e-01,  7.4968e-02, -1.1686e-01, -3.6071e-01, -7.9299e-02,\n          1.8760e-01,  1.6195e-01, -3.2272e-01, -2.1438e-01, -7.2898e-02,\n          9.8829e-02,  7.1539e-02,  1.3703e-01, -1.5568e-01,  6.3408e-04,\n         -3.5787e-02, -2.7407e-01, -5.7378e-02, -2.0438e-01, -2.4371e-02,\n          1.7313e-01, -4.1306e-01, -9.4938e-02,  3.8556e-02, -2.3727e-01,\n          5.0274e-02, -5.2022e-02,  6.9763e-03,  1.2209e-01, -1.4279e-01,\n         -2.5014e-01, -1.8495e-02, -1.3463e-02, -2.4504e-01, -1.3166e-01,\n         -7.7291e-02,  7.7370e-02,  1.1513e-02, -7.0425e-02,  1.5736e-01,\n         -2.1174e-01, -4.2664e-02, -2.9207e-01,  3.2393e-02,  2.1656e-02,\n          9.9900e-02, -1.3805e-01,  2.5438e-01,  2.0831e-01,  3.6837e-02,\n         -3.3914e-03,  4.1395e-01,  5.6420e-02,  8.9263e-02,  2.1450e-02,\n         -5.5800e-02,  7.0606e-02, -4.1126e-02,  3.8725e-03, -1.5734e-01,\n          5.0738e-01,  1.5756e-02,  3.4117e-01, -3.4182e-01,  2.3014e-01,\n          2.9587e-02, -8.8264e-02,  3.3711e-01, -1.4313e-01,  1.5262e-01,\n         -8.7762e-02,  2.4450e-01, -2.0987e-01,  1.9820e-01,  1.7844e-01,\n          1.4303e-01, -5.0851e-02, -9.4576e-02,  1.8408e-02,  1.1286e-01,\n          3.3272e-01,  3.5103e-01, -4.2428e-02, -1.9907e-01,  9.6479e-02,\n          3.2967e-02, -1.9729e-01,  2.2756e-01,  8.3037e-02,  2.5401e-01,\n          2.9031e-01, -1.5839e-01, -1.3418e-02,  1.0571e-01, -3.5190e-01,\n         -8.5125e-02,  1.5848e-01,  2.5322e-01,  2.0388e-02,  1.4573e-01,\n          1.7365e-02,  3.1611e-01, -2.0127e-01,  8.0616e-02, -1.4502e-02,\n          6.7866e-01,  5.2572e-01,  6.3858e-02,  3.9846e-02,  5.1869e-01,\n         -7.9728e-03,  3.9597e-01,  4.7967e-01,  2.7590e-01,  9.2782e-02,\n          3.3310e-01,  2.2875e-01,  3.4428e-01,  4.6610e-01, -1.0366e-01,\n          3.4020e-01,  2.3838e-01,  3.1878e-01,  1.2648e-01,  5.1629e-01,\n          3.4091e-01,  4.3710e-01,  6.2221e-01,  1.7226e-01,  4.4662e-01,\n          4.0081e-01,  7.7952e-01,  4.0586e-01,  1.0278e+00,  3.0402e-01,\n          1.5113e-01,  8.0986e-02,  2.9811e-01,  6.0928e-01,  3.3816e-01,\n          5.8209e-01,  5.3371e-01,  3.8662e-01,  2.0641e-01,  3.6023e-01,\n          3.1196e-02,  4.9345e-01,  3.2226e-01,  2.7840e-01,  2.7691e-01,\n          9.6109e-01,  1.2737e-01,  4.1566e-01,  3.9062e-01,  3.0825e-01,\n          4.9397e-01,  4.5440e-01,  5.2856e-01,  2.1089e-01,  4.5024e-01,\n          3.9093e-01,  4.3543e-01,  1.2896e-01,  3.8236e-01,  5.7791e-02,\n          5.9610e-02,  3.2190e-01,  4.1077e-01,  6.7217e-01,  3.1503e-01,\n          4.5539e-01,  3.8127e-01,  3.7299e-01,  4.9606e-01,  5.1592e-01,\n          8.7739e-01,  1.2913e-01,  3.2640e-01,  5.1213e-01,  2.5983e-01,\n          3.1244e-01,  8.0140e-02,  3.2804e-01,  1.5592e-01,  4.3599e-01,\n          5.4296e-01,  3.3799e-01,  5.6262e-01,  9.3698e-01,  4.7990e-01,\n          4.9927e-02,  4.0214e-01,  5.5437e-01,  4.3915e-01,  1.3080e-01,\n          3.5957e-01,  6.5735e-02,  9.8948e-02,  4.7541e-01,  9.1836e-02,\n          3.4417e-01,  3.5615e-01,  4.0770e-02,  4.5717e-01,  6.4114e-01,\n          2.4542e-01,  5.0354e-01,  1.7951e-01,  6.0904e-01,  1.5958e+00,\n          2.1165e-01,  3.6238e-01,  2.0053e-01,  4.2348e-01,  6.8393e-01,\n          8.5349e-01,  1.3414e-01, -1.2184e-03,  4.1054e-01,  7.6441e-01,\n          6.1769e-02,  3.8833e-01,  3.6897e-01,  3.5290e-01,  2.8261e-01,\n          3.1730e-01,  4.8138e-01, -1.5993e-01,  3.7400e-01,  2.7083e-01,\n          2.0941e-01,  5.4596e-01], requires_grad=True)]\n\n\nLooking at shape of last layer\n\n# collapse_output\nfinal = model.get_submodule(\"model.decoder.layers.5.final_layer_norm\")\nfinal_paramaeters = list(final.parameters())\nprint(f\"{final_paramaeters = }\")\n\nfinal_paramaeters = [Parameter containing:\ntensor([ 9.2454,  9.3895,  9.3544,  9.0685,  9.2224,  9.8569,  9.3900,  9.4416,\n         9.4985,  9.2981,  9.5326,  9.2260,  8.8878,  9.4862,  9.5422,  9.3088,\n         9.6653,  8.9836,  9.5670,  9.0307,  9.4179,  9.8929,  9.3411,  8.9442,\n         8.3855,  9.0165,  9.5142,  9.5201,  9.2902,  9.5196,  8.8687,  9.3270,\n         8.7709,  9.5791,  9.4227,  8.9457,  9.4278,  9.2320,  9.5537,  9.3045,\n         9.2281,  9.1897,  8.9683,  9.3930,  9.1265,  9.2261,  9.1755,  9.2192,\n         9.1531,  9.2323,  9.1581,  9.3413,  8.4585,  9.3836,  9.7359,  8.8970,\n         9.4054,  8.9220,  9.2355,  9.6045,  9.6126,  9.4839,  9.2955,  9.2803,\n         9.5649,  8.8892,  9.4749,  8.8119,  9.3922,  9.0771,  9.7973,  8.9035,\n         9.7339,  9.1203,  9.5283,  8.9696,  8.4717,  9.3626,  9.3828,  7.9538,\n         8.8453,  9.0190,  9.3108,  8.3297,  8.7236,  8.8562,  9.1680,  8.8641,\n         7.8828,  8.7943,  8.4220,  8.8387,  9.3143,  8.1786,  9.1979,  9.0642,\n         8.2838,  8.6224,  8.8548,  8.2028,  8.3914,  9.4564, 10.2469,  9.0537,\n         8.7376,  9.3791,  8.5842,  8.4631,  8.6599,  8.8171,  7.8897,  8.6041,\n         8.4556,  8.9208, 10.1143,  7.9758,  8.2237,  8.5698,  9.2252,  8.1479,\n         8.0188,  8.9071,  8.1475,  9.6910,  8.2373,  8.2525,  8.6017,  8.4775,\n         7.6445,  8.5943,  8.4234,  9.5359,  7.9101,  9.0395,  8.2788,  9.1683,\n         8.9006,  9.3443, 10.6461,  8.7802,  8.7067,  8.1328,  8.4786,  9.5398,\n         8.9038,  8.7195,  8.6432,  8.6484,  8.0920,  7.6238,  8.0674,  9.1098,\n         8.9414,  8.5768,  8.5224,  8.2418,  8.2112,  8.5999,  8.4768,  8.9988,\n         9.0594,  8.4397,  7.2651,  8.8350,  8.4989,  8.2867,  9.2490,  8.9484,\n         9.0761,  9.4235,  8.6788,  8.3734,  8.5445,  8.6480,  8.5919,  8.7318,\n         8.9115,  8.3845,  7.7635,  8.0614,  8.0440,  8.3904,  9.2142,  8.9592,\n         8.3101,  8.5018,  8.3161,  8.6132,  8.5134,  8.6191,  9.2030,  8.4010,\n         8.6543,  8.9678,  8.5206,  8.7887,  8.4305,  8.9793,  8.4836,  8.3803,\n         8.5192,  9.0187,  8.2780,  8.4214,  8.5277,  8.3268,  8.6899,  8.8909,\n         8.5217,  8.8556,  8.1597,  9.0187,  8.8114,  9.0544,  8.1888,  8.0256,\n         8.2712,  7.8735,  8.3806,  8.3239,  8.1951,  8.1542,  8.8955,  8.1172,\n         8.7627,  8.6084,  8.8146,  8.5941,  8.4780,  7.9555,  8.5277,  8.8061,\n         8.1250,  8.5714,  8.6387,  7.6968,  8.5164,  8.5684,  8.8306,  8.1602,\n         8.7625,  8.7649,  8.5770,  8.8186,  8.6728,  8.8203,  8.8378,  8.8105,\n         8.2568,  8.4017,  9.9819,  9.0695,  8.9472,  8.4494,  7.6861,  8.1042,\n         9.4347,  9.3720,  9.0644,  9.1978,  9.8322,  9.0001,  9.1845,  9.4331,\n         9.3469, 11.0728,  9.3463,  8.5851,  9.6459,  9.1978,  9.2272,  9.5648,\n         9.5100,  9.6435,  9.5191,  9.8178,  9.3789,  9.5861,  9.2071,  9.2581,\n         8.5441,  9.6824,  9.0314,  9.2823, 10.2148, 10.1498,  9.3458,  8.9451,\n         9.7831,  9.0849,  8.7979,  9.0224,  8.8580,  9.6999,  9.0158,  9.4426,\n         9.2253,  9.1951,  9.4550,  9.1783,  9.5661,  9.3228,  9.4391,  9.2358,\n         9.1685,  8.8517,  9.4883,  9.0652,  9.4498,  8.6077,  9.7002, 10.4473,\n         9.9884,  8.8662,  9.4317,  9.2922,  9.0668,  9.7620,  9.2281,  9.4860,\n         9.6106,  8.0309,  8.9221,  9.0221,  9.0459, 10.2337,  9.7973,  9.5885,\n         9.0249,  8.8571,  8.7396,  8.9452,  9.2020,  9.1573,  8.4453,  9.3205,\n         8.6279,  8.8441,  8.9208,  9.7410,  8.9751,  9.3891,  9.5010,  8.9050,\n         8.8219,  8.4705,  9.4688,  9.2351,  9.1935,  9.7405,  9.1623,  8.1793,\n         8.0767,  8.1733,  8.9422,  8.4693,  8.9346,  9.1120,  8.0441,  9.5878,\n         9.5636,  8.8612,  9.0740,  9.1084,  9.7573,  9.8492,  9.6772,  9.1868,\n         8.7703,  8.4915,  8.4426,  8.7710,  9.0574,  8.4157, 10.3115,  9.0996,\n         8.5651,  9.0585,  8.4534,  8.7063,  8.4291,  8.3241,  7.9195,  9.0210,\n         8.3222,  8.5985,  8.7874,  9.1164, 10.2389,  7.7741,  8.5940,  9.1308,\n         9.3498,  8.7384,  8.3300,  8.2650,  8.7969,  8.6335,  8.6550,  8.7559,\n         8.2821,  8.7692,  8.7830,  8.4424,  8.6879,  8.6025,  8.6327,  8.8367,\n         9.4620,  8.5763,  8.3675,  8.4179,  9.2793,  8.8078,  9.3775,  9.6580,\n        10.1902,  8.9006,  8.5452,  8.6059,  8.5685,  8.4081,  9.1445,  8.5781,\n         8.9791,  8.7608,  8.6678,  8.4435,  7.6760,  8.6099,  8.8083,  8.1700,\n         8.5081,  8.1777,  9.2411,  8.9585,  8.1853,  8.3657,  7.9898,  8.8000,\n         8.1188,  9.3628,  8.9330,  7.7698,  9.6513,  9.2959,  9.1233,  9.0433,\n         8.2871,  8.7241,  8.2236,  8.3967,  8.2571,  9.3786,  8.6354,  8.7345,\n         8.3856,  8.4556,  8.7689,  8.7359,  8.6211,  9.7834,  8.9445,  8.8958,\n         8.1290,  8.5490,  9.0263,  8.3258,  8.2379,  8.8249,  8.7301,  8.6340,\n         9.3168,  8.7775,  9.9242,  8.9798,  9.1412,  8.5955,  8.1734,  8.9969,\n         9.5123,  9.0581,  8.2497,  8.3555,  9.3501,  8.7719,  8.4376,  8.8456,\n         8.2080,  8.9806,  8.5660,  9.1352,  8.5920,  8.2595,  8.1272,  9.0418,\n         8.6972,  8.3413,  8.2742,  8.3118,  8.2167,  8.5550,  8.7187,  8.8749,\n         9.7556,  8.4383,  9.0293,  8.1725,  8.5115,  8.9174,  8.9519,  9.0915],\n       requires_grad=True), Parameter containing:\ntensor([-1.6685e+00, -6.0155e-01, -5.9975e-01,  8.4297e-01,  8.5853e-01,\n         5.6530e-02, -1.2840e+00, -5.1519e-01,  1.6774e+00,  3.2501e-01,\n         1.4737e-01, -9.6427e-01,  2.1513e-01,  9.5219e-01, -3.7011e-03,\n         6.6861e-01,  7.9758e-01,  2.4703e-01, -9.5743e-02,  1.9413e-01,\n        -4.1348e-01, -8.3267e-01,  9.7684e-01, -5.1446e-01,  5.3158e-01,\n         1.0447e+00,  1.7422e-01,  1.8719e+00,  7.0798e-01, -5.2600e-01,\n         3.0636e-01,  3.1010e-01, -6.3830e-02, -2.3082e-01,  1.1787e+00,\n        -2.5507e-01, -1.2747e+00,  7.3436e-01, -6.5267e-01,  1.0654e+00,\n         7.2399e-01, -1.2560e+00, -6.7986e-01, -2.0358e-01, -2.1730e-01,\n         5.1018e-02,  3.6179e-01,  2.0001e+00, -6.3287e-01,  1.5726e+00,\n         2.8116e-01, -5.0017e-01, -1.6484e+00, -9.0159e-01, -2.5041e-01,\n        -1.7400e-01,  6.4630e-01,  5.9313e-02, -7.2617e-03,  5.0565e-01,\n         1.8716e+00, -8.8190e-01, -1.5941e-02,  7.8757e-02, -7.3102e-01,\n        -4.5485e-01,  1.1036e+00, -3.2698e-01, -8.0969e-01, -6.6129e-01,\n        -6.8337e-01, -1.6216e-01, -9.3829e-02,  6.4593e-01, -1.3784e+00,\n         5.6243e-01,  8.1852e-01,  1.3817e-01,  5.7122e-01, -7.8534e-01,\n        -9.2640e-01,  1.3659e-01, -6.8277e-01,  8.1809e-01, -1.4720e-01,\n        -2.1538e+00, -7.1303e-02,  3.9166e-01, -7.9192e-01,  1.0671e+00,\n         1.1110e+00,  9.8533e-01, -4.9213e-01, -8.4603e-01, -1.1119e+00,\n         1.6191e+00,  7.9375e-02, -1.0472e-01, -5.4553e-01, -2.3597e-01,\n        -2.6790e-01, -1.5157e+00, -2.6880e+00,  1.6904e-01,  2.3876e-01,\n        -5.1432e-01,  5.7074e-01,  1.5021e+00, -1.7612e+00, -5.1162e-01,\n         1.8071e+00, -2.2087e-01,  2.1651e-01,  3.1280e-01, -7.8104e-01,\n        -2.3347e-01,  2.3287e+00,  4.3430e-01,  6.7748e-02, -7.1022e-01,\n         1.3716e+00, -6.8236e-01, -1.9249e-02,  6.1708e-01,  3.5377e-01,\n        -3.0060e-01,  8.7717e-01,  7.6281e-02,  1.6436e+00,  6.5745e-02,\n         1.3911e+00, -1.1550e+00, -1.0942e+00, -5.4705e-02, -3.8439e-01,\n        -2.0564e-01, -4.0284e-01,  1.8441e+00,  1.9942e+00, -3.3832e-01,\n        -8.4892e-02,  2.6425e-01, -1.2417e-01, -8.9078e-01,  9.9491e-01,\n        -1.2496e-01,  1.8860e-01, -1.9992e-01,  1.2828e+00, -1.6894e+00,\n         1.7569e+00, -1.2428e-01, -6.2974e-01,  9.5339e-01,  5.5913e-01,\n         8.3872e-01,  3.8710e-01,  4.7107e-01,  8.8813e-01,  1.5112e+00,\n         6.4772e-02,  2.2407e+00, -2.4373e+00,  5.4596e-02, -2.3119e+00,\n         7.8280e-01, -1.9582e+00, -4.4601e-01, -7.2071e-01,  1.0691e+00,\n        -6.3960e-01, -9.6271e-01,  2.2167e+00,  1.6286e+00,  1.8287e-01,\n        -1.0599e+00,  8.2727e-01,  4.2197e-01, -1.7488e-01,  2.2607e+00,\n         1.6864e+00,  1.5625e+00, -2.4543e-01,  1.7482e-01, -1.4680e+00,\n        -6.5810e-01, -1.7268e-01,  4.3401e-02,  1.2926e+00,  4.0332e-01,\n         1.2770e-01, -5.4604e-02,  6.3163e-01,  5.8788e-01,  3.2761e-01,\n         5.9546e-01, -1.4995e-02, -2.2789e-01, -3.0784e-01, -1.0060e-01,\n        -1.6770e-01, -1.0096e+00,  9.2021e-01, -8.9897e-01, -5.9694e-01,\n         8.2038e-01, -9.0749e-01, -3.0484e-01,  3.2038e-01,  1.2042e+00,\n         6.0027e-01,  1.8709e-02, -4.0982e-01,  9.0638e-01, -9.6504e-01,\n        -6.3824e-01, -2.3503e-02, -2.9762e-01,  1.1074e+00,  1.2170e-01,\n         1.1205e+00, -1.9938e-01, -2.7814e-01, -3.8689e-01,  1.1914e+00,\n        -6.5604e-01,  7.1130e-02, -7.0655e-01,  1.4939e+00, -2.6654e-01,\n         4.9578e-01, -1.8316e+00, -6.2531e-01,  2.2550e+00, -9.1826e-01,\n         2.1526e+00,  1.7631e-01,  1.2235e+00, -9.9429e-01, -8.9968e-01,\n        -9.7487e-01, -3.5716e-01, -3.8364e-01, -2.2766e+00, -1.4803e+00,\n         2.7549e-01, -5.8828e-01, -4.4274e-01,  2.0661e-02,  9.6894e-01,\n        -5.4657e+00,  3.6806e+00, -5.8913e-01,  6.1390e-02,  9.8940e-01,\n         1.8229e+00,  3.6467e-01,  2.9497e-01,  2.1930e+00,  1.8576e+00,\n        -7.6800e-01,  1.3635e+00,  2.8457e-01,  2.9478e-02, -1.5696e+00,\n         6.0662e-01, -1.1586e+00,  7.8294e-01,  3.4371e-01,  1.4571e-01,\n        -4.5860e-01, -1.1644e+00, -1.2903e-01, -1.0055e+00, -5.4373e-02,\n         1.3311e+00, -1.2074e+00,  8.7602e-02,  8.2454e-01, -2.2496e+00,\n         2.4152e+00, -7.4065e-02,  3.5327e-01,  1.2092e+00,  6.9553e-02,\n         2.4961e+00, -1.5597e+00,  4.1607e-01, -7.9795e-02, -4.4723e-01,\n         2.6720e-01, -1.9072e+00, -6.5835e-01, -2.5336e-01, -9.1617e-01,\n         8.8624e-01, -6.2251e-01,  9.6169e-01,  1.1279e+00, -5.6577e-01,\n         1.8407e-01,  6.5294e-01, -6.1990e-01,  7.9014e-01, -6.0878e-01,\n         1.0077e+00,  1.2790e+00, -1.3704e-02,  7.4945e-02,  5.6748e-01,\n         1.0100e+00, -2.2963e-01, -9.2723e-01, -3.3553e-01, -7.0238e-01,\n        -2.3026e+00, -5.3322e-02, -9.2703e-01,  1.4448e+00, -8.7800e-01,\n        -6.4034e-01, -1.2203e+00, -1.1720e+00,  4.9662e-01,  3.4336e-01,\n        -1.3538e+00,  4.1525e-01, -6.6715e-01,  4.1263e-01, -4.0352e-01,\n        -3.7377e-01,  2.3441e+00,  3.5528e-01, -3.1402e-01,  3.5890e+00,\n         2.8886e-02,  3.1700e-01, -7.7702e-01,  4.6834e-01,  5.4264e-01,\n        -1.0964e+00,  1.4711e+00,  9.3168e-01,  5.4778e-01, -7.4466e-01,\n         7.7792e-01,  1.5176e+00,  1.6450e+00,  2.6295e-02, -1.8510e+00,\n         2.2687e-01,  1.3993e-01,  1.1727e+00,  6.4835e-02,  1.9505e-01,\n         2.2950e-01, -1.3806e+00,  7.7071e-02, -1.8424e+00, -9.5833e-01,\n        -8.7708e-01,  9.1619e-01,  1.0074e+00,  8.0151e-03,  1.0098e+00,\n        -3.9247e-02, -2.7759e-01, -2.1021e+00, -4.1539e-01, -1.5258e-01,\n         3.3655e-01, -2.6506e-01,  2.1964e+00,  6.0517e-01,  5.7097e-01,\n         7.5984e-02,  1.0848e+00,  4.8223e-01,  8.0175e-01, -9.1310e-01,\n         6.3781e-01,  1.1286e-01,  1.3899e+00, -4.5585e-01, -8.9240e-01,\n        -5.6478e-01, -1.0510e+00, -6.3237e-01,  7.5205e-01, -5.0555e-01,\n        -4.2338e-01,  1.1653e+00, -4.3769e-01, -4.9660e-01,  8.4734e-01,\n         3.1255e-01,  1.4222e+00,  5.1850e-01,  5.9261e-03,  6.8774e-01,\n        -2.2485e+00, -2.1259e-01,  1.7378e-01, -3.9461e+00,  8.5505e-01,\n        -1.4455e+00,  2.2031e-02, -8.7173e-01,  9.4395e-01,  1.3690e+00,\n         9.2501e-01,  5.9211e-01,  5.9655e-01, -9.7749e-01,  5.1079e-01,\n         1.7735e-02,  3.1332e-01,  2.8223e-01,  2.2100e-01,  9.7640e-01,\n         7.5128e-01, -1.2068e+00,  8.0254e-01,  4.7232e-01, -5.7225e-01,\n         3.0082e-01, -4.5279e-01, -3.4367e-01, -2.8903e-01,  1.1790e+00,\n        -2.3224e+00,  7.0363e-01,  4.5137e-01,  1.5505e+00,  8.4144e-01,\n         3.9210e-02, -9.5217e-01, -9.1495e-01, -3.6971e-01,  1.3037e-01,\n         1.0739e+00, -5.2155e-02, -1.7844e+00, -6.9291e-01,  6.2565e-01,\n        -1.6121e+00, -4.0668e-01,  6.9844e-01,  2.1026e-01, -3.4400e-01,\n        -2.3706e-02, -4.4798e-01,  6.0481e-01,  7.8424e-01,  6.2746e-01,\n        -7.7199e-01,  2.0300e-01,  9.1969e-01, -1.1502e+00, -3.1036e-01,\n         3.8410e-01,  3.3024e+00,  9.6322e-02,  3.5212e-01,  1.4104e+00,\n        -2.7992e-01,  4.1524e-01, -1.1456e+00, -2.6424e-01, -6.5836e-02,\n        -5.0440e-01,  5.7824e-01, -7.8925e-01, -2.0960e+00, -1.2973e-01,\n         1.0862e+00,  1.3762e+00, -3.2528e-02, -2.2924e+00, -8.9146e-01,\n        -3.0597e+00,  6.0693e-01, -2.5389e-01, -2.9927e-01,  3.3115e-01,\n        -4.1729e-01,  1.3418e+00,  8.3576e-01, -1.0882e+00,  1.0617e+00,\n        -2.8175e-01,  1.1439e+00, -4.9022e-01, -1.1799e-01, -4.8219e-01,\n         9.3034e-02,  1.2776e+00, -1.2725e-01,  5.8007e-01,  1.3756e+00,\n         1.2398e-01, -3.1594e-01, -7.3134e-02,  4.6101e-01,  1.4797e-01,\n        -8.3583e-01, -1.8117e+00,  1.3540e-01,  1.4121e-01,  5.1246e-01,\n         1.6791e-01, -1.5676e+00], requires_grad=True)]"
  },
  {
    "objectID": "posts/2022/2022-05-10-fastai-53.html#looking-at-how-neural-network-really-work",
    "href": "posts/2022/2022-05-10-fastai-53.html#looking-at-how-neural-network-really-work",
    "title": "Practical Deep Learning for Coders Course - Lesson 3",
    "section": "Looking at how neural network really work?",
    "text": "Looking at how neural network really work?\npartial in python is something which is usually used in lot of languages. Itâ€™s just subsituting value of x, in a function which is partial filled with already existing function.\nJeremy explained with this notebook today\nI followed along the notebook, making slight changes in variable names, function names, along with changing defined value to expirement with how does a neural network really work notebook version on my own.\n\n\n\n\n\n\nNote\n\n\n\nThanks to Alex Strick for sharing this trick when working with notebooks during delft-fastai sessions.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUsually when I explain this is how neural networks work, one of my students said this is like how we draw an outline of an owl. In deep learning â€¦, there is just step 1 where we draw outline, computer automatically does the step2 by drawing a beautiful owl."
  },
  {
    "objectID": "posts/2022/2022-05-10-fastai-53.html#intuitive-understanding-with-neural-networks",
    "href": "posts/2022/2022-05-10-fastai-53.html#intuitive-understanding-with-neural-networks",
    "title": "Practical Deep Learning for Coders Course - Lesson 3",
    "section": "Intuitive understanding with neural networks",
    "text": "Intuitive understanding with neural networks\nUsing RELUs we can tweak our function in such a way to fit the data. What neural networks, with a bunch of RELU functions does is it helps to optimize in such a way to fit any swiggly line or complex things which neednâ€™t be always quadratic.\n\n\n\n\n\n\nImportant\n\n\n\nFor Linear algebra, almost all time you need is matrix multiplcation. In schools, you learn linear algrebra as if you need tons of experience to do machine learning. Yet itâ€™s this operation of matrix multiplication that GPUs are so good at it, and there are even tensor cores for this.\n\n\nRefresher on matrix multiplication\nUsing Titanic dataset,see who survived and who didntâ€™ with excel to understand neural networks. In video from [1:05:00].\nNext week, we are going to look into how validation sets and more into metrics. We will be looking into Kaggle notebook on how to get started with NLP."
  },
  {
    "objectID": "posts/2022/2022-02-16-first_vacation.html",
    "href": "posts/2022/2022-02-16-first_vacation.html",
    "title": "First Vacation Experience",
    "section": "",
    "text": "I had taken my longest vacation from work in past week for four days. It was really enjoyable, and I was planning to do that for a long time. Yet due to Covid lockdown and bunch of other things it was late.Also the planning part for the vaction was really interesting at the same time hectic. My initial plan was to visit a old palace ie Padmnapuram palace in Thuckalay, Tamil Naidu. Unfortunately, it didnâ€™t work out.\n\n\n\nimage\n\n\nSo I pivoted my plan to stay in any hotel/resort. Before telling about the experience let me tell, what motivated me to take my vacation at the first place. It was an article by Randy Au who gave a really useful insight why taking vacation is important with a realistic angle. In the article the lessons on why we are not important to any organization and planning aspect is well covered. Randy Au answered my question on how to treat travelling as a hobby as well when I asked in a Twitter AMA.\nI applied for four days of leave well in advance before planning where I am going to plan. Then I started my journey of hunting the perfect place to stay. After almost 20+ hotel enquiries and also trying to accomodate everyone in my family to come with me. The outcome was kind of final. Unfortunately not everyone in my family was willing to come with me except my mother. After a lot of searching we decided to book the hotel at Mango Meadows and set aside one day to travel with my family in Kochi(by spending time on shopping & a buffet dinner).\nAbout the resort I stayed. Mango meadows resort is an excellent place and is an agricultural themed park. The place is filled with natural beauty and bought me closer to nature during the stay. Unlike typical resorts where you have just one small area, here you have 30 acres of land with lot of facilities like a amusment park. For me some of the memorable experiences during two day stay at Mango Meadows was swimming in pay pool, then cycling followed by bumpy ride in london metro. Staff were excellent and 5 stars review for the resort.\nSo was all the travelling really worth it? Back to work next day back I had lot of messages and 70+ emails to answer. This proved everything Randy said, and slowly one day I will also get better at vacationing. Also we started planning for our next big trip to Armenia which will materialize one day hopefully."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html",
    "href": "posts/2022/2022-05-19-fastai-54.html",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "",
    "text": "Almost 100+ people watched live virtually and lesson were held live in front of a bunch of audience in University of Queensland. Prof. John Williams opened session by telling about filling a separate form, for people interested in attending the hackathon organized end of the course.\nDuring the start Jeremy mentioned he would love folks to organize a online hackathon by community for folks attending remotely as well. Yet right now Jeremy and John doesnâ€™t have the capacity to organize one.\n\nTodays lesson is something a lot of regulars of fast.ai course are excited about as it covers really new material on transformers."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#why-using-a-different-framework---transformers",
    "href": "posts/2022/2022-05-19-fastai-54.html#why-using-a-different-framework---transformers",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "Why using a different framework - Transformers",
    "text": "Why using a different framework - Transformers\nSince this course is fastai, it may feel a bit weird when we are today going to use a different library called transformers.\n\n\n\n\n\n\nImportant\n\n\n\nAs practitioners, itâ€™s important for us to learn more than one framework.\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferences with fastai and transformers:\n\n\n\nTransformers provide lot of state of art models, and the Tokenizers library build with Rust is really good at the moment.\nItâ€™s good to get exposure to a library which is not so layered like fast.ai, which is reason that makes it super useful for beginners."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#ulmfit-architecture",
    "href": "posts/2022/2022-05-19-fastai-54.html#ulmfit-architecture",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "ULMFiT architecture",
    "text": "ULMFiT architecture\nThe idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm called Universal Language Model Fine-tuning for Text Classification aka ULMFiT which was first presented actually in a fastai course.\n\nWhatâ€™s a pretrained model and what is finetuning?\nConsider finetuning, as tweaking functions in such a way when if you are already some values of a, b lever are good and optimal for a particular function. Then tweaking value of c is easier right?\n\n\n\n\n\n\nImportant\n\n\n\nA pre-trained model is a bunch of parameters that have already been fitted, where some of them weâ€™re already pretty confident of what they should be, and some of them we really have no idea at all. And so fine-tuning is the process of taking those ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit.\n\n\n\n\nSteps in ULMFiT\nULMFiT archtecture consits of three steps:\n\nTraining a language models with general dataset like wikipedia. So it gets so good in predicting next words. Now in transformers one big difference compared to ULMFiT is we use masking instead of predicting next word\nIMDB lnagage build a language model, build on top of LM for wikipedia\nIn three step, is where model classifier comes and based on this label sentences as postive, negative etc."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#fundamental-libraries-in-ml",
    "href": "posts/2022/2022-05-19-fastai-54.html#fundamental-libraries-in-ml",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "Fundamental libraries in ML",
    "text": "Fundamental libraries in ML\nFour fundamental libraries you always need in datascience are:\n\nNumPy\nPandas\nmatplotlib\nPytorch\n\n\n\n\n\n\n\nImportant\n\n\n\nIt looks pretty cool, if you build the state of art stuff. Yet if you donâ€™t know fundamentals, you will encounter trouble. So i will recommend you to get started by first complete reading the Deep Learning for Coders book, then the Macinskey book on Python for Data Analysis, 3E which is free completely online."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#nlp-notebook-tokenization",
    "href": "posts/2022/2022-05-19-fastai-54.html#nlp-notebook-tokenization",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "NLP notebook tokenization",
    "text": "NLP notebook tokenization\nGetting started with NLP for absolute beginners\nItâ€™s been only a year or two since NLP has been getting good results, for computer vision things are being optimistic for a long time now.\n\nTokenization, is converting the text blurbs into a set of small tokens.\nNumericalization is the process of converting these tokens to numbers for models to train\n\nWe used deberta-v3 as base model as some models are always found to give good results. Yet there are lot of pretrained models available in public which can just found by searching like Patent for patent models in Huggingface models hub.\n\n\n\n\n\n\nNote\n\n\n\n(Jeremy) For under 2000 words use transformers approach, for more than 2000 words per sequence it would a good idea to try ULMFiT also along with transformers."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#test-validation-training-dataset",
    "href": "posts/2022/2022-05-19-fastai-54.html#test-validation-training-dataset",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "Test, Validation, Training Dataset",
    "text": "Test, Validation, Training Dataset\nThe most important concept in ML is creating:\n\nTest set\nValidation set\nTraining set\n\n\n\n\n\n\n\nImportant\n\n\n\n(Jeremy) Kaggle competitions are really a good way to create a good validation setâ€¦ Beginners generally tend to overfit â€¦ In real world outside of kaggle you often wonâ€™t know itâ€™s overfit. You just destroy value for organizations silentlyâ€¦ You really donâ€™t get it untill you screw it up a few times.\n\n\nHow (and why) to create a good validation set\nTest Set is a separate data which is not used by ML model for learning. Itâ€™s kept as separate hold out dataset for further testing."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#understanding-metrics",
    "href": "posts/2022/2022-05-19-fastai-54.html#understanding-metrics",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "Understanding metrics",
    "text": "Understanding metrics\nWith the validation set, we are measuring some metrics like accuracy which tell how good our ML model is. In Kaggle for every competition there is a metric available to optimize based on.\n\nWhy metrics is different from loss?\n\n\n\n\n\n\nImportant\n\n\n\n(Jeremy) If you were taking something like accuracy as loss function for classifying cats vs dogs. Then it may go on finding gradient and optimizing, yet at some point if inacurate cats are still labelled as dogs by models. Evaluating with accuracy as loss functions will be same. We canâ€™t proceed further with such a loss function, thatâ€™s why we use functions like MSE for loss usually.\n\n\nMetrics in real world is ofcourse a big issue, so with one ML model which claims to have got good results in a particular metrics when implemented has caused issues in real world which affect lot of people.\nSo check the article written by Rachael Thomas on The problem with metrics is a big problem for AI."
  },
  {
    "objectID": "posts/2022/2022-05-19-fastai-54.html#pearson-coefficient",
    "href": "posts/2022/2022-05-19-fastai-54.html#pearson-coefficient",
    "title": "Practical Deep Learning for Coders Course - Lesson 4",
    "section": "Pearson Coefficient",
    "text": "Pearson Coefficient\nUnderstanding metrics is very key, especially in Kaggle competitons. According to this Kaggle competition page: â€œSubmissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.â€\n\n\n\n\n\n\nNote\n\n\n\nâ€œThis coefficient is usually abbreviated using the single letter r. It is the most widely used measure of the degree of relationship between two variables. r can vary between -1, which means perfect inverse correlation, and +1, which means perfect positive correlation.â€\n\n\n[source: Kaggle Notebook]((https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)\nJeremyâ€™s way of teaching this concept was explaining with code for us to get intuition\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\n\ndf = fetch_california_housing(as_frame=True)\ndf = df[\"data\"].join(df[\"target\"]).sample(1000, random_state=52)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      MedInc\n      HouseAge\n      AveRooms\n      AveBedrms\n      Population\n      AveOccup\n      Latitude\n      Longitude\n      MedHouseVal\n    \n  \n  \n    \n      7506\n      3.0550\n      37.0\n      5.152778\n      1.048611\n      729.0\n      5.062500\n      33.92\n      -118.28\n      1.054\n    \n    \n      4720\n      3.0862\n      35.0\n      4.697897\n      1.055449\n      1159.0\n      2.216061\n      34.05\n      -118.37\n      3.453\n    \n    \n      12888\n      2.5556\n      24.0\n      4.864905\n      1.129222\n      1631.0\n      2.395007\n      38.66\n      -121.35\n      1.057\n    \n    \n      13344\n      3.0057\n      32.0\n      4.212687\n      0.936567\n      1378.0\n      5.141791\n      34.05\n      -117.64\n      0.969\n    \n    \n      7173\n      1.9083\n      42.0\n      3.888554\n      1.039157\n      1535.0\n      4.623494\n      34.05\n      -118.19\n      1.192\n    \n  \n\n\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\n\n# to get correlation coefficent between every row of matrix with every other matrix\nnp.corrcoef(df, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\n\ndef relation_matrix(x, y):\n    return np.corrcoef(x, y)[0][1]\n\n\nnp.corrcoef(df.HouseAge, df.MedHouseVal)\n\narray([[1.  , 0.12],\n       [0.12, 1.  ]])\n\n\n\nrelation_matrix(df.HouseAge, df.MedHouseVal)\n\n0.1165853555067797\n\n\nWhen I ran through this code I was thinking about how [0,1] element in this corelation matrix is 0.12, when value of relation_matrix returns something as 0.11658535. I asked this simple doubt in the forum and after a while, I got answer from one of the course TAs Nick(n-e-w).\n\n\n\n\n\n\n\nNote\n\n\n\nThis is one of the best things IMO about taking the course live, rather than attending online. There is lot more activity, and you even get some of your questions answered by lot of experts and even Jeremy too.\n\n\n\ndef show_corr(df, a, b):\n    x, y = df[a], df[b]\n    plt.scatter(x, y, alpha=0.5, s=4)\n    plt.title(f\"{a} vs {b}; r: {relation_matrix(x, y):.2f}\")\n\n\nshow_corr(df, \"MedInc\", \"AveRooms\")\n\n\n\n\n\nshow_corr(df[df.AveRooms < 15], \"MedInc\", \"AveRooms\")\n\n\n\n\nIf you look at two graphs, once we removed the Average room <15. We notice a huge difference in r value which denotes the pearson coefficent are sensitive to outliers. Thus we got an intutive feeling of what the metrics is doing, and how itâ€™s being affected by outliers. Even if you make small error in some of predictions you will notice a hugh bump in leaderboard which affects your position, as pearson correlation penalizes heavily for wrong predictions.\nNext week, will be the fifth lesson and the last one for month of May. The course will resume again after a three weeks breaks during month of June when monsoon season delights us here in Kerala with rain and everyone else with more fastai."
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "",
    "text": "malayalam letters"
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html#why-i-am-starting-this-project",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html#why-i-am-starting-this-project",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "Why I am starting this project?",
    "text": "Why I am starting this project?\nI have been doing the fastai course since 2018. Yet I have been taking it seriously probably, only after I bought the book Deep Learning for Coders with FastAI & Pytorch almost one year back. This year I took the fastai v5 course, and I feel itâ€™s time to follow a piece of advice which I have heard multiple times.\n\nImportant: Jeremy Howard, who is teaching this course and wrote the book prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those whoâ€™ve found any success with the course emphasise repeatedly.)"
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html#problem-domain",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html#problem-domain",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "Problem Domain",
    "text": "Problem Domain\nAccording to huggingface tasks page:\n\nText Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.\n\nMalayalam is a highly inflectional and agglutinative language compared to other languages. The quantitative complexity of Malayalam classification was explained in this paper. The computer still doesnâ€™t seem to have understood the basic behaviour of the language to do text classification. Malayalam is a language which morphologically complex making it even more difficult.\nVery few people seem to have applied techniques in deep learning in Malayalam, and it seems to be a good place to see if really deep learning techniques can be applied in my mother tongue, Malayalam. A lot of progress in other languages has happened and in general NLP, yet itâ€™s a good time to see if it works in Indic languages like Malayalam."
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html#why-text-classification-is-interesting",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html#why-text-classification-is-interesting",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "Why Text classification is interesting?",
    "text": "Why Text classification is interesting?\nI believe working on tasks like Text classification is way more difficult when we are working in low-resource languages like Malayalam. Yet when working on problems like this, you realize what are things you take for granted in the English language.\nIn the English language, there are plenty of labelled datasets on any problem set you to want. A lot of articles and blogs have been written on how to apply various NLP techniques in English. When it comes to Malayalam, there is just a handful of people who have tried and applied this in Malayalam.\n\nNote to myself: Will is more important than Skill and itâ€™s important to be tenacious here.\n\nI believe this is here, itâ€™s very important to believe in oneâ€™s tenacity and try out new things in a field where very less research happening, and there are no proper open datasets for researchers to work on. This is why I feel this project can be challenging, and my approach is to see if the latest transformer approaches can do something or not."
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html#previous-work-vaaku2vec",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html#previous-work-vaaku2vec",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "Previous work: Vaaku2Vec",
    "text": "Previous work: Vaaku2Vec\nThe most important work in Malayalam text classification as far as I know is Vaaku2Vec project - State-of-the-Art Language Modeling and Text Classification in the Malayalam Language.\nAccording to their Github README:\n\nWe trained a Malayalam language model on the Wikipedia article dump from Oct, 2018. The Wikipedia dump had 55k+ articles. The difficulty in training a Malayalam language model is text tokenization since Malayalam is a highly inflectional and agglutinative language. In the current model, we are using an nltk tokenizer (will try better alternative in the future) and the vocab size is 30k. The language model was used to train a classifier which classifies a news into 5 categories (India, Kerala, Sports, Business, Entertainment). Our classifier came out with a whooping 92% accuracy in the classification task.\n\nIt was revolutionary at that time, to see deep learning techniques applied to get SOTA in Malayalam. IndicNLP as an organisation did a lot of work, from working on projects like Word2vec, Vaakk2vec etc. They worked on creating a Named entity recognition dataset for Malayalam etc. They conducted Devsprints in colleges like Model Engineering collegeâ€¦ and presented their work in Pycon India and Kochi Python. Most of the work was done by Adam Shamsudeen and Kamal K Raj."
  },
  {
    "objectID": "posts/2022/2022-05-30-malayalamtext-0.html#whats-the-plan-for-the-project",
    "href": "posts/2022/2022-05-30-malayalamtext-0.html#whats-the-plan-for-the-project",
    "title": "Starting an open-source project - Malayalam Text Classifier",
    "section": "Whatâ€™s the plan for the project?",
    "text": "Whatâ€™s the plan for the project?\n\nImportant: Cervantes once wrote that â€œthe journey is better than the innâ€, which means that the process is more valuable than the destination.\n\nAt moment, the project doesnâ€™t have any concrete goals and itâ€™s just me who is working in my free time.\nI have created a few issues and my next blog post will be on creating a baseline model on a private dataset that a few kind folks shared with me. I expect the dataset creation to be an iterative task. I am looking forward to blogging about what I work on and stumble upon in each stage of the project.\nWhen I was looking for where I wanted to create this as an open-source project obviously, I choose Swathanthra Malayalam Community because:\n\nI feel SMC as an organization played a pivotal part in revolutionizing Malayalam computing and has a strong community presence. They have made a lot of work by creating fonts, helping in internationalization efforts, â€¦\nPeople like Santhosh Thottingal and Kavya Manohar have helped me a lot in my previous failed attempt to build TTS with deep learning in Malayalam.\nSome of the open-source projects made by SMC still survive like the website of Malayalam Speech Corpus which is impressive to me.\n\nI would like to thank the following people for all the support and motivation they have given me in starting this open-source project on this occasion:\n\nAlex Strick van Linschoten\nSanthosh Thottingal and Kavya Manohar\nAshwin Jayaprakash"
  },
  {
    "objectID": "posts/2022/2022-06-04-baselinemodel.html",
    "href": "posts/2022/2022-06-04-baselinemodel.html",
    "title": "Building a baseline model for Malayalam Text Classification",
    "section": "",
    "text": "I have been working on open source project to work on Malayalam text classification approaches. If you want to follow along the previous blogs check the tag #malayalamtextmodels\nOne of the most under-rated advices which I actually learned from the Approaching almost any Machine learning Problem(AAAMLP) by Abhishek Thakur was to start always with simple models and to create a baseline first. This is something Jeremy also repeatedly emphasises:\n\nSo we should always be careful to benchmark simple models, as see if theyâ€™re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But thereâ€™s no need to guess â€“ itâ€™s so easy to try a few different models, thereâ€™s no reason not to give the simpler ones a go too!\n\nsource: 7-how-random-forests-really-work"
  },
  {
    "objectID": "posts/2022/2022-06-04-baselinemodel.html#doing-modeling-with-simple-approaches",
    "href": "posts/2022/2022-06-04-baselinemodel.html#doing-modeling-with-simple-approaches",
    "title": "Building a baseline model for Malayalam Text Classification",
    "section": "Doing modeling with simple approaches",
    "text": "Doing modeling with simple approaches\nInstead of directly trying out transformers models, I first thought of working with some simple models and see how well they perform. Since I had previously read AAAMLP and seen Abhisheks chapter on how trying out simple techniques in IMDB dataset have him impressive results. I also thought of trying the same approaches.\nWhat follows is my attempt to follow steps initially outlined in AAAMLP book.My code doesnâ€™t depart from the original code in book much.\nLetâ€™s start off by importing libraries:\n\nimport nltk\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nnltk.download(\"punkt\")\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/kurianbenoy/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\nFor training, I used a privately shared dataset with me which contained news articles with their associated labels. It contained almost 9000+ sentences labelled in 6 categories of news like Sports, Kerala, Business, Gulf, India, Entertainment.\n\ndf[\"labels\"].value_counts()\n\nKerala           3847\nEntertainment    1968\nSports           1061\nGulf             1034\nIndia             881\nBusiness          572\nName: labels, dtype: int64\n\n\n\nLogistic regression + CountVectorizer\nOne of the first models shared was about using Logistic regression and Count Vectorizer model in AAAMLP book. Letâ€™s see how well it performs in our dataset.\n\ncount_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n\n\n%%time\ncount_vec.fit(df.text)\n\nCPU times: user 15.3 s, sys: 196 ms, total: 15.5 s\nWall time: 15.5 s\n\n\nCountVectorizer(token_pattern=None,\n                tokenizer=<function word_tokenize at 0x7f940dc34d30>)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizerCountVectorizer(token_pattern=None,\n                tokenizer=<function word_tokenize at 0x7f940dc34d30>)\n\n\n\n%%time\nxtrain = count_vec.transform(df.text)\nmodel = linear_model.LogisticRegression()\nmodel.fit(xtrain, df.labels)\n\nCPU times: user 4min 27s, sys: 7.5 s, total: 4min 35s\nWall time: 1min 2s\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ndata = {\n    \"text\": [\n        \"à´¸àµà´•àµ‚à´¬à´¾à´¡àµˆà´µà´¿à´™àµà´™àµ, à´¸àµà´¨àµ‹àµ¼à´•àµà´•àµ‡à´²à´¿à´™àµà´™àµ, à´¸àµà´ªàµ€à´¡àµà´¬àµ‹à´Ÿàµà´Ÿà´¿à´™àµà´™àµ, à´¸àµ¼à´«à´¿à´™àµà´™àµ à´¤àµà´Ÿà´™àµà´™à´¿à´¯ à´•à´Ÿàµ½à´µà´¿à´¨àµ‹à´¦à´™àµà´™àµ¾à´•àµà´•àµ à´ªàµ‡à´°àµà´•àµ‡à´Ÿàµà´Ÿ à´¬à´¾à´²à´¿à´¯à´¿àµ½ à´ªàµ‹à´¯à´¿à´Ÿàµà´Ÿàµà´‚ à´‡à´¤àµŠà´¨àµà´¨àµà´‚ à´ªà´°àµ€à´•àµà´·à´¿à´šàµà´šà´¿à´²àµà´².à´§àµˆà´°àµà´¯à´‚ à´µà´°à´¾à´¤àµà´¤à´¤àµà´•àµŠà´£àµà´Ÿà´¾à´£àµ. à´‡à´ªàµà´ªàµ‹àµ¾ à´†à´²àµ‹à´šà´¿à´•àµà´•àµà´®àµà´ªàµ‹àµ¾ à´’à´°àµà´•àµˆ à´¨àµ‹à´•àµà´•à´¾à´®à´¾à´¯à´¿à´°àµà´¨àµà´¨àµ†à´¨àµà´¨àµàµ‹à´¨àµà´¨àµà´¨àµà´¨àµ. à´¸à´¾à´°à´®à´¿à´²àµà´², à´¬à´¾à´•àµà´•à´¿à´µàµ†à´šàµà´š à´†à´—àµà´°à´¹à´™àµà´™à´³à´¾à´£à´²àµà´²àµ‹ à´®àµà´¨àµà´¨àµ‹à´Ÿàµà´Ÿàµà´¨àµ€à´™àµà´™à´¾à´¨àµà´³àµà´³ à´ªàµà´°àµ‡à´°à´£. à´…à´µà´¸à´°à´™àµà´™àµ¾ à´‡à´¨à´¿à´¯àµà´®àµà´£àµà´Ÿà´¾à´•àµà´®àµ†à´¨àµà´¨àµ à´•à´°àµà´¤àµà´¨àµà´¨àµ.\"\n    ],\n    \"labels\": [\"sport\"],\n}\ndata\n\n{'text': ['à´¸àµà´•àµ‚à´¬à´¾à´¡àµˆà´µà´¿à´™àµà´™àµ, à´¸àµà´¨àµ‹àµ¼à´•àµà´•àµ‡à´²à´¿à´™àµà´™àµ, à´¸àµà´ªàµ€à´¡àµà´¬àµ‹à´Ÿàµà´Ÿà´¿à´™àµà´™àµ, à´¸àµ¼à´«à´¿à´™àµà´™àµ à´¤àµà´Ÿà´™àµà´™à´¿à´¯ à´•à´Ÿàµ½à´µà´¿à´¨àµ‹à´¦à´™àµà´™àµ¾à´•àµà´•àµ à´ªàµ‡à´°àµà´•àµ‡à´Ÿàµà´Ÿ à´¬à´¾à´²à´¿à´¯à´¿àµ½ à´ªàµ‹à´¯à´¿à´Ÿàµà´Ÿàµà´‚ à´‡à´¤àµŠà´¨àµà´¨àµà´‚ à´ªà´°àµ€à´•àµà´·à´¿à´šàµà´šà´¿à´²àµà´².à´§àµˆà´°àµà´¯à´‚ à´µà´°à´¾à´¤àµà´¤à´¤àµà´•àµŠà´£àµà´Ÿà´¾à´£àµ. à´‡à´ªàµà´ªàµ‹àµ¾ à´†à´²àµ‹à´šà´¿à´•àµà´•àµà´®àµà´ªàµ‹àµ¾ à´’à´°àµà´•àµˆ à´¨àµ‹à´•àµà´•à´¾à´®à´¾à´¯à´¿à´°àµà´¨àµà´¨àµ†à´¨àµà´¨àµàµ‹à´¨àµà´¨àµà´¨àµà´¨àµ. à´¸à´¾à´°à´®à´¿à´²àµà´², à´¬à´¾à´•àµà´•à´¿à´µàµ†à´šàµà´š à´†à´—àµà´°à´¹à´™àµà´™à´³à´¾à´£à´²àµà´²àµ‹ à´®àµà´¨àµà´¨àµ‹à´Ÿàµà´Ÿàµà´¨àµ€à´™àµà´™à´¾à´¨àµà´³àµà´³ à´ªàµà´°àµ‡à´°à´£. à´…à´µà´¸à´°à´™àµà´™àµ¾ à´‡à´¨à´¿à´¯àµà´®àµà´£àµà´Ÿà´¾à´•àµà´®àµ†à´¨àµà´¨àµ à´•à´°àµà´¤àµà´¨àµà´¨àµ.'],\n 'labels': ['sport']}\n\n\n\ntest_df = pd.DataFrame(data)\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      text\n      labels\n    \n  \n  \n    \n      0\n      à´¸àµà´•àµ‚à´¬à´¾à´¡àµˆà´µà´¿à´™àµà´™àµ, à´¸àµà´¨àµ‹àµ¼à´•àµà´•àµ‡à´²à´¿à´™àµà´™àµ, à´¸àµà´ªàµ€à´¡àµà´¬àµ‹à´Ÿàµà´Ÿà´¿à´™...\n      sport\n    \n  \n\n\n\n\n\ntest_df = df.sample(frac=0.1, random_state=1)\ntest_df.shape\n\n(936, 2)\n\n\n\nxtest = count_vec.transform(test_df.text)\n\n\npreds = model.predict(xtest)\naccuracy = metrics.accuracy_score(test_df.labels, preds)\naccuracy\n\n0.9989316239316239\n\n\nYou may be wondering, why I havenâ€™t deleted all these code piece even after experimenting. This is something, which recently picked up following fastai v5 course. Instead of always deleting after experimenting, keep your experiments also public. But after experimentation is done, wrap it up as function like the one below so you can reuse it in future experiments as well.\n\ndef vectorize_evaluate_loop(train_df, test_df):\n    count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n    count_vec.fit(train_df.text)\n    dependent_train = count_vec.transform(train_df.text)\n    model = linear_model.LogisticRegression()\n    model.fit(dependent_train, train_df.labels)\n    dependent_test = count_vec.transform(test_df.text)\n    predictions = model.predict(dependent_test)\n    return metrics.accuracy_score(test_df.labels, predictions)\n\n\n%%time\nvectorize_evaluate_loop(df, test_df)\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nCPU times: user 4min 37s, sys: 6.69 s, total: 4min 43s\nWall time: 1min 17s\n\n\n0.9989316239316239\n\n\nThe function to create k-folds for calculating validation accuracy across K folds of data. Itâ€™s very important to create good validation sets.\n\ndf[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\n\n\ndf.shape\n\n(9363, 3)\n\n\n\nY_value = df.labels.values\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor fold, (text_, value_) in enumerate(kf.split(X=df, y=Y_value)):\n    df.loc[value_, \"kfold\"] = fold\n\n\nfor fold_ in range(5):\n    train_df = df[df.kfold != fold_].reset_index(drop=True)\n    test_df = df[df.kfold == fold_].reset_index(drop=True)\n    print(f\"Fold value: {fold_}\")\n    print(f\"Accuracy: {vectorize_evaluate_loop(train_df, test_df)}\")\n\nFold value: 0\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8873465029364656\nFold value: 1\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8777362520021356\nFold value: 2\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8916177255739456\nFold value: 3\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.875\nFold value: 4\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8669871794871795\n\n\n\n\nNaive Bayes Classifier\nA bit faster to complete training, yet only difference is itâ€™s having less accuracy compared to previous approach.\n\ndef naive_bayes_evaluate_loop(train_df, test_df):\n    count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n    count_vec.fit(train_df.text)\n    dependent_train = count_vec.transform(train_df.text)\n    # changing different model name for function\n    model = naive_bayes.MultinomialNB()\n    model.fit(dependent_train, train_df.labels)\n    dependent_test = count_vec.transform(test_df.text)\n    predictions = model.predict(dependent_test)\n    return metrics.accuracy_score(test_df.labels, predictions)\n\n\nfor fold_ in range(5):\n    train_df = df[df.kfold != fold_].reset_index(drop=True)\n    test_df = df[df.kfold == fold_].reset_index(drop=True)\n    print(f\"Fold value: {fold_}\")\n    print(f\"Accuracy: {naive_bayes_evaluate_loop(train_df, test_df)}\")\n\nFold value: 0\nAccuracy: 0.8441003737319808\nFold value: 1\nAccuracy: 0.8227442605445809\nFold value: 2\nAccuracy: 0.8296849973304858\nFold value: 3\nAccuracy: 0.8269230769230769\nFold value: 4\nAccuracy: 0.8183760683760684\n\n\n\n\nLogistic Regression + Tfidf Vectorizer\n\ndef tf_idf_evaluate_loop(train_df, test_df):\n    # note we are using TfidfVectorizer instead of CountVectorizer\n    count_vec = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n    count_vec.fit(train_df.text)\n    dependent_train = count_vec.transform(train_df.text)\n    model = linear_model.LogisticRegression()\n    model.fit(dependent_train, train_df.labels)\n    dependent_test = count_vec.transform(test_df.text)\n    predictions = model.predict(dependent_test)\n    return metrics.accuracy_score(test_df.labels, predictions)\n\n\nfor fold_ in range(5):\n    train_df = df[df.kfold != fold_].reset_index(drop=True)\n    test_df = df[df.kfold == fold_].reset_index(drop=True)\n    print(f\"Fold value: {fold_}\")\n    print(f\"Accuracy: {tf_idf_evaluate_loop(train_df, test_df)}\")\n\nFold value: 0\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8478376935397758\nFold value: 1\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8344901227976508\nFold value: 2\nAccuracy: 0.8489054991991457\nFold value: 3\nAccuracy: 0.8290598290598291\nFold value: 4\n\n\n/home/kurianbenoy/mambaforge/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nAccuracy: 0.8253205128205128"
  },
  {
    "objectID": "posts/2022/2022-06-04-baselinemodel.html#evaluating-and-looking-results",
    "href": "posts/2022/2022-06-04-baselinemodel.html#evaluating-and-looking-results",
    "title": "Building a baseline model for Malayalam Text Classification",
    "section": "Evaluating and looking results",
    "text": "Evaluating and looking results\nI am genuinely surprised by the following:\n\nA simple linear regression on this text classification tasks getâ€™s close to 87-89% accuracy when evaluated using K-fold validation approach. We havenâ€™t done any complex fine tuning or even label_encoding at the moment. Based on improving with some more tweaks, I am trying a few things here.\nThe state of art model for Text classification in Malayalam claims to have got 92% accuracy based on training only on validation dataset, like we have done in sklearn it seems.\n\n\n\n\nimage\n\n\nSource\nWhat is the difference between Vaaku2Vec and this simple model if itâ€™s just less than 3% more accurate?\n\nYet there is a huge difference between ULMFiT approach which Vaaku2Vec and our baseline model. The Vaaku2Vec model has been trained on base model of Malayalam Wikipedia text, so more text corpus will be present and the model has learned from it. In case of our simple baseline model, it has learned just from 9000+ data points only. Now in case of a new word which is not in this dataset, there is a probability it maybe found in Vaaku2Vec model. Yet itâ€™s not always true, because words like Covid will defenitely be not recognized by Vaaku2Vec model also."
  },
  {
    "objectID": "posts/2022/2022-06-04-baselinemodel.html#using-simple-models-for-labelling",
    "href": "posts/2022/2022-06-04-baselinemodel.html#using-simple-models-for-labelling",
    "title": "Building a baseline model for Malayalam Text Classification",
    "section": "Using simple models for labelling",
    "text": "Using simple models for labelling\nOne of the best things about using simple models to train is also that you can use it for data labelling efforts. To be honest, I am still learning more about data-annotation and labelling. My friend Alex is an expert though here and checkout his awesome blog How to get the most out of data annotation. I really loved this image showing 5 steps of datalabelling.\n\n\n\nimage\n\n\nSource and credits: Alex\nI see more work on data annotation coming soon as part of this project also. With that have a nice day."
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html",
    "href": "posts/2022/2022-05-03-fastai-52.html",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "",
    "text": "Jeremy was taking this session from his home, as the venue in University of queensland was already booked by someone else. Jeremy was really really pumped for this lesson and itâ€™s like going to the early days of fast.ai with lot of super exciting work happening.\n\ntwitter: https://twitter.com/bhutanisanyam1/status/1521511103406043137\n\nJeremy mentioned some technique on using Jupyter notebooks, and asked to take a look at jupyter extensions. The navigation section and how to collapse headings was explained during class. [24:00]"
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html#fastbook-chapter-2",
    "href": "posts/2022/2022-05-03-fastai-52.html#fastbook-chapter-2",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "Fastbook Chapter 2",
    "text": "Fastbook Chapter 2\nThis week we started by taking a look at putting model in production using fastai. This was the same thing which is covered in chapter 2 of Deep Learning book To build grizzly bears and teddy bears classifier.\nFew things have changed in book in this version:\n\nusing search_images_ddg instead of bing search apis\nusing huggingfaces spaces as deployment instead of voila even though itâ€™s still works\n\n\nRandomResizedCrop could be a good idea to understand different varieties of same image.\n\nDoes RandomResizedCrop crop duplicate the image â€“ i.e.Â you get multiple copies and you ensure that all the parts of the image are used in training? or does it just make one crop?\nJeremy answered it in video at [32:30]. His answer was it doesnâ€™t copying image. In each epoch every image getâ€™s written and what happens is in-memory image is being wrapped by recropping and colouring in realtime during model training. Itâ€™s like infinitely multi-copies of images.\nCheck the book to learn more in detail about various augmentations.\nSanyam mentioned that RandomResized crop as a augmentation is very helpful:\n\n\n\n\n\n\nImportant\n\n\n\nActually this technique is SUPER helpful-in a recent interview, Chris Deotte (4x Grandmaster) shared how these resizing techniques helped them win a solo gold. This was in the petfinder Kaggle competition (2nd run of the comp)\n\n\n\n\n\n\n\n\nNote\n\n\n\nJeremy is running on a laptop with 4GB GPU. Jeremy says in GPU, just run one thing at a time else you will get CUDA error."
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html#how-to-do-fast.ai-course",
    "href": "posts/2022/2022-05-03-fastai-52.html#how-to-do-fast.ai-course",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "How to do fast.ai course",
    "text": "How to do fast.ai course\nTips for people in Yellow bucket:\n\n\n\n\n\n\nNote\n\n\n\nIf you are in yellow, always stop try. First go ahead and watch video fully without touching your keyboard and write code. Then watch again and follow the course. This is an unusual way as it canâ€™t be done in real college lectures, but itâ€™s very effective way indeed.\n\n\nI asked Wayde Gilliam who is a long term fastai community member after the lesson about his process of watching lectures. He was gracious enough to share it with mith\n\n\n\n\n\n\nImportant\n\n\n\n\nWatch the livestream and jot down timestamp to go back to for anything I found interesting in journal A (or just a piece of paper)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nGo back through the video after 2-3 days, hit those spots I noted during the livestream. Will write detailed notes in another Journal (weâ€™ll call that journal B)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThereâ€™s too much info to digest in real-time so this approach works well and its what Iâ€™ve been doing for 4-5 yrs."
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html#huggingface-spaces",
    "href": "posts/2022/2022-05-03-fastai-52.html#huggingface-spaces",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "Huggingface spaces",
    "text": "Huggingface spaces\nJeremy pointed to tanishq tutorial on Gradio + HuggingFace Spaces.\n\n\n\nimage\n\n\nAlso Jeremy mentioned some good tools which are useful:\n\nGithub Desktop: Hamel who was a employee in github previously, is even using github desktop. Some complicated stuff in git can be solved using this tool. Even knowing terminal is cool.\nWSL: As a datascientist, you spend a lot of time in terminals. Just use ubuntu with windows terminal. Any time Jeremy shows in terminal, he just uses windows terminal.\nIn terminal, he uses Tmux as a terminal emulator as pointed out in fast.ai forums for my question.\n\nJeremy like Windows due to easiness in streaming, good apps and recording capabilities. Yet Jeremy also has a linux environment with a good Deep learning jig.\n\n\n\n\n\n\nNote\n\n\n\nJupyter notebooks debugging with magic methods %time, %debug\n\n\nIn fastai for inference, it returns back a tensor. One of issue in gradio tensors is not supported at moment. So we need to convert tensors to float and do prediction.\nJeremy created a cats vs dogs classifier using spaces. His daughter when realised he is building such a classifier googled something which is a mix of cat and dog. For that his initial prediction was like 50-50% for both cats and dogs.\nThis kind of shows how important the support system around you and how much they acknowledge the work you do. This personally touched me. As my sister was encouraging me to go an all-nighter to complete the Music genre classification spaces.\nTODO: Look through Jeremy setup and how he worked with gradio in local [58:00 onwards 1:14:00]"
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html#fastsetup",
    "href": "posts/2022/2022-05-03-fastai-52.html#fastsetup",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "fastsetup",
    "text": "fastsetup\nInstalling python and jupyter-notebooks with proper git and conda setup.\nFastai setup\n\n\n\n\n\n\nImportant\n\n\n\nA big issue in laptops with linux or mac there is a python default version, donâ€™t use that python. As that python version is for your operating system to do itâ€™s stuff. Donâ€™t mess on top of it.\n\n\nUse mamba based installation for fastai now:\nmamba install fastai\nmamba install -c fastchan jupyter nbdev"
  },
  {
    "objectID": "posts/2022/2022-05-03-fastai-52.html#trying-gradio-api-with-github-pages",
    "href": "posts/2022/2022-05-03-fastai-52.html#trying-gradio-api-with-github-pages",
    "title": "Practical Deep Learning for Coders Course - Lesson 2",
    "section": "Trying gradio API with github Pages",
    "text": "Trying gradio API with github Pages\nAn example API in gradio Example Jeremy showcased\n\nWith live demo, we could have easily used it with any websites. Without any software just with the browser, you can run this file. Thatâ€™s the cool thing about javascript and can host in website called github pages\n\n\n\nCode\nfetch('https://hf.space/embed/kurianbenoy/audioclassification/+/api/predict/', \n{ method: \"POST\",\n body: JSON.stringify({\"data\":[ {\"data\": null, \"is_example\": true, \"name\": \"000003.ogg\"}\n]}), headers: { \"Content-Type\": \"application/json\" }})\n.then(function(response){ return response.json(); })\n.then(function(json_response){ console.log(json_response) })\n\n\nHe used alembic theme. With a particular configuration. At top of any github pages, you should add three dashes. The world of javascript apps, he build this cool apps.\n\n\n\n\n\n\nImportant\n\n\n\nThe magic of using gradio APIs can be summarized as the following. It exposes a reliable way of sharing microservices. With this if you are just creating any hugging face spaces, with that APIs. You can use it any websites, apps etc. It looks to me there is no limitation with using Gradio API at the moment."
  },
  {
    "objectID": "posts/2022/2022-02-27-questionansweringwithhf.html",
    "href": "posts/2022/2022-02-27-questionansweringwithhf.html",
    "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
    "section": "",
    "text": "from datasets import load_dataset\n\nraw_datasets = load_dataset(\"squad\")\n\n\n\n\n\n\n\nDownloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data."
  },
  {
    "objectID": "posts/2022/2022-02-27-questionansweringwithhf.html#recently-a-tweet-went-viral",
    "href": "posts/2022/2022-02-27-questionansweringwithhf.html#recently-a-tweet-went-viral",
    "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
    "section": "Recently a tweet went viral",
    "text": "Recently a tweet went viral\n\n\nWe can see that NLP is hard, and even the best publicly avaiable models performs poorly in this tasks\n\n\n\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\nclassifier(\n    \"Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo\"\n)\n\n\n\n\n\n\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n\n\n\n\n\n\n\n[{'label': 'LABEL_0', 'score': 0.5706434845924377}]\n\n\n\ncheckpoint = \"cardiffnlp/twitter-roberta-base-sentiment\"\n\nclassifier = pipeline(\"sentiment-analysis\", model=checkpoint)\n\nclassifier(\n    \"Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'label': 'LABEL_2', 'score': 0.9608174562454224}]\n\n\n\ncheckpoint = \"finiteautomata/bertweet-base-sentiment-analysis\"\n\nclassifier = pipeline(\"sentiment-analysis\", model=checkpoint)\n\nclassifier(\n    \"Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nemoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n\n\n[{'label': 'NEG', 'score': 0.9447618126869202}]\n\n\n\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n\nNo model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'answer': 'Hugging Face', 'end': 45, 'score': 0.6949771046638489, 'start': 33}\n\n\n\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"What was feedback?\",\n    context=\"Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo\",\n)\n\nNo model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n\n\n{'answer': 'Brilliant service',\n 'end': 101,\n 'score': 0.13661321997642517,\n 'start': 84}"
  },
  {
    "objectID": "posts/2022/2022-02-27-questionansweringwithhf.html#for-specific-question-answering---use-question-answering-models",
    "href": "posts/2022/2022-02-27-questionansweringwithhf.html#for-specific-question-answering---use-question-answering-models",
    "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
    "section": "For specific question answering - use question answering models",
    "text": "For specific question answering - use question answering models\nEncoder-only models like BERT tend to be great at extracting answers to factoid questions like â€œWho invented the Transformer architecture?â€ but fare poorly when given open-ended questions like â€œWhy is the sky blue?â€ In these more challenging cases\nIf youâ€™re interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset. Using Lonformer model.\n[1] https://yjernite.github.io/lfqa.html\n\nprint(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\nprint(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\nprint(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])\n\nContext:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\nQuestion:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nAnswer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n\n\n\nraw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)\n\n\n\n\nDataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 0\n})\n\n\n\nprint(raw_datasets[\"validation\"][0][\"answers\"])\nprint(raw_datasets[\"validation\"][2][\"answers\"])\n\n{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}\n\n\n\nprint(raw_datasets[\"validation\"][2][\"context\"])\nprint(raw_datasets[\"validation\"][2][\"question\"])\n\nSuper Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\nWhere did Super Bowl 50 take place?\n\n\n\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontext = raw_datasets[\"train\"][0][\"context\"]\nquestion = raw_datasets[\"train\"][0][\"question\"]\n\ninputs = tokenizer(question, context)\ntokenizer.decode(inputs[\"input_ids\"])\n\n'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'\n\n\n\ninputs = tokenizer(\n    question,\n    context,\n    max_length=100,\n    truncation=\"only_second\",\n    stride=50,\n    return_overflowing_tokens=True,\n)\n\nfor ids in inputs[\"input_ids\"]:\n    print(tokenizer.decode(ids))\n\n[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n\n\n\ninputs = tokenizer(\n    question,\n    context,\n    max_length=100,\n    truncation=\"only_second\",\n    stride=50,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n)\ninputs.keys()\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n\n\n\ninputs[\"overflow_to_sample_mapping\"]\n\n[0, 0, 0, 0]\n\n\n\ninputs = tokenizer(\n    raw_datasets[\"train\"][2:6][\"question\"],\n    raw_datasets[\"train\"][2:6][\"context\"],\n    max_length=100,\n    truncation=\"only_second\",\n    stride=50,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n)\n\nprint(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\nprint(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n\nThe 4 examples gave 19 features.\nHere is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n\n\nOnce we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If thatâ€™s not the case, we loop to find the first and last token of the answer:\n\nanswers = raw_datasets[\"train\"][2:6][\"answers\"]\nstart_positions = []\nend_positions = []\n\nfor i, offset in enumerate(inputs[\"offset_mapping\"]):\n    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n    answer = answers[sample_idx]\n    start_char = answer[\"answer_start\"][0]\n    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n    sequence_ids = inputs.sequence_ids(i)\n\n    # Find the start and end of the context\n    idx = 0\n    while sequence_ids[idx] != 1:\n        idx += 1\n    context_start = idx\n    while sequence_ids[idx] == 1:\n        idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label is (0, 0)\n    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nstart_positions, end_positions\n\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])\n\n\n\nidx = 0\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n\nTheoretical answer: the Main Building, labels give: the Main Building\n\n\n\nidx = 4\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\ndecoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n\nTheoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]\n\n\n\nmax_length = 384\nstride = 128\n\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n\ntrain_dataset = raw_datasets[\"train\"].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)\nlen(raw_datasets[\"train\"]), len(train_dataset)\n\n\n\n\n(87599, 88729)\n\n\nâ€˜Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend â€ Venite Ad Me Omnes â€œ. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]â€™\nIndeed, we donâ€™t see the answer inside the context.\nâœï¸ Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied.\nNow that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. Weâ€™ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here:\n\ndef preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs\n\n\nvalidation_dataset = raw_datasets[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"validation\"].column_names,\n)\nlen(raw_datasets[\"validation\"]), len(validation_dataset)\n\n\n\n\n(10570, 10822)\n\n\n\nsmall_eval_set = raw_datasets[\"validation\"].select(range(100))\ntrained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = small_eval_set.map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"validation\"].column_names,\n)\n\n\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\nimport torch\nfrom transformers import AutoModelForQuestionAnswering\n\neval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\neval_set_for_model.set_format(\"torch\")\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nbatch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n    device\n)\n\nwith torch.no_grad():\n    outputs = trained_model(**batch)\n\n\nstart_logits = outputs.start_logits.cpu().numpy()\nend_logits = outputs.end_logits.cpu().numpy()\n\n\nimport collections\n\nexample_to_features = collections.defaultdict(list)\nfor idx, feature in enumerate(eval_set):\n    example_to_features[feature[\"example_id\"]].append(idx)\n\n\nimport numpy as np\n\nn_best = 20\nmax_answer_length = 30\npredicted_answers = []\n\nfor example in small_eval_set:\n    example_id = example[\"id\"]\n    context = example[\"context\"]\n    answers = []\n\n    for feature_index in example_to_features[example_id]:\n        start_logit = start_logits[feature_index]\n        end_logit = end_logits[feature_index]\n        offsets = eval_set[\"offset_mapping\"][feature_index]\n\n        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                # Skip answers that are not fully in the context\n                if offsets[start_index] is None or offsets[end_index] is None:\n                    continue\n                # Skip answers with a length that is either < 0 or > max_answer_length.\n                if (\n                    end_index < start_index\n                    or end_index - start_index + 1 > max_answer_length\n                ):\n                    continue\n\n                answers.append(\n                    {\n                        \"text\": context[\n                            offsets[start_index][0] : offsets[end_index][1]\n                        ],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                )\n\n    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n\n\nfrom datasets import load_metric\n\nmetric = load_metric(\"squad\")\n\n\n\n\n\n\n\n\nmetric\n\nMetric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\nComputes SQuAD scores (F1 and EM).\nArgs:\n    predictions: List of question-answers dictionaries with the following key-values:\n        - 'id': id of the question-answer pair as given in the references (see below)\n        - 'prediction_text': the text of the answer\n    references: List of question-answers dictionaries with the following key-values:\n        - 'id': id of the question-answer pair (see above),\n        - 'answers': a Dict in the SQuAD dataset format\n            {\n                'text': list of possible texts for the answer, as a list of strings\n                'answer_start': list of start positions for the answer, as a list of ints\n            }\n            Note that answer_start values are not taken into account to compute the metric.\nReturns:\n    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n    'f1': The F-score of predicted tokens versus the gold answer\nExamples:\n\n    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n    >>> squad_metric = datasets.load_metric(\"squad\")\n    >>> results = squad_metric.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'exact_match': 100.0, 'f1': 100.0}\n\"\"\", stored examples: 0)\n\n\n\ntheoretical_answers = [\n    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n]\n\nprint(predicted_answers[0])\nprint(theoretical_answers[0])\n\n{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n\n\n\nmetric.compute(predictions=predicted_answers, references=theoretical_answers)\n\n{'exact_match': 83.0, 'f1': 88.25000000000004}\n\n\n\nfrom tqdm.auto import tqdm\n\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[\n                            offsets[start_index][0] : offsets[end_index][1]\n                        ],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [\n        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n    ]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n\n\ncompute_metrics(start_logits, end_logits, eval_set, small_eval_set)\n\n\n\n\n{'exact_match': 83.0, 'f1': 88.25000000000004}\n\n\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\n\n\n\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\nAuthenticated through git-credential store but this isn't the helper defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n\ngit config --global credential.helper store\n\n\n\n! sudo apt install git-lfs\n! git-lfs install\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.3.4-1).\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-470\nUse 'sudo apt autoremove' to remove it.\n0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\nError: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\nGit LFS initialized.\n\n\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"bert-finetuned-squad\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    fp16=True,\n    push_to_hub=True,\n)\n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n\n\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()\n\n/content/bert-finetuned-squad is already a clone of https://huggingface.co/kurianbenoy/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`.\nUsing amp half precision backend\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 88729\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 11092\n\n\n\n\n    \n      \n      \n      [ 4154/11092 2:29:47 < 4:10:18, 0.46 it/s, Epoch 0.37/1]\n    \n    \n  \n \n      Step\n      Training Loss\n    \n  \n  \n    \n      500\n      1.691300\n    \n    \n      1000\n      1.575600\n    \n    \n      1500\n      1.441200\n    \n    \n      2000\n      1.356100\n    \n    \n      2500\n      1.297700\n    \n    \n      3000\n      1.245200\n    \n    \n      3500\n      1.242400\n    \n    \n      4000\n      1.181400\n    \n  \n\n\n\n\npredictions, _ = trainer.predict(validation_dataset)\nstart_logits, end_logits = predictions\ncompute_metrics(\n    start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n)\n\n\ntrainer.push_to_hub(commit_message=\"Training complete\")"
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html",
    "href": "posts/2022/2022-07-28-fastai55.html",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "",
    "text": "Most of the time as a practitioner, your job is to connect a set of inputs to the sets of outputs you want with machine learning algorithm together in a framework. According to Jeremy what is important is how you tweak on first layer and last layer of neural network. The middle layer is usually not that important."
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html#remind-yourself-these-concepts",
    "href": "posts/2022/2022-07-28-fastai55.html#remind-yourself-these-concepts",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "Remind yourself these concepts",
    "text": "Remind yourself these concepts\nBefore getting started with this lesson, letâ€™s remind ourselves What is matrix & vector?\n\n\n\nimage\n\n\nIn this chaper, you may stumble into terms like matrix-vector multiplication, matrix matrix products etc. So itâ€™s a good idea to remind yourself with the concept of matrix multiplication and broadcasting.\nIn this chapter three notebooks where covered, so itâ€™s bit more hectic compared to the previous chapters to be honest. The notebooks covered were:\n\nLinear Models and Neural net from Scratch\nWhy you should use a framework?\nHow random forests really work\n\nFor the course there was close to one month gap between fifth and sixth lesson, because of exams in University of Queenzland."
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html#linear-model-neural-network-from-scratch-notebook",
    "href": "posts/2022/2022-07-28-fastai55.html#linear-model-neural-network-from-scratch-notebook",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "Linear model & neural network from scratch notebook",
    "text": "Linear model & neural network from scratch notebook\nIn this notebook first few sections covers on data cleaning and feature engineering with pandas. A few notes which I jotted down, when I started looking into the lesson at first.\n\nIn pandas never delete columns\nYou can replace missing values using mode of column\nWe can have multiple modes, so choose the first element as 0\nIn first baseline model, donâ€™t do complicated things at the start.\nfor categorical variables we can set dummy variables for Pclass with pd.get_dummies\n\nThen the notebook progresses first into building:\n\nLinear models\nNeural networks\nDeep Learning\n\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is a pre-requisite for lesson 7 when we are covering collabrative filtering also."
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html#why-you-should-use-a-framework",
    "href": "posts/2022/2022-07-28-fastai55.html#why-you-should-use-a-framework",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "Why you should use a framework?",
    "text": "Why you should use a framework?\nThis notebook, does some interesting feature engineering followed by building models with fastai framework. It also shows how to use ensembling with fastai library and to get in the top 25% of accuracy.\nI have seen this cliche argument that for learning ML, you need to go into details and using frameworks is a step down. Jeremy emphasises always use good frameworks on top of it. Rather than re-inventing from scratch. Lot of the success of fast.ai comes from it not asking practitioners to go into details. One of the reasons I like frameworks like blurr, Icevision is also because of that and itâ€™s helping users who are familiar with fastai to easily build complex computer vision and NLP models.\nDuring a conversation with Icevision core-developer, Dickson Neoh:\n\nIn icevision, within 10 minutes I can train an object detection model with any dataset. It may not be most accurate, yet I can iterate so quickly."
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html#how-random-forests-really-work",
    "href": "posts/2022/2022-07-28-fastai55.html#how-random-forests-really-work",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "How random forests really work?",
    "text": "How random forests really work?\nJeremy was know as the random forest guy before he became know as the Deep learning person. One of the cool things about random forest is itâ€™s very hard to get something wrong unlike logistic regression.\nRandom forests are really intereptables, and helps in getting good accuracy. He also covered about gradient boosted trees during this lesson."
  },
  {
    "objectID": "posts/2022/2022-07-28-fastai55.html#homework-dataset",
    "href": "posts/2022/2022-07-28-fastai55.html#homework-dataset",
    "title": "Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)",
    "section": "Homework dataset",
    "text": "Homework dataset\nTo practise these techniques, I feel a good place to start is by participating in Kaggle Tabular Playground dataset competition or previous tabular competitions in Kaggle."
  },
  {
    "objectID": "posts/2022/2022-04-26-fastai-51.html",
    "href": "posts/2022/2022-04-26-fastai-51.html",
    "title": "Practical Deep Learning for Coders Course - Lesson 1",
    "section": "",
    "text": "First there was a set of introductions by university officials at UQ like VC. One curious thing was everyone of UQ staff were honouring something traditionaly of that land to live in reconciliation.\nThen lecture of Jeremy starts, seeing his face the chatbox is in delight.\n\n\n\nimage\n\n\nJeremy mentions there are two categories of students who attend the course:\n\nStudents who have enrolled via University of Queensland(with almost 350 people attending in-person and about 100 people remotely as well).\nfastai fellows who have acknowledged for their contribution to community.\n\nJeremy recommends having study buddies when we are learning the course is important. So he asks to create Study groups wherever possible. This course is now happening after a gap of 2 years, so there is a lot of new things which has to be covered as Deep learning moves so fast.\nUsing Dalle-2 technique we can generative creative images from generate twitter bios. For a creative person, this can be very helpful to create good artwork. Then one of another popular techniques was using Pathways language model which is able to answers question with explanations and even explains why some jokes are funny.\nJeremy talks about his interest in education.He is a homeschooler and learned from books by Paul Lockhart & David Perkins which were inspiration for fast.ai. fastai teaches stuff in top-down manner. You will go into as much technical stuff as you, yet you will learn and implement cool stuff steadily.\nAbout fast.ai course\nHe wrote an awesome book and this course. His book is one of the best sellers in Deep Learning and used to teach folks in companies like Tesla, OpenAI etc. Almost 6 million people watched his videos so far.Jeremy has won multiple competitions in Kaggle, was the CEO of kaggle. He build Enlitic, a medical company which was build for medical purpose with two other succesful startups.\n\nJeremy mentioned for this course, we are not using any material directly from Deep Learning For Coders with Fastai & Pytorch book. Yet he recommends to read portions of book after each chapter.\n\nUsually multiple people learn better if the same idea is exposed in different way from multiple sources. Thatâ€™s the why behind this approach.\nJeremy started coding hands-own a bird or park classifier, which was considered as a very difficult problem in 2015. Even a comic depicted this. Yet things have changed so drastically in past few years, that itâ€™s very easy to do that now.\n\nYet letâ€™s look, why we couldnâ€™t build a bird classifer in 2015:\n\n\nFor classifying histopothical images. They used computer vision techniques.\nThey got big team of datascientist, mathematicans with lot of features who build relevant feature for machine learning hand by hand.\nThese project took years\nAlso deep learning was not in radar for researchers then.\n\n\nWhat has now changed? - Using neural network they build these features on their own. - Mathew D Zeiler & Rob Fergus(and actual weights) showed with visualization how neural networks work - Combine all features to learn and go slowly in past, neural networks learned on itâ€™s own these techniques.\n\nIf itâ€™s a bird or not? notebook can be found here. I am slightly tweaking this model to leverage pytorch image-models released by timm.\n\nurls = search_images(\"bird photos\", max_images=1)\nurls[0]\n\n\nfrom fastdownload import download_url\n\ndest = \"bird.jpg\"\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\n\nim = Image.open(dest)\nim.to_thumb(256, 256)\n\nNote:\n\nImage based algorithms, are not for images. Image for music classification by Deolho, Ethan sutin sounds from image recognizer. You can do music classification, with some creativity using cnns.\nAlso needing lots of data is a myth created by companies who sell data processng units. There are lot of free resources like Kaggle, Colab etc.\n\n\nObservation by Jeremy: Tensorflow is slowly dying. Check this article which he cited. Yet pytorch has lot of hairy code, which can be solved using good abstractions in fastai.\n\n\nfastai library tries to provide good and the best fine-tuned models, which work well compared to other libraries. He showed code required for implementing AdamW in pytorch and in fastai.\n\nTanishq Abraham pointed me to implemtation of AdamW to chapter 16 in fastbook.\n\ndownload_url(\n    search_images(\"forest photos\", max_images=1)[0], \"forest.jpg\", show_progress=False\n)\nImage.open(\"forest.jpg\").to_thumb(256, 256)\n\nsearches = \"forest\", \"bird\"\npath = Path(\"bird_or_not\")\n\nfor o in searches:\n    dest = path / o\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f\"{o} photo\"))\n    resize_images(path / o, max_size=400, dest=path / o)\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\nAs the code showed, data cleaning is a big part of machine learninng. When we are learning this course as practitioners, we will spend lot of time of building and loading models. Like in compiler course lot of time is not spend on techniques, but on getting the environment up and ready.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(224, method=\"squish\")],\n).dataloaders(path)\n\ndls.show_batch()\n\nAfter examining, 100s of project and datascience requirments. fastai came up with this approach of DataBlock, which consists of five things:\n\nblocks\nget_items\nsplitter\nBatch_tfms(optional)\nget_y\nitem_tfms\n\nWithout validation data, it wonâ€™t allow to train. parent_label, return parent folder. we saved as forests or birds. We need same size. Idea to do quickly, why not publish vision_learners with pets dataset.\nNow itâ€™s time to train our model\n\nlearn = vision_learner(dls, \"vit_tiny_patch16_224\", metrics=error_rate)\n\nlearn.fine_tune(10)\n\nOne thing which is cool is that the whole presentation is also made with Jupyter Notebooks using RiseJS. Also jupyter notebooks can be used for writing books like Deep Learning for Coders, for blogging using fastpages, for CI/CD pipeline to run in parallel execution in fastai repo.\nTanishq Mathew Abraham has summarized on what can be done in this twitter threads.\n\ntwitter: https://twitter.com/iScienceLuvr/status/1519242326517829632\n\nAfter this Jeremy, showed all the examples in Chapter 1 in Deep Learning for coders. My notes then:\nWe are still scratching the surface. Lot of marketing out there, some of first open source models available. The deep learning when it broke X, y, z in domain. In NLP it breaks lot of stuff\nWhatâ€™s really go in on : in arthur samuel with graph. The graphs are build with gv2 in jupyter notebook. Deploying models in ML is a bit tricky. But itâ€™s just predict and shows results.\nConclusion by Jeremy\nSo after first lesson:\n\nIf you know python, then itâ€™s kind of easy for you.\nIf donâ€™t know python, itâ€™s very difficult\n\nRegardless of what level you are. Experiment yourself and do something more complex. Go ahead and push yourself a little bit, but not much. Then present your work. Do stuff on things where you are interested."
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html",
    "title": "First thoughts on Swift",
    "section": "",
    "text": "According to Chris Lattner, swift is promised as an infinetely hackable language. I recently binge watched two lessons of FastAI part2 which covered about new language for Deep Learning.\nLesson13\nLesson14\nJeremy Horwards and Sylvian rewrote the entire lessons taught in Part2(Foundations of Deep Learning) in Swift. This notebook can be found in the course repo.\nMy first impression after watching these lessons are that swift is an amazing language and in the near future it has a potential to capture the entire Machine Learning Landscape of Programming Languages like Python and R. It promises to go underneath the current barriers of exsisting languages, make Differentiable programming possible.\nI am quoting the exact wordings for Swift 4 tensorflow project on type of users its expecting currently:"
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html#importing-library-and-downloading-data",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html#importing-library-and-downloading-data",
    "title": "First thoughts on Swift",
    "section": "Importing library and downloading data",
    "text": "Importing library and downloading data\nIn above tutorial, going to do a Model training of Iris 10 dataset to identify from the given image which class of Iris flower is it part of. Most of code is from here.\nimport TensorFlow\n\nimport Python\n%include \"EnableIPythonDisplay.swift\"\nIPythonDisplay.shell.enable_matplotlib(\"inline\")\nlet plt = Python.import(\"matplotlib.pyplot\")\n\nimport Foundation\nimport FoundationNetworking\n\nfunc download(from sourceString: String, to destinationString: String){\n    let source = URL(string: sourceString)!\n    let destination = URL(fileURLWithPath: destinationString)\n    let data = try! Data.init(contentsOf: source)\n    try! data.write(to: destination)\n}\ndownload(from: \"https://raw.githubusercontent.com/tensorflow/swift/master/docs/site/tutorials/TutorialDatasetCSVAPI.swift\" , to: \"TutorialDatasetCSVAPI.swift\")\nlet trainDataFilename = \"iris_training.csv\"\ndownload(from: \"http://download.tensorflow.org/data/iris_training.csv\", to: trainDataFilename )\n// Checking the files in the runtime\nlet os = Python.import(\"os\")\nos.listdir()\n[â€˜.configâ€™, â€˜TutorialDatasetCSVAPI.swiftâ€™, â€˜iris_training.csvâ€™, â€˜sample_dataâ€™]"
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html#inspect-the-data",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html#inspect-the-data",
    "title": "First thoughts on Swift",
    "section": "Inspect the data",
    "text": "Inspect the data\nlet f = Python.open(trainDataFilename)\nfor _ in 0..<10{\n    print(Python.next(f).strip())\n}\nf.close()\n120,4,setosa,versicolor,virginica 6.4,2.8,5.6,2.2,2 5.0,2.3,3.3,1.0,1 4.9,2.5,4.5,1.7,2 4.9,3.1,1.5,0.1,0 5.7,3.8,1.7,0.3,0 4.4,3.2,1.3,0.2,0 5.4,3.4,1.5,0.4,0 6.9,3.1,5.1,2.3,2 6.7,3.1,4.4,1.4,1\nNone"
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html#image-classification-problem",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html#image-classification-problem",
    "title": "First thoughts on Swift",
    "section": "Image Classification problem",
    "text": "Image Classification problem\nImagine you are a botanist seeking an automated way to categorize each iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modestâ€”weâ€™re going to classify iris flowers based on the length and width measurements of their sepals and petals.\nThe Iris genus entails about 300 species, but our program will only classify the following three:\n\nIris setosa\nIris virginica\nIris versicolor\n\nlet featureNames = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlet labelName = \"species\"\nlet columnNames = featureNames + [labelName]\n\nprint(\"Features: \\(featureNames)\")\nprint(\"Label: \\(labelName)\")\nprint(\"Column names:\\(columnNames)\")\n\nFeatures: [â€œsepal_lengthâ€, â€œsepal_widthâ€, â€œpetal_lengthâ€, â€œpetal_widthâ€] Label: species Column names:[â€œsepal_lengthâ€, â€œsepal_widthâ€, â€œpetal_lengthâ€, â€œpetal_widthâ€, â€œspeciesâ€]\nlet classNames = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"]"
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html#create-a-dataset-api",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html#create-a-dataset-api",
    "title": "First thoughts on Swift",
    "section": "Create a Dataset API",
    "text": "Create a Dataset API\nSwift for TensorFlowâ€™s Dataset API is a high-level API for reading data and transforming it into a form used for training.\nEventually, the Dataset API will be able to load data from many file formats. The Dataset API is currently very incomplete, so we include some glue code from â€œTutorialDatasetCSVAPI.swiftâ€ that helps the Dataset API load data from the CSV file into the IrisBatch struct.\nlet batchSize = 32\n\n\nstruct IrisBatch {\n    let features: Tensor<Float>\n    let labels: Tensor<Int32>\n}\n\n%include \"TutorialDatasetCSVAPI.swift\"\n\nlet trainDataset: Dataset<IrisBatch> = Dataset(\n    contentsOfCSVFile: trainDataFilename, hasHeader: true,\n    featureColumns: [0, 1, 2, 3], labelColumns: [4]\n).batched(batchSize)\n\nlet firstTrainExamples = trainDataset.first!\nlet firstTrainFeatures = firstTrainExamples.features\nlet firstTrainLabels = firstTrainExamples.labels\nprint(\"First batch of features:\\n \\(firstTrainFeatures)\")\n\nFirst batch of features: [[6.4, 2.8, 5.6, 2.2], [5.0, 2.3, 3.3, 1.0], [4.9, 2.5, 4.5, 1.7], [4.9, 3.1, 1.5, 0.1], [5.7, 3.8, 1.7, 0.3], [4.4, 3.2, 1.3, 0.2], [5.4, 3.4, 1.5, 0.4], [6.9, 3.1, 5.1, 2.3], [6.7, 3.1, 4.4, 1.4], [5.1, 3.7, 1.5, 0.4], [5.2, 2.7, 3.9, 1.4], [6.9, 3.1, 4.9, 1.5], [5.8, 4.0, 1.2, 0.2], [5.4, 3.9, 1.7, 0.4], [7.7, 3.8, 6.7, 2.2], [6.3, 3.3, 4.7, 1.6], [6.8, 3.2, 5.9, 2.3], [7.6, 3.0, 6.6, 2.1], [6.4, 3.2, 5.3, 2.3], [5.7, 4.4, 1.5, 0.4], [6.7, 3.3, 5.7, 2.1], [6.4, 2.8, 5.6, 2.1], [5.4, 3.9, 1.3, 0.4], [6.1, 2.6, 5.6, 1.4], [7.2, 3.0, 5.8, 1.6], [5.2, 3.5, 1.5, 0.2], [5.8, 2.6, 4.0, 1.2], [5.9, 3.0, 5.1, 1.8], [5.4, 3.0, 4.5, 1.5], [6.7, 3.0, 5.0, 1.7], [6.3, 2.3, 4.4, 1.3], [5.1, 2.5, 3.0, 1.1]]\nprint(\"First batch of labels: \\(firstTrainLabels)\")\nFirst batch of labels: [2, 1, 2, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 1, 1]\nlet firstTrainFeaturesTransposed = firstTrainFeatures.transposed()\nlet petalLengths = firstTrainFeaturesTransposed[2].scalars\nlet sepalLengths = firstTrainFeaturesTransposed[0].scalars\n\nplt.scatter(petalLengths, sepalLengths, c: firstTrainLabels.array.scalars)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Sepal length\")\nplt.show()\n\n\nlet hiddenSize: Int = 10\nstruct IrisModel: Layer {\n    var layer1 = Dense<Float>(inputSize: 4, outputSize: hiddenSize, activation: relu)\n    var layer2 = Dense<Float>(inputSize: hiddenSize, outputSize: hiddenSize, activation: relu)\n    var layer3 = Dense<Float>(inputSize: hiddenSize, outputSize: 3)\n    \n    @differentiable\n    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n        return input.sequenced(through: layer1, layer2, layer3)\n    }\n}\n\nvar model = IrisModel()\n\n// Apply the model to a batch of features.\nlet firstTrainPredictions = model(firstTrainFeatures)\nfirstTrainPredictions[0..<5]\n\n[[ 0.21581106, -0.4621974, 0.25179374], [ 0.03792523, -0.6189664, 0.014299346], [ 0.13903831, -0.29850948, 0.1627174], [ -0.2161826, -0.80433285, -0.2289076], [ -0.2677291, -0.8302074, -0.26013878]]\nsoftmax(firstTrainPredictions[0..<5])\n\n[[0.39304087, 0.19951813, 0.40744105], [ 0.4007837, 0.20779046, 0.3914258], [0.37459084, 0.24184248, 0.38356668], [0.39328128, 0.21841016, 0.3883085], [ 0.3879857, 0.22107239, 0.39094183]]\nprint(\"Prediction: \\(firstTrainPredictions.argmax(squeezingAxis: 1))\")\nprint(\"    Labels: \\(firstTrainLabels)\")\n\nPrediction: [2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0] Labels: [2, 1, 2, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 1, 1]\nlet untrainedLogits = model(firstTrainFeatures)\nlet untrainedLoss = softmaxCrossEntropy(logits: untrainedLogits, labels: firstTrainLabels)\nprint(\"Loss test: \\(untrainedLoss)\")\n\nLoss test: 1.147685\nlet optimizer = SGD(for: model, learningRate: 0.01)\nlet (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n    let logits = model(firstTrainFeatures)\n    return softmaxCrossEntropy(logits: logits, labels: firstTrainLabels)\n}\nprint(\"Current loss: \\(loss)\")\n\nCurrent loss: 1.147685\noptimizer.update(&model, along: grads)\n\nlet logitsAfterOneStep = model(firstTrainFeatures)\nlet lossAfterOneStep = softmaxCrossEntropy(logits: logitsAfterOneStep, labels: firstTrainLabels)\nprint(\"Next loss: \\(lossAfterOneStep)\")\n\nNext loss: 1.1295123\n\nlet epochCount = 500\nvar trainAccuracyResults: [Float] = []\nvar trainLossResults: [Float] = []\n\n\nfunc accuracy(predictions: Tensor<Int32>, truths: Tensor<Int32>) -> Float {\n    return Tensor<Float>(predictions .== truths).mean().scalarized()\n}\n\nfor epoch in 1...epochCount {\n    var epochLoss: Float = 0\n    var epochAccuracy: Float = 0\n    var batchCount: Int = 0\n    for batch in trainDataset {\n        let (loss, grad) = model.valueWithGradient { (model: IrisModel) -> Tensor<Float> in\n            let logits = model(batch.features)\n            return softmaxCrossEntropy(logits: logits, labels: batch.labels)\n        }\n        optimizer.update(&model, along: grad)\n        \n        let logits = model(batch.features)\n        epochAccuracy += accuracy(predictions: logits.argmax(squeezingAxis: 1), truths: batch.labels)\n        epochLoss += loss.scalarized()\n        batchCount += 1\n    }\n    epochAccuracy /= Float(batchCount)\n    epochLoss /= Float(batchCount)\n    trainAccuracyResults.append(epochAccuracy)\n    trainLossResults.append(epochLoss)\n    if epoch % 50 == 0 {\n        print(\"Epoch \\(epoch): Loss: \\(epochLoss), Accuracy: \\(epochAccuracy)\")\n    }\n}\n\nEpoch 50: Loss: 0.6417311, Accuracy: 0.6640625 Epoch 100: Loss: 0.4391577, Accuracy: 0.9661458 Epoch 150: Loss: 0.33583364, Accuracy: 0.9661458 Epoch 200: Loss: 0.2520119, Accuracy: 0.9635417 Epoch 250: Loss: 0.19547871, Accuracy: 0.9635417 Epoch 300: Loss: 0.16002558, Accuracy: 0.9557292 Epoch 350: Loss: 0.13793558, Accuracy: 0.9661458 Epoch 400: Loss: 0.123241246, Accuracy: 0.9739583 Epoch 450: Loss: 0.1116381, Accuracy: 0.9739583 Epoch 500: Loss: 0.104046196, Accuracy: 0.9817708\nplt.figure(figsize: [12, 8])\n\nlet accuracyAxes = plt.subplot(2, 1, 1)\naccuracyAxes.set_ylabel(\"Accuracy\")\naccuracyAxes.plot(trainAccuracyResults)\n\nlet lossAxes = plt.subplot(2, 1, 2)\nlossAxes.set_ylabel(\"Loss\")\nlossAxes.set_xlabel(\"Epoch\")\nlossAxes.plot(trainLossResults)\n\nplt.show()\n\n\n\n\npng\n\n\nlet testDataFilename = \"iris_test.csv\"\ndownload(from: \"http://download.tensorflow.org/data/iris_test.csv\", to: testDataFilename)\n\nlet testDataset: Dataset<IrisBatch> = Dataset(\n    contentsOfCSVFile: testDataFilename, hasHeader: true,\n    featureColumns: [0, 1, 2, 3], labelColumns: [4]\n).batched(batchSize)\n\n// NOTE: With `batchSize = 32` and 30 examples in the test dataset, only one batch will run in the loop.\nfor testBatch in testDataset {\n    let logits = model(testBatch.features)\n    let predictions = logits.argmax(squeezingAxis: 1)\n    print(\"Test batch accuracy: \\(accuracy(predictions: predictions, truths: testBatch.labels))\")\n}\n\nTest batch accuracy: 0.96666664\nlet firstTestBatch = testDataset.first!\nlet firstTestBatchLogits = model(firstTestBatch.features)\nlet firstTestBatchPredictions = firstTestBatchLogits.argmax(squeezingAxis: 1)\n\nprint(firstTestBatchPredictions)\nprint(firstTestBatch.labels)\n\n[1, 2, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1]\n[1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1]\nlet unlabeledDataset: Tensor<Float> =\n    [[5.1, 3.3, 1.7, 0.5],\n     [5.9, 3.0, 4.2, 1.5],\n     [6.9, 3.1, 5.4, 2.1]]\n\nlet unlabeledDatasetPredictions = model(unlabeledDataset)\n\nfor i in 0..<unlabeledDatasetPredictions.shape[0] {\n    let logits = unlabeledDatasetPredictions[i]\n    let classIdx = logits.argmax().scalar!\n    print(\"Example \\(i) prediction: \\(classNames[Int(classIdx)]) (\\(softmax(logits)))\")\n}\n\nExample 0 prediction: Iris setosa ([ 0.99242, 0.0075798077, 1.0206181e-07])\nExample 1 prediction: Iris versicolor ([0.0015674275, 0.9790035, 0.019429056])\nExample 2 prediction: Iris virginica ([0.0024372158, 0.2561954, 0.74136734])\nSo this example showed how we can train a Classification of IRIS flowers. Training a neural network in Swift is a bit harder at the first insight than Python.\nBut as Chris Lattner said, the promise of Swift having a infinitely hackable language is just wonderful in my opinion."
  },
  {
    "objectID": "posts/2019/2019-12-19-swift4tensorflowintro.html#ending-notes-on-swift",
    "href": "posts/2019/2019-12-19-swift4tensorflowintro.html#ending-notes-on-swift",
    "title": "First thoughts on Swift",
    "section": "Ending Notes on Swift",
    "text": "Ending Notes on Swift\n\nThe swift syntax is a bit blocker to develop anything now for me. CamelCase is such a nice good thing\nYet Itâ€™s so rich. But but hard for understanding the syntax.\nThinks like Protocol, struct are not so intuitive after coming from Python background. Yet maybe people who come from Java background\nIf you get your hands wet with iOS development you will learn how much those features are useful."
  },
  {
    "objectID": "posts/2019/2019-06-22-Quotes_From_The_Alchemist.html",
    "href": "posts/2019/2019-06-22-Quotes_From_The_Alchemist.html",
    "title": "Quotes from the book Alchemist",
    "section": "",
    "text": "Singaporean travels\n\n\nThe Alchemist is written by the famous Brazilian lyricist and novelist Paulo Coelho. I recently finished reading his book and am sharing some of amazing quotes I liked from his books. The last quote made me start reading this amazing book.\n\nâ€ Thatâ€™s what Alchemist do. They show that when we strive to become better that we are, everything around us becomes better tooâ€.\n\n\nâ€œWhere your treasure is, there also will be your heartâ€.\n\n\nâ€ When you want something, the entire Universe conspires for the sameâ€\n\n\nâ€œThere is only one thing that makes a dream impossible to achieve: the fear of failure.â€\n\n\nâ€œAnd, when you want something, all the universe conspires in helping you to achieve it.â€\n\n\nA merchant sends his son to a sage to learn the secret to happiness. After traversing for forty days, the boy arrived at the luxurious palace of the sage, atop a hillock. He had to wait for two hours before seeing the polymath. â€˜Do you want to know the secret to happiness?â€™ asked the sage. â€œI am too busy to explain it right now. Go round the mansion to see the myriads of things there and come to me after two hours. While you saunter around the castle, make sure that the oil is not lostâ€, the old man said, after handing over a spoon with two drops of oil over to the boy. The boy strolled around the manor for two hours, but all his attention was on the spoon. After two hours, he presented himself before the wise man. â€œDid you not see everything?â€ the old man asked. â€œI could not watch everything clearlyâ€, he told the truth. â€œNever` mind, go around and relish the beauty of this edifice anewâ€, the old squire said. The boy felt elated. He set out to enjoy the elegance of the manor in a new way, holding the spoon containing two drops of oil. He admired the murals and frescoes on the ceiling, luxuriant gardens, captivating landscapes and panorama of the mountain ranges beyond all measure. Again, he appeared before the sagacious man and made an account of what he had seen. â€œWhat happened to the two drops of oil?â€ the mentor asked. Only then did the boy realize that the spoon had been empty.The secret of happiness lies in looking at all the marvels of the world without forgetting the drops of oil in the spoonâ€ the old man spoke very prudently; here the â€˜oilâ€™ stands for moral values while the â€˜spoonâ€™ for life. â€"
  },
  {
    "objectID": "posts/2019/2019-07-14-Brownie_points_on_SSD.html",
    "href": "posts/2019/2019-07-14-Brownie_points_on_SSD.html",
    "title": "Brownie points on Single Shot Multibox Detector",
    "section": "",
    "text": "In this article, I am going to talk about Single Shot Multibox Detector(SSD) which one of the algorithms which does object detection. SSD is known for detecting objects in real time faster compared to other algorithms.\nIf you are new to object detection, it can be simply defined as understanding what all are the objects in an frame. There are number of other algorithms like RCNN, fast RCNN, faster RCNN which perform object detection. Yet the problem is usually most of algorithms suck in real time performance.\n\nExtremely good in detecting objects of large size in real time\ncreates anchors through forward pass and obtains the result in the sufficient ways\n8732 boxes on building on base network of vgg16\nAccording to original research paper, we are able to obtain 74% mAP at 59 FPS for PASCALVOCC2007 test\nIt created by single shot, multi box, detector\nloss = confidence loss + location loss\nAlways classification loss should be small\nquite fast detection results\nSSD-500 and SSD-300 are of two types. The difference between both models are that: SSD300 uses 300px size images while SSD500 uses 512px size.\na real time detector compared to faster R-CNN, faster RCNN, fast RCNN\nSSD architecture builds on the venerable VGG-16 architecture, but discards the fully connected layers. The reason VGG-16 was used as the base network is because of its strong performance in high quality image classification tasks and its popularity for problems where transfer learning helps in improving results. Instead of the original VGG fully connected layers, a set of auxiliary convolution layers (from conv6 onwards) were added, thus enabling to extract features at multiple scales and progressively decrease the size of the input to each subsequent layer.\nMulti Box an Inception-style convolution network is used. The 1x1 convolutions that you see below help in dimensionality reduction since the number of dimensions will go down (but â€œwidthâ€ and â€œheightâ€ will remain the same).\nconfidence loss is computed by cross-entropy\nThe major difference between SSD and other detector algorithms, is the usage of region proposals which uses ground truth information which needs to be assigned to specific outputs in fixed set of Detector algorithms.\nFor each default box, we predict both shape offsets and confidences for all object categories\nFor a feature layer of size m*n where the kernel is applied, it produce an o/p layer which uses 3*3*p small kernels.\nFor each feature maps, it creates a set of kernels which can do unprecedented things, like understanding the relationship of what object needs to be recognised in Internet\n\n\nReferences\n\nSSD Original Paper\nReview-SSD"
  },
  {
    "objectID": "posts/2019/2019-09-08-fossmec1outn.html",
    "href": "posts/2019/2019-09-08-fossmec1outn.html",
    "title": "FOSSMEC Report(1/n)",
    "section": "",
    "text": "So in a land alien to me, I recieved this news about being made a Chairman of FOSSMEC for the coming academeic year. Let me document what all activities had taken place at FOSSMEC in the past few months.\nDuring the month of March we conducted a GSoC motivation talk, to find out the really interesting people who are planning to participate in GSoC 2019. About 15-20 people turned up for the event, where all the four GSoC scholars of 2018- Saran Narayanan, Vidhyadheeshan, Adarsh S and Ashwin G conducted the session. We planned for a proposal review section there after in the coming weeks.\nThe various students who particpated in GSoC proposal review section and submitted a GSoC proposal for 2019 were:\n1st year - Pranav sridhar, Sandeep(didnâ€™t submit proposals finally) 2nd year - Arun Hari, Ashwin M Prabhu, Devdutt Shenoi. 3rd year - Kurian Benoy, Jithin James.\nFinally GSoC results came and Ashwin M Prabhu got selected as a GSoC scholar from Moira organisation.\nFOSS in 30 Days was an awareness event conducted by FOSSMEC to learn the foundations of what is FOSS, itâ€™s core principles and applications. This event was co-ordinated by Sneha and various posters were designed by Priyangini, Ashwin G, Arun Hari, Devdutt, Joel, Nithin S(sorry if I have missed any names).\nMEC.Conf, Developer conference held on July 27, 28 and 29. This event was jointly organised by FOSSMEC, IEEE MEC and IETE. This event took enormous amount of effort in organising. There were 10 - 12 team like Software, Talks team, Documentation team, Devsprints team, ambience and much more. The theme which was initially planned was to be a FOSS Conference for both hardware and software. Then we moved to somewhat neutral term of Developer conference.\nThe organising team involved in activities ranging from outreach to various schools(to particpate in MEC.Conf), to bringing the best speakers, ambience of the venue, anchoring the sessions and arranging Devsprints. Devsprints was a unique first in Kerala for MEC.Conf. We had 5 Open source organisations like FOSSASIA, Dynamic Learning(from Processing foundation), Zulip, DVC and MEC.Contrib(from FOSSMEC) participating. Most of the participants made their first contribution during Devsprints.\nAlgorithm challenge which consisted of solving more than 50 Hackerrank challenges was the idea of Rahul R. He single handedly organised the entire event with the help of Ashwin G. About 40 participants participated and best part is even though there was no prize, there was active participation. It was amazing to hear people saying how much they had learned by even finishing in the 10 th position for this challenge.\nOrientation day of FOSSMEC for first years was conducted on September 5, 2019. The session of how to prepare for GSoC was taken by Adarsh S and the GSoC scholars Aswin G and Ashwin M Prabhu talked about their experience participating in GSoC. Also Varun and Mohita talked about the how they got an amazing internship in first year itself. I would like to thank everyone from FOSSMEC team for making it a memorable day 140+ first years.\nCode-a-pookalam contest at Govt. Model Engineering College and invites entries for pookalam designs made with code. The event details can be found here.\nAlso there are exciting events like Devsprints, FOSSMEC special section for GSoC, and much more coming soon. So stay tuned.\nFin."
  },
  {
    "objectID": "posts/2019/2019-07-19-Thoughts_on_linux_distros.html",
    "href": "posts/2019/2019-07-19-Thoughts_on_linux_distros.html",
    "title": "Thoughts on linux distros",
    "section": "",
    "text": "Linux comes in different variants based on your taste, interest and system design choices. Some of the most notable ones are: Ubuntu, Fedora, Redhat, Debian, Arch Linux. There are more than 1000of variants of linux. One of my college seniors ,Sarath Lakshman made a linux variant called Slynux. You can choose whichever linux variant as you want to be your favourite. If you are someone new to Linux World you can use linux distributions like Ubuntu/ Fedora/Linux Mint. My personal favourite beginner linux variant is Linux mint as it resembles very likely to Windows.\nI welcome all of you to Linux world :)"
  },
  {
    "objectID": "posts/2019/2019-03-15-FOSSASIA-Day3.html",
    "href": "posts/2019/2019-03-15-FOSSASIA-Day3.html",
    "title": "FOSSASIA Summit - Day 3",
    "section": "",
    "text": "Interview with Remi Denis Courmont\n\n\nThe day started off with Workshops from IBM. The first session was about Using Kuberneetes and KNative. The workshop was lead by Sai Vennam, IBM Engineer in US. I liked the way he took sessions and he personally cleared a lot of my doubts about Kuberneetes. The amount of preparation he did for workshop was amazing. Usually most of workshops end up being not so much learning and even if they teach something basics into the topics they donâ€™t go into depth of the things. It was followed by sessions on Responsible in AI using a cool tool called AI Fairness360. I went ahead to attend amazing talks by Francois about VLC 4.0.\nIt was followed by Codeheat presentations and Award cermony. A special applause to all winners of CodeHeat who have been contributiong to Opensource organisations for the past 6 months. Conducted an interview with Remi Denis Courtmount, Lead SW Developer of VLC. He really took questions on even some controversial things and he was really gratious to me during the entire duration of Interview. I had a long chat with teams from RedHat, Indeed and IBM. To be honest, I will rate this as my best Open source interview ever. Pics and entire interview will be released soon.\nI really started hacking for the UNESCO Indigenous Hackathon. My initial idea was to do transulation of a Malayalam website which can act as Open source alternative for meetup.com like thiing. I initially had some misconceptions about Open Event, which is a project of FOSSASIA. I found it similar to meetup.com, but Pradeep who was a previous GSoC scholar at FOSSASIA(also my roommate) showed all the features of Open Event and it turned out it was not something similar to meetup.com. My idea was a platform which will create awareness by Keshavan Maman post about new things like AI for parents also. So there is better awareness for parents about the events which are happening in the current scenario. I guess my idea was really vague and turned out I became a lone hacker.\n\n\n\nSnap from FOSSASIA Hackathon\n\n\n\n\n\nFront view of our hotel"
  },
  {
    "objectID": "posts/2019/2019-07-30-Malayalam_computing.html",
    "href": "posts/2019/2019-07-30-Malayalam_computing.html",
    "title": "Malayalam Computing(Highlight from talk by Santhosh Thottingal)",
    "section": "",
    "text": "During MEC.Conf, we had one of of our keynotes by Santhosh Thottingal, Developer Wikipedia and active contributor of Swathanthra Malayalam Computing.\n\nDuring his talk he had asked who all knew to type Malayalam in computer?\n\nVery few people raised their hands. He asked the audience to name some fonts in Malayalam equivalents to Aerial and TimesNew Roman which is familar to all of us. Only Lakshmi Sunil and Aathira Naveenan raised their hands. Santhosh emphasised in his talk that only 20% of world popluation know to read and write English. This is very true, I have seen a bunch of Switzerland natives who donâ€™t even know to spell their names correctly in Immigration counter.\nSanthosh challenged us to write a Hello world program in Amma Malayalam. Check out my hello world program\n>>> print(\"à´¹àµ†à´²àµà´²àµŠ à´µàµ‡àµ¾à´¡àµ \")\nà´¹àµ†à´²àµà´²àµŠ à´µàµ‡àµ¾à´¡àµ\nYou can even program just with Malayalam. Did you folks know Python3 was revamped from Python2 for unicode support? See the function implementation as shown here:\n>>> def à´—à´£à´¿à´•àµà´•(à´•,à´–):\n...     return(à´•*à´–)\n...  \n>>> à´—à´£à´¿à´•àµà´•(3,6)\n18\n\n\n\n\n\nPicture along with Santhosh Thottingal and Kavya Manohar\n\n\nHe told about some important Malayalam projects like Morphological Analyser, sentence predictor. The unique thing is most of his project doesnâ€™t use Machine learning hype. I am planning to understand these project and explore this in depth in coming months.\nAs an AI enthusiast. I enquired with Santhosh about the possibility of Malayalam computing expanding with Machine Learning. According to Santhosh, inorder to use machine learning in Malayalam, you need lot of data and infrastructure. So some of popular techniques in NLP like Neuro Machine Transulation cannot be applied for Malayalam. So most of the projects by SMC are discrete and definite in nature. Instead of predicting nature in Machine Learning.\nDo checkout Santhosh Thottingal sir being interviewed by Sreeram Venkitesh in the below link:\n\n\n\nInterview of Santhosh by Sreeram Venkitesh"
  },
  {
    "objectID": "posts/2019/2019-09-15-MLSlackgroups.html",
    "href": "posts/2019/2019-09-15-MLSlackgroups.html",
    "title": "Best slack communities for Machine Learning",
    "section": "",
    "text": "So this has been a blogpost to give a shout out to all these amazing Machine Learning Communities is immense.\nSome of the amazing Machine Learning communities are:\n\nOpen Data Science(ods.AI)\nKaggle Noobs\nDataScience Network(DSNet.org)\nTwiML community"
  },
  {
    "objectID": "posts/2019/2019-11-03-Devsprints_experience.html",
    "href": "posts/2019/2019-11-03-Devsprints_experience.html",
    "title": "Devsprints Experience",
    "section": "",
    "text": "Genearated image with stable diffusion 1.5 with prompot community-of-practice\n\n\nDevsprints is an event unlike in Hackathon, you start working in established projects which are usally FOSS/Open source. @MEC our Devsprints was planned to be a one day event on November 3, even though we would have loved to make the event duration a bit more long. We had participating organisations:\n\nDebian\nProcessing Foundation\nIndicNLP\nDVC\nHabitica\n\nThe day started of by me collecting [Abhijit PA], DD from Pattimatom which near my house. Abhijit works remotely from home and gets paid for his contributions to Debian by sponsoring companies(how cool is that). The event kickstarted at about 9:20 AM. At first we had Joel V Zachriah who introduced about why Devsprints and why contributing to DVC is important. We had various mentors who introduced their respective organisation during Devsprints. Abhijit pitched for Debian project. While Jithin KS came all the way from Chennai with his friend Anupam to mentor for Processing Foundation. I was the mentor for DVC and explained a bit about that project. Adam Shamsudden and Kamalraj introduced about what is IndicNLP and their project.\n\nNow for the DVC sprint, we had Ivan Shcklien joining us on a video call to see the particpants of the event. He gave a engaging talk to audience which was based on two planks that is: - What is DVC? - Why contribute to Opensource?\n\nParticipants of devsprints were delighted to see someone from SanFrancisco, USA joining for Devsprints. The video call plan with Ivan was a last minute effort and I should have planned such things much more earlier. Next as majority of DVC particpants were relatively new to Machine Learning, all of them attendes were asked to attend KamalRajâ€™s talk on NLP techniques in general. Kamalraj gave a very informative talk on various NLP techniques from the bag of words which was used in 1990s, to CountVectoriser methods, TFIDF and even Word2Vec(I finally was able to understand that). He went on explaining about Sentence classifying task, to named entity recognition, to much more. This talk was like 6 months of learning curve of content being condensed to a single 1 hour talk by one of the Indiaâ€™s best NLP enginners, KamalRaj. Adam after that explained the scope of this project and tried to woo some DVC contributors to IndicNLP.\n\nGood lunch was served for the event! (really)\nWe continued doing some actual work by afternoon. Installing development environment requirements in everyones laptop was a difficult especially with majority of laptops using windows. - solved 2-3 PRS for newbie contributors\nAfter that one my way back to home with Abhijit. We had an amazing conversation regarding FOSS, security. As most of the secure stuff was not available\n\nditch gmail(forwarers etc)\nremove unnecessary google maps\nFix other service, like mail client"
  },
  {
    "objectID": "posts/2019/2019-07-21-FastAI-lecturenotes-lesson7.html",
    "href": "posts/2019/2019-07-21-FastAI-lecturenotes-lesson7.html",
    "title": "FastAI Lesson-7 Notes",
    "section": "",
    "text": "Resnets came from one amazing researcher intution Kamming who understood that Conv nets can be made better by stacking things up together.\nRather than earlier understanding of:\nOutput = Conv2(Conv1(x))\nhe said\nOutput = x + Conv2(Conv1(x))\nHis theory was 56 layers worth of convolutions in that has to be at least good as the 20 layer version because it could always just set conv2 and conv1 to a bunch of 0 weights for everything except for the first 20 layers because the X (i.e.Â the input) could just go straight through. So this thing here is (as you see) called an identity connection. Itâ€™s the identity function - nothing happens at all. Itâ€™s also known as a skip connection.\n\n\n\nalt text\n\n\n\n\n\nDensenet- instead of weight + relu pattern followed by usual Resnets. We use concat function between two layer to do the intented functionaliy. Denseblocks get bigger and bigger They work really well for segmentation Resnets - skip connection\n\n\n\n\nTurn crappy images to good images (lesson7-superres-gans) Choose the proper functions and use appropriate resizing. If crappy is not proper make it do that. unet_learner can work in these datasets. And use imagenet stats blur, norm_type and self_attention = new functions\n\nlearn_get make it fit for gans water map removal is done in small amount of time\nA loss function which calls another model using GANS.\nCrappy image-> Generator->prediction discriminator(critics) -> pred In fastai new approach with pretrained models for Generator and Discriminator."
  },
  {
    "objectID": "posts/2019/2019-07-21-FastAI-lecturenotes-lesson7.html#coding",
    "href": "posts/2019/2019-07-21-FastAI-lecturenotes-lesson7.html#coding",
    "title": "FastAI Lesson-7 Notes",
    "section": "Coding",
    "text": "Coding\n\na seperate file for new folder generator and critic\nnn.BCEWIthLogitsLoss for loss_critic\nnot using resnet but gan_critic() for model.\nGANS hate momentum and with GANLearner\nLoss in the gans should have same critics and discriminator learning\nsome critics are really bad. The eyeballs of cat in critic does not know if itâ€™s proper\n\nUnets are when size of output is equal to size of input. eg: Segmentation,\nlesson7-wgan: do check out - downsampling unet encode and upsampled decoder\n\nSuperes Loss\n\nused F11 loss\nFeature loss class: m_feat is the pretrained loss use vgg_16_bnn for pretrained loss All the layers are RELU which are in block loss_features: all layers in network\n\nmake_features: grab a copy of a layer intermediate class uses hooks in pytorch feature loss gives a list and print length as sum get list which uses learning // see how cat is finally made in the lecture[1 hr:30 min]\nJason Handik, his project combined gaps. He created old black and white pictures and colourised them. WOw!\nFor good image restoration, need good crappy functions\n\n\nRNNâ€™s\n\nsee how basic NN wiht activation, various layers and shapes\nfinal output = batch_size* #class\n\nFinal Takeout from Jeremy Howards:\n\nRewatch lectures 2-3 times to understand concepts\nPractise by applying to real life problems\nKeep on improving"
  },
  {
    "objectID": "posts/2019/2019-02-11-SVD_NMF.html",
    "href": "posts/2019/2019-02-11-SVD_NMF.html",
    "title": "What is SVD and NMF? (Learning from Computational Linear Algebra course)",
    "section": "",
    "text": "Singular-value decomposition (SVD)\nSingular-value decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m Ã— n {mn} ï¿¼ matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics. Formally, the singular-value decomposition of an m Ã— n real or complex matrix M ï¿¼ is a factorization of the form U Î£ V âˆ— ï¿¼, where U ï¿¼ is an m Ã— m ï¿¼ real or complex unitary matrix, Î£ ï¿¼ is an m Ã— n rectangular diagonal matrix with non-negative real numbers on the diagonal, and V ï¿¼ is an n Ã— n ï¿¼ real or complex unitary matrix. The diagonal entries Ïƒ i ï¿¼ of Î£ ï¿¼ are known as the singular values of M ï¿¼. The columns of U ï¿¼ and the columns of V ï¿¼ are called the left-singular vectors and right-singular vectors of M ï¿¼, respectively. The singular-value decomposition can be computed using the following observations: The left-singular vectors of M are a set of orthonormal eigenvectors of MMâˆ—. The right-singular vectors of M are a set of orthonormal eigenvectors of Mâˆ—M. The non-zero singular values of M (found on the diagonal entries of Î£) are the square roots of the non-zero eigenvalues of both Mâˆ—M and MMâˆ—.source.\nYou maybe bored with this wikipedia definition. Yet what if I say to you Singular value decomposition is one of the most popular algorithms which have influence our society and is used in many fields like medicines, data science , Spacial science and much more like famous algorithms like fast fourier integration, Monte Carlo Integration , quick sort and much more. But according to math_rachael, the algorithm has not yet recieved the attentinon it really deserves to get.\nSVD is basically a small equation such that:\nA[data matrix] = U[left singular matrix] * epsilon(diagonal of singular values) * VT[right singular values)\nwhere: - A is an mn matrix - U is an mn orthogonal matrix - S is an nn diagonal matrix - V is an nn orthogonal matrix\nTo dive more into SVD check topic modelling from fast.ai Linear algebra class\nNon-negative Matrix Factorization\nRather than constraining our factors to be orthogonal, another idea would to constrain them to be non-negative. NMF is a factorization of a non-negative data set V:\nV=WH\ninto non-negative matrices W,H. Often positive factors will be more easily interpretable (and this is the reason behind NMFâ€™s popularity).\nNMF is a state of art feature detection algorithm. NMF is useful when there are many attributes and these attributes are weak predictabililty. By combining attributes, NMF can produce meaningful patterns, topics, or themes. NMF is often useful in text mining. In a text document, the same word can occur in different places with different meanings\nNMF is commonly used in : - Topic Modelling - Face Decomposition - Audio source seperation - Chemistry - Bioinformation and much more\nLetâ€™s dive more into NMF with a face decomposition examples : link"
  },
  {
    "objectID": "posts/2019/2019-03-13-FOSSASIA-Day1.html",
    "href": "posts/2019/2019-03-13-FOSSASIA-Day1.html",
    "title": "FOSSASIA Summit - Day 1",
    "section": "",
    "text": "Indoors of Changi Airport\n\n\nI arrived at Changi Airport, Singapore at morning at 7 AM. I was lucky to be catered by my fatherâ€™s friend. Uncle also accompanied with me to check-in RuckSackin, which was the FOSSASIA Hostel at about 3 PM. I arrived at LLI(Life Long Learning Institute), which was the official venue for FOSSASIA Conference at about 4 PM. I was supposed to be part of Event Setup, yet I got confused by the grandness of venue, and I joined with Speakers who were heading to check out Singapore.\nI was very thankful for Blueman and Lauernce for taking the time to guide us through the city and got introduced with Speakers and amazing FOSSASIA friends who were from India like Jogendra, IIT Pune batch, Akshat, Divyanchal and much more. We explored Marina Bay and had a walk through the Helix Bridge. I got to talk to a lot of speakers there and had a wonderful experience there having food from Hawkers street in Marina bay.\n\n\n\nRucksackin hotel\n\n\nAfter the dinner, I travelled back with Chathu and his friend who guided me to return to Paya Lebar MRT. At about 7:30 PM, I returned to LLI and helped in setting up the event. I met people like Akshat,humble_d, Rahul whom I have previously talked earlier and also forged a lot of new friends like Shubam Gupta, Shubam Kumar, Pranav, Harshit, Poonam, Anupam.\nAfter the pizza party, Mario had a brief chat about what FOSSASIA is planning to achieve in future and their Goals at the end of dinner. Mario is now passionate about building FOSS Project, which is useful of others as well. Compared to that project running in your system only. Also, at last, I was guided by Martin Lehr, who his TShirts collection and conference swag collection. He told about its history and story of each conference he attended and cool swags he collected. Also, I shared my room with Aakash, Anupam, Yash, Rahul and Pradeep."
  },
  {
    "objectID": "posts/2019/2019-03-01-Contributing_to_Open_Source_With_CloudCV.html",
    "href": "posts/2019/2019-03-01-Contributing_to_Open_Source_With_CloudCV.html",
    "title": "Contributing to OpenSource With CloudCV",
    "section": "",
    "text": "So here I am going to describe about experience contributing CloudCV organisation. I started to know about this organisation because of my friend Adarsh S who is a perivious GSoC winner, with CloudCV. CloudCV has mainly three Projects ie:\n\nEvalAI - a Open source platform for Machine Learning Competitions like Kaggle\nOrigami - A services to make your Machine learning models deployable anywhere in WEb\nFabrik - A model making software\n\nI started contributing to Origami initially from January. I started solving some issues initially. But the organisation mentors were not so active So some basic PRâ€™s I put in Origami didnâ€™t get merged as they were not active. I became also active in forums. Being Discouraged by lack of inactivity, I pinged sometime in January end, Cloud CV org Administrator. RishabJain told to me on pinging him that Origami wonâ€™t be participating this time. Instead of that he told me to start contributing on EvalAI instead. Some of the PRâ€™s I worked on Origami were related updating docs and requirements.\nThen I started contributing to EvalAI. EvalAI is actually is a competition platform for Deep Learning and Machine Learning challenges. Itâ€™s quite similar to Kaggle. Yet EvalAI has certain amazing features which make it popular among universities and other organisation. Do checkout more about EvalAI here.\nSO far I have worked on following PRâ€™s:\n\nAdd Leaderboard sorting for featured challenges:\nThis PR gave me an opportunity to learn about AngularJS. It made me learn about navigating through large code. Honestly it was lot of copy pasting as similar functionality was implemented in Challenges Module.\nAdd slug field\n\nIt taught me a lot about writing proper pep8 code and incorporating feedback from Mentors.\n\nTweet button - another learning experience\n\nMerged PR\n\nDocsfix\nFirstfix\n\nI know the docs fix doesnâ€™t match the patch requirement for Gsoc. Anyways I am looking to learn and contributing to this amazing community of EvalAI. Now Atlanda Time Now has been now my frequently queried Search result now."
  },
  {
    "objectID": "posts/2019/2019-03-16-FOSSASIA-Day4.html",
    "href": "posts/2019/2019-03-16-FOSSASIA-Day4.html",
    "title": "FOSSASIA Summit - Day 4",
    "section": "",
    "text": "Selfie with Sai Venam\n\n\nThis was a day where things were getting a bit serious. I was in Hackathon room for almost the entire day. Being a lone hacker, you sometimes gets to a point were you change ideas, loose momentum and even get depressed during Hackathon. There were some people who were mentoring Students there, and now I changed my Idea once again to something giving better interpretion of the Keralarescue.in. I planned on to use Watson Natural language apis and Kuberneetes. Kurd ie Sai vennam told me it was pretty easy to change from docker-compose to kubernetees and I started migrating process. He helped me a lot and told what all to get things working. I and Sai started opening a Cluster and doing even more cooler stuff. I attended a panel discussion and later attended amazing sessions. There was an amazing session about TF2.0 by Guardeon and Accesibily of ML:MLHub. I continued hacking even at Hostel and started preparing for my lightining talks.\n\n\n\nPosing with Singapore wheels in background\n\n\n\n\n\nInside Singapore wheel"
  },
  {
    "objectID": "posts/2019/2019-08-02-SeminarTopics.html",
    "href": "posts/2019/2019-08-02-SeminarTopics.html",
    "title": "Seminar ideas for project",
    "section": "",
    "text": "We are currently working in projects and works with greater impact: - CNN(can even beat a human in Imagenet challenge) CNN is an architecture in my opinion which uses Max pooling to increase the image size, and use a filter to obtain the output from each layer. At the end you have fully connected layers which are capable of producing obtaining what the image is in typical CNN. fc for a single digit is 9."
  },
  {
    "objectID": "posts/2019/2019-08-02-SeminarTopics.html#stacked-capsule-encoderssuitable-for-the-final-year-seminar",
    "href": "posts/2019/2019-08-02-SeminarTopics.html#stacked-capsule-encoderssuitable-for-the-final-year-seminar",
    "title": "Seminar ideas for project",
    "section": "1. Stacked Capsule Encoders(suitable for the final year seminar)",
    "text": "1. Stacked Capsule Encoders(suitable for the final year seminar)\n\nblogpost\nresearch paper\n\nIn summary, a Stacked Capsule Autoencoder is composed of: - the PCAE encoder: a CNN with attention-based pooling, - the OCAE encoder: a Set Transformer, - the OCAE decoder: K MLPs, one for every object capsule, which predicts capsule parameters from Set Transformerâ€™s outputs, KÃ—M constant 3Ã—3 matrices representing constant object-part relationships, and the PCAE decoder, which is just M constant part templates, one for each part capsule. SCAE defines a new method for representation learning, where an arbitrary encoder learns viewpoint-equivariant representations by inferring parts and their poses and groups them into objects. This post provides motivation as well as high-level intuitions behind this idea, and an overview of the method. The major drawback of the method, as of now, is that the part decoder uses fixed templates, which are insufficient to model complicated real-world images. This is also an exciting avenue for future work, together with deeper hierarchies of capsules and extending capsule decoders to three-dimensional geometry. If you are interested in the details, I would encourage you to read the original paper: A. R. Kosiorek, S. Sabour, Y.W. Teh and G. E. Hinton, â€œStacked Capsule Autoencodersâ€, arXiv 2019."
  },
  {
    "objectID": "posts/2019/2019-08-02-SeminarTopics.html#deep-learning-for-classicial-japaneseseminar",
    "href": "posts/2019/2019-08-02-SeminarTopics.html#deep-learning-for-classicial-japaneseseminar",
    "title": "Seminar ideas for project",
    "section": "Deep Learning for Classicial Japanese(seminar)",
    "text": "Deep Learning for Classicial Japanese(seminar)\n\nPaper"
  },
  {
    "objectID": "posts/2019/2019-08-02-SeminarTopics.html#data-augmentation-using-learned-transformations-for-one-shot-medical-image-segmentation",
    "href": "posts/2019/2019-08-02-SeminarTopics.html#data-augmentation-using-learned-transformations-for-one-shot-medical-image-segmentation",
    "title": "Seminar ideas for project",
    "section": "Data Augmentation using Learned transformations for one-shot medical image segmentation",
    "text": "Data Augmentation using Learned transformations for one-shot medical image segmentation\n\npaper\n\nUpdate: on August 7\nFinally the paper of Deep Learning for Classical Japenese got selected by sir.\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/2019/2019-03-14-FOSSASIA-Day2.html",
    "href": "posts/2019/2019-03-14-FOSSASIA-Day2.html",
    "title": "FOSSASIA Summit - Day 2",
    "section": "",
    "text": "Today was when the 4 day long FOSSASIA International Conference started. We woke up a bit late, due to some time difference. The first talk which I attended was the State of OpenTech by Mario Behling. When I was sitting the crowd, I noticed someone who was from VideoLAN. I just went ahead and told to him how much I liked VLC and asked if he could introduce me to Remi for being part of my opensource interview series. He replied back saying I am Remi. There was a feeling of suprisement and afraidness after meeting Remi.\n\n\n\nNervous selfie with VLC core Developer\n\n\nI had heard a lot about this guy. He was the lead Software developer of VideoLAN and wrote a lot of code(about 20% which Francois clarified to me). I have heard he gives a lot of strong comments to patches in VLC, just like Linus Torvalds is famous for. Yet here he is sitting just besides me. I asked for a quick selfie and was too afraid even to be sitting besides him.\n\n\n\nTalk by Graham Williams\n\n\nI told about the interview thing and told I will come when he is free. I ran virtually away from Remi, during Lunch break. There were amazing sessions about how Hardware can be used from Mitch and Bunni. At next we had a panel discussion about â€œBuisness,Government,Science-What Opportunities Doesâ€Openâ€ Bring to societyâ€ composing of an amazing panel with V Srinivasta, Hong Pong, Graham Williams, Carlsen, Bunnie and our own FOSSASIA Fellows Mario Behling and Hong Pong. I explored the Exhibition hall in the evening and attended another panel discussion on â€œIs future of FOSS Bright?!â€ lead by Michael Downey.\n\n\n\nA miniature version of india in Singapore"
  },
  {
    "objectID": "posts/2019/2019-09-03-What_is_Openstack.html",
    "href": "posts/2019/2019-09-03-What_is_Openstack.html",
    "title": "What is the difference between Open stack and AWS EC2?",
    "section": "",
    "text": "Cloud computing is one of the buzz words of today. Today there are a lot of cloud operators in the market like Amazon, IBM, Google Cloud Platform and other associate technologies of each company like Amazon has EC2, ECS, CloudWatch, Lamda server and much more.\nOpen stack is a free and open source platform for cloud computing. Open stack is an infrasturcture providing company where virtual servers, containers, bare metal are made available to customers. It controls controls large pools of compute, storage, and networking resources throughout a datacenter, all managed and provisioned through APIs with common authentication mechanisms.\nIf you are looking for a neutral cloud provider, Open stack may be the best option because all their software is Open source and have lot of associated members like OpenSuse, Oracle, RedHat, IBM, HP etc. Open stack is now being used by a lot of reliable users like BBC, GoDady, CERN, BMW etc.\nOpenStack is a cloud operating system that controls storage and networking resouce through datacentre, by giving peace of mind for users who provisions this through a WEB user interface.\nWhile Amazon EC2 is service in Amazon web service which offers virtual servers on rent. With just servers in Amazon EC2 you can rent computers only. Yet for deployment of services and storages you need to use other technologies in Amazon Tech stack.\nThis is where the main difference between amazon EC2 and Open stack, while Open stack is a big computing provider with itâ€™s storage provided(Swift), networking, security(Keystone), Orchestration(Heat) and even more. While EC2 is just a small part of entire computing stack of Amazon Web services.\nNova is the OpenStack project that provides a way to provision compute instances (similar to AWS EC2). Nova supports creating virtual machines, baremetal servers (through the use of ironic), and has limited support for system containers. It can be used with any associated . Currently amazon has a lot more fraction of users that Open stack environment. Yet situation is going to change in a few years with lot more Cloud Operators."
  },
  {
    "objectID": "posts/2019/2019-09-23-paper_summary.html",
    "href": "posts/2019/2019-09-23-paper_summary.html",
    "title": "Speaking style adaption in text-to-speech synthesis usng sequence-to-sequence models with attention",
    "section": "",
    "text": "Please note: I am writing this research paper summaries in mind to build an advanced(State of the art) Text-to speech system for Malayalam, as a FOSS project for SMC. I would like to thank for all the help I have recieved from Santhosh Thottingal, so far and suggesting me this project.\nAbstract:\nCurrently, there is an increasing interests in text-to-speech (TTS) synthesis to use sequence-to-sequence models with attention. However, in challenging speaking styles, like Lombard speech, which has higher intensity and fundamental frequency(F0) being large, cuurent approaches are not always efficent. In this study we propose a transfer learning method to adapt a sequence-to-sequence based TTS system of normal speaking style to Lombard style. More- over, we experiment with a WaveNet vocoder in synthesis of Lombard speech. We conducted subjective evaluations to assess the per- formance of the adapted TTS systems. The subjective evaluation results indicated that an adaptation system with the WaveNet vocoder clearly outperformed the conventional deep neural network based TTS system in synthesis of Lombard speech.\nA system which was build using the Seq2Seq-TTS models and the mel-spectogram as output which employed the Wavenet Vocoder got higher accuracy when trained with only 30 min- utes of Lombard speech, system S5 generated synthetic speech that was most Lombard-like among the systems compared.\nThe contributions of the paper are twofold. First, we develop a speaking style adaptation system using a Seq2Seq-TTS model. Sec- ond, we study the use of a WaveNet vocoder for the application of Lombard speech synthesis. To the best of our knowledge, the cur- rent study is the first investigation on speaking style adaptation of speech synthesis using a modern Seq2Seq-TTS system.\nSEQ2SEQ-TTS System\nDespite promising results in synthetic speech using Statistic parametric speech synthesis. These models depend heavily on encoder-decoder neural network structures that map a sequence of characters to a sequence of acous- tic frames. These models combine the front-end and back-end and learn relations between them from data only. When sequence-to- sequence models are coupled with neural vocoders, they enable generating raw waveforms directly from text. It was demonstrated that state-of-the-art results in TTS can be achieved with the sequence-to-sequence technology.\nThe model accepts either mono-phonemes or graphemes as inputs and emits acoustic parameters as outputs. It consists of three main components: 1) en- coder, 2) attention, and 3) decoder. The encoder takes text sequence x of length L as input, which represented either in the character or phoneme domain as one-hot vectors. The encoder learns a continu- ous sequential representation h using various neural network archi- tectures such as LSTMs and/or CNNs.\nh = encoder(x)\n\nÎ±t = attention(stâˆ’1 , Î±tâˆ’1 , h)\n\nct = sum of(Î±t, ht)\n\nyt = decoder(stâˆ’1 , ct )\nEquations in paper to fear you\nwhere stâˆ’1 is the (t âˆ’ 1)-th state of the decoder recurrent neural network and Î±t âˆˆ RL are the attention weights or the alignment and ct is the context or attention vector. The decoder takes the previous hidden state stâˆ’1 and the current context vector ct as inputs and generates the current output yt . This process runs until the end of the utterance is reached\nAdaption of Seq2Seq models\nUsing Adaption we are able to generate amamzing results that show models which generate synthetic speech of good quality using around 30 minutes of data. In the present study, we first train a Seq2Seq- TTS system using a large amount of normal speech of one speaker and then fine-tune the learned model with normal speech of another speaker with limited data. Finally, using Lombard speech of the latter speaker, we fine-tune the model again to synthesize Lombard speech. We predict both mel-spectrograms and the World vocoder parameters as output acoustic frames. To render final speech wave- form, we employ both the WaveNet vocoder and the World vocoder.\n\nExperiments mentioned in the model\nTO experiment, the model used Blizzard Challenge 2011 speech corpus. This corpus contains utterances of 12000 voices(which add upto 16 hours). The dataset contains voices of: a) Nancy (Normal) b) Nick (Normal) c) Nick(Lombard)\nWe create five systems for this model experimentation The systems were different in terms of their output parameter types and the vocoder used. System S1 is the baseline system which uses a LSTM-type of recurrent neural network (RNN)- baseTTS system for adaptation, and synthesizes the speech waveform using the World vocoder. System S2 is built using the Seq2Seq-TTmodel, and the final waveform is rendered by the World vocodeSystems S3 and S4 have same architectures as systems S1 and S2respectively, but they use the WaveNet vocoder for synthesis. System S5 has the same architecture and vocoder as S4, but instead ousing the World vocoder parameters, it predicts the mel-spectrogram as the output.\nTwo types of listening tests were conducted: 1) speaking style sim- ilarity test and 2) comparison category rating (CCR) test of speech naturalness. The goal of the similarity test is to assess whether the technology developed is capable of generating synthetic speech of different speaking styles (normal vs.Â Lombard) while the CCR test aims to evaluate how much the naturalness of speech is sacrificed when the speaking style is adapted. We used an evaluation setup\nConclusion\nThis paper compared different TTS models and vocoders to adapt the speaking style of speech synthesis from normal to Lombard. The study proposes using an adaptation method based on fine-tuning combined with sequence-to-sequence based TTS models and the WaveNet vocoder conditioned using mel-spectrograms. Listening tests show that the proposed method outperformed the previous best method that was developed using a LSTM-RNN based adapted sys- tem. Future work includes an extensive subjective evaluations and training both the WaveNet and Seq2Seq-TTS model in a single pipeline."
  },
  {
    "objectID": "posts/2019/2019-03-17-FOSSASIA-Day5.html",
    "href": "posts/2019/2019-03-17-FOSSASIA-Day5.html",
    "title": "FOSSASIA Summit - Day 5",
    "section": "",
    "text": "For the last 4 days, I have never arrived in time for conference(the earliest I reached was at 9:30 AM. Today was nothing different except I was also having an opportunity to tak in FOSSASIA summit. Pradeep, Anupam(who was also taking about GraphQL that day) and I went together in morning from Rucksackinn Hotel(FOSSASIA Hostel). I had my talk scheduled for 10:45 where I was talking about Keralarescue.in website.\n\n\n\nMy lightning talk\n\n\nI had a video in my ppt of about 1min30s showing severity of Kerala floods, then I went ahead talking how platform was build, the features of keralarescue.in and scale at which keralarescue.in operated, which helped in saving millions of talk. Since this was my first talk in a big venue, I had my own hiccups. I didnâ€™t manage my time properly, the talk I was supposed to complete in 5 minutes ended only at 7min 30s. Luckily for me, the next speaker who was supposed to speak didnâ€™t turn out, so I was allowed to continue my talk by Moderator(Ronald Turner).\nSaptak and Saran told me some tips on how to manage time talks in future which I plan to stick on in future. After that, the day was lit for me. As I was able to interview Hardware Hero Mitch Haltman and Michael Downey, DIAL. I had a lot of conversations with exhibitors in FOSSASIA. I talked to Coreboot founders about their product and now at the time of writing this article only I realise Coreboot is a GSoC organisation this year. Yea conferences are an opportunity to literally talk to anyone out there quite freely and person who you talks may turn out be CEOs, Directors in Microsoft, Opensource Core Developers with tons of experience.\n\n\n\nEating out with Sayan and Saptak\n\n\nThis day was made memorable by going together with @Saptak013 and @yudocaa to eat the local crusines in Singapore at a Hawkers place. It was amazing eating Chilly Crab, Prawns, Momo and Hotpot(recommended by @OrangeCMS ). #memories live for ever and @yudocaa taught me how to use chopsticks and he even gifted me a set of chopsticks to practise even at home and keep on teaching this art of using Chopsticks :)\n\n\n\nTravelling to gardens of Singapore\n\n\n\n\n\nIndoors of Changi Airport"
  },
  {
    "objectID": "posts/2019/2019-10-01-gobeyondhacktoberfest.html",
    "href": "posts/2019/2019-10-01-gobeyondhacktoberfest.html",
    "title": "How to go beyond Hacktoberfest?",
    "section": "",
    "text": "Hacktoberfest Logo\n\n\nLot of folks I know are Open source contributors because of one crazy reason, Hacktoberfest. Itâ€™s perfectly okay to get a cool T-shirt shipped from America, grab swags and claim to be an Open source Developer.\n\nJust contribute to Open source and opportunities will fly towards you\n\n\nMario Behling, Co-founder FOSSASIA\n\nThis is perfectly justified by a number of people who has done great things just by contributing to Open source. Read Vishnu Ko experience in Open source and how he is now a Software Developer at Zulip(which pays him handsomely as well).\nVishnu Experience\nWhen you are properly contributing to Open source. There is a number of things to consider like from properly formulating git commit messages(https://chris.beams.io/posts/git-commit/), to communication between team members, etc.\nTry to learn more on how to get into GSoC by properly contributing to Open source like following the steps here."
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "",
    "text": "This blogpost is a short summary of the research paper authored by Tarin Clanuwat, Mikel Bober-Irizar(18 y.o Kaggle grandmaster), Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha. This research paper was presented in NeurIPS 2018.\nJapan is an amazing place and my knowledge of the land of Samurais, bullets trains has increased after I started learning about this my paper which was mentioned as a must read materials for Kaggle competition of Kuzushiji Recognition."
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#abstract",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#abstract",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Abstract",
    "text": "Abstract\n\nTo encourage ML researchers to produce models for Social or Cultural relevance to transcribe Kuzushiji into contemporary Japanese characters.\nTo release Kuzushiji MNIST dataset, Kuzushiji 49 and Kuzushiji-Kanji datasets to general public."
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#introduction",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#introduction",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\nJapan This place was so cool!\n\n\nHistorically, Japan and itâ€™s culture had been isolated from the west for a long period of time. Untill the Meiji restoration in 1868, when a 15 year old emperor brought unity to whole of Japan which was earlier broken down into regional small rulers. This caused a massive change in Japanese Language, writing and printing system. Even though Kuzushiji had been used for over 1000 years there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). So now most Japan natives cannot read books written and published over 150 years ago. In General Catalog of National Books, there is over 1.7 million books and about 3 millions unregistered books yet to be found. Itâ€™s estimated that there are around a billion historical documents written in Kuzhushiji language over a span of centuries. Most of this knowledge is now inaccessible to general public.\nWith this research paper, three easy to use pre-processed datasets has been released for Machine learning research to help in Recognising Kuzhushiji, domain transfer of contents from unseen Kuzhushiji Kanji to Modern Kanji(classical Japanese literature). Also the baseline classification results for the same has been mentioned in this paper."
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#kuzhushiji-dataset",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#kuzhushiji-dataset",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Kuzhushiji Dataset",
    "text": "Kuzhushiji Dataset\nThe Japanese language can be divided into two types of systems:\n\nLogographic systems, where each character represents a word or a phrase (with thousands of characters). A prominent logographic system is Kanji, which is based on the Chinese System.\nSyllabary symbol systems, where words are constructed from syllables (similar to an alphabet). A prominent syllabary system is Hiragana with 49 characters (Kuzushiji-49), which prior to the Kuzushiji standardization had several representations for each Hiranaga character.\n\nThe Kuzhushiji dataset is created by National Institute of Japanese Literature(NIJL) and is curated by Center for Open Data in Humanities(CODH). This dataset includes characters in both Kanji and Hiranaga, based on pre-processed images of characters from 35 books from the 18th century. It includes 3 parts:\na)Kuzhushiji MNIST:\nMNIST for handwritten digits is one of the most popular datasetâ€™s till and is usually the hello world for Deep Learning. As a easy to process beginner dataset, this is consist of 10 classes with 7000 images for each. Yet there are fewer than 49 letters needed to fully represent Kuzhushiji Hirangana. So currently we choose 10 rows of Hirangana when creating dataset with 5 letter being stacked together to form this dataset. Each image is 28*28 pixel resolution. Kuzhushiji MNIST is more difficult compared to MNIST because for each image the chance for a human to detect characters correctly when a single image is of small size and is stacked together of 5 rows is very less. Also there are more challenges in Kuzhushiji recognition.\n\nKuzhushiji 49:\n\nAs the name suggest, it is a much larger imbalanced dataset containing 49 hirangana characters with about 266,407 images. Both Kuzhushiji-49 and Kuzhushiji-MNIST consists of grey images of 28*28 pixel resolution. The training and test is split in ratio of 6/7 to 1/7 for each classes. There are several rare characters with small no of samples such as (e) in hirangana has only 456 images.\nc)Kuzhushiji Kanji:\nKuzhushiji Kanji has a total of 3832 classes of characters in this dataset with about 140,426 images. Kuzhushiji-Kanji images are are of larger 64x64 pixel resolution and the number of samples per class range from over a thousand to only one sample. This dataset is not created merely for classification images, instead for more creative experimental task.\nIf you are interested in downloading the dataset with detailed documentation.\nGo here"
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#experiments",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#experiments",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Experiments",
    "text": "Experiments\n\nClassification of Kuzushiji Characters\nIn machine learning and statistics, classification is a supervised learning approach in which the computer program learns from the data input given to it and then uses this learning to classify new observation. This data set may simply be bi-class (like identifying whether the person is male or female or that the mail is spam or non-spam) or it may be multi-class too when more than two objects needs to be classifed.\nWe try to focus on calculating the accuracy of recognising Kuzushiji datasets which in both Kanji and Hiragana, based on pre-processed images of characters from 35 books from the 18th century for datasets Kuzushiji-MNIST, Kuzushiji-49 and Kuzushiji Kanji, each having respectively 9, 49 and 3832 classes for the dataset.\nThe following table shows the accuracy of various algorithms which is used in the research paper baseline. ROIS-CODH calls out for improving the results on the Kuzushiji dataset and there are even interesting Kaggle competitions to get state of the art results for this competition.The current state of the art model which gives a better performance is Resnet networks being ensembled over Capsule networks.\n\n\n\nPhiladelphiaâ€™s Magic Gardens. This place was so cool!\n\n\nAccording to paper we get 97.33% accuracy for dataset on using PreActResnet18 + manifold mixup and 98.9% accuracy for Resnet Networks ensembed over Capsule Networks. Letâ€™s take a closer look at both this architectures:\n\nPreAct Resnet with Manifold mixup\n\nA method for learning better representations, that acts as a regularizer and despite its no significant additional computation cost , achieves improvements over strong baselines on Supervised and Semi-supervised Learning tasks. Manifold Mixup is that the dimensionality of the hidden states exceeds the number of classes, which is often the case in practice.\nWhen deeper networks starts converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated and then degrades rapidly. For almost all segmentation, classification and object detection and even regression with tabular data, Resnet gives state of the art results.\n\nResnet networks ensembled with Capsule networks\n\nThe best results were achieved with an ensemble of VGG and ResNet â€“ a 98.9% accuracy on the test set, which is a state-of-the-art result on the new dataset. TO unnderstand how this model gives this much accuracy, itâ€™s essential to understand about Convolutional Neural networks. Capsule neural networks were introduced by Jeffrey Hinton and his team to solve a very important important disadvantage of Convolutional neural networks(CNN).\nInternal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects. For a CNN, a mere presence of these objects can be a very strong indicator to consider that there is a face in the image. Orientational and relative spatial relationships between these components are not very important to a CNN. As a mere presence of 2 eyes, a mouth and a nose in a picture does not mean there is a face, yet CNN always see like that.\nFor objects in 3D representation, the relationship of various angles and poses are not properly mapped in a usual CNN. Yet deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. Intuition behind them is very simple and elegant. This has been now possible due to a new algorithm for Dynamic Routing between Capsules, for calculating distance between capsule.\nOn blending our architecture with Resnets which help in converging our neural networks with less loss rate and using Capsule networks for better representation of objects. We are able to get state of art results for classifying Kuzushiji letters."
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#domain-transfer",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#domain-transfer",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Domain Transfer",
    "text": "Domain Transfer\nDomain transfer focuses on pixel images, we explore instead the transfer from pixel images to vector images, across two different domains ie from Kuzushiji japanese to contemporary Japanese. Our proposed model aims to generate Modern Kanji versions of a given Kuzushiji-Kanji input, in both pixel and stroke-based formats. We employ KanjiVG, a font for Modern Kanji (ie the Japanese Language) in both pixel and stroke format. This is a various interesting application to bring life to an almost extinct language with usage of Machine Learning.\n\n\n\nDomain transfer architecture\n\n\n\nArchitecture of Domain Transfer\nInput is converted to contempary language from the old Kuzushiji-KanjiVG format of 64x64px resolution format. We employ KanjiVG, a font for Modern Kanji in a stroke-ordered format. Variational Autoen-coders [14,18] provide a latent space for both Kuzushiji-Kanji and a pixel version of KanjiVG. A Sketch-RNN model is then trained to generate Modern Kanji strokes, conditioned on the VAEâ€™slatent space. Predicting pixel versions of Modern Kanji using a VAE also aids human transcribers as the blurry regions of the output can be interpreted as uncertain regions to focus on.\nWe first train two separate ConvolutionalVariational Autoencoders, one on the Kuzushiji-Kanji dataset, and also a second on a pixel version ofKanjiVG dataset rendered to 64x64 pixel resolution for consistency. The architecture for the VAEis identical to and both datasets are compressed into their own respective 64-dimensional latentspace,Zold and Znew.\n\n\nComponents of this model\n\nAuto-encoder and decoder\n\nThey are widely used unsupervised application of neural networks whose original purpose is to find latent lower dimensional state-spaces of datasets, but they are also capable of solving other problems, such as image denoising, enhancement or colourization. Variational Autoencoders is used to provide latent space of KanjiVG to Kuzushiji Kanji. Itâ€™s used in the architecture to finetune the input and provide better colourization and enhancement. Itâ€™s used in complex generative models.\n\nMixture Density Networks\n\nUsed to model density function to a new domain. Itâ€™s used for making the neural networks to translate from Kuzushiji Kanji to KanjiVG format in pixels.\n\nSketch RNN\n\nItâ€™s a decoder network which conditions the model in a new latent vector.\nAlgorithm 1. Train two seperate variational autoencoder on pixel version of KanjiVG and Kuzhushiji-Kanji 2. Train mixture density network to mode P(Znew | Zold) as mixture of gaussians. 3. Train sketch RNN to generate Kanji VGG strokes conditioned on either znew or z~new ~P(Znew|Zold)"
  },
  {
    "objectID": "posts/2019/2019-08-10-Deeplearningjapan.html#why-not-such-a-system-for-malayalam",
    "href": "posts/2019/2019-08-10-Deeplearningjapan.html#why-not-such-a-system-for-malayalam",
    "title": "Deep Learning for Japanese Classical Literature-Paper Review",
    "section": "Why Not such a system for Malayalam?",
    "text": "Why Not such a system for Malayalam?\nIn Malayalam , there are about 1200+ letters in the old Dravidian Malayalam which was prominently. In 1956 a government order reduced the total plausible characters as 120 to make it compatible with ASCII format.\nin the current Malayalam alphabets there are not much characters. Yet there is no need for a domain transfer system in Malayalam all the Malayalam characters are mapped in Unicode format by Swanthanthra Malayalam community. So unlike in Japanese old and new letter mappings in unicode format are different while in Malayalam both have same unicode mapping.\nFin."
  },
  {
    "objectID": "posts/2019/2019-06-26-Jewels_from_DGPLUG_training.html",
    "href": "posts/2019/2019-06-26-Jewels_from_DGPLUG_training.html",
    "title": "Jewels from DGPLUG training",
    "section": "",
    "text": "Dgplug conduct awesome FOSS training every year. I have decided to participate this years training. The following are some of the questions which have been answered during training by experts here.\nQuestion:\nmbuf, what are the basics Industry expect from a Bachelors student?\nAnswer:\n<mbuf> django_master, I cannot answer for the industry, but, there are two schools of thought that you should be familiar with\n\n\n<mbuf> django_master, there is the cathedral style of working, where you do what your managers ask you to do, and ask no questions; most of the service companies follow this model\n\n\n<mbuf> django_master, there is also the bazaar model in lot of start-up like companies, where they have the FLOSS culture, where you question everything and see how to improve thingns\n\n\n<mbuf> django_master, there are few large enterprises, where small teams work in a bazaar model too\n\n\n<mbuf> django_master, to understand the differences, you need to read http://www.catb.org/~esr/writings/cathedral-bazaar/\n\n\n<mbuf> next\n\n\n<mbuf> django_master, basically your basics should be strong, attitude to learn\n\n\n<mbuf> django_master, if you want to survive and work in FLOSS companies\n\n\n<mbuf> django_master, again, it is important to work in a project that interests you, than be stuck in the wrong company\n\nQuestion:\nI get stuck when solving harder problems and bugs that take about 4-5 days effort. How to show perseverance when solving bigger problems and when mentors continuosly get back to you with comments?\nAnswer:\n<mbuf> django_master, this is where the communication guidelines come into play\n\n\n<mbuf> django_master, you need to describe all the approaches you have taken to solve the problem or bug, document it, maybe in a blog post, and send it for review\n\n\n<mbuf> django_master, and if the mentors find that you have really put in the effort, then they may guide you or give you pointers\n\n\n<mbuf> django_master, nobody said life is easy; if you are afraid, or scared to experiment, learn and try out things, you are in the wrong industry\n\n<mbuf> django_master, there is a reason why engineers are paid well :) (to solve hard problems)\nQuestion:\nHave you given any talk proposal for Pycon India?\nAnswer:\n<sayan> django_master: nope!\n\n\n<sayan> I'm not working much on Python these days\n\n\n<django_master> Oh you are working infrastructure track right?\n\n\n<sayan> django_master: Yup\n\n\n<kushal> django_master, I would love to, but, my proposals do get rejected with strange columns over many years\n\n\n<sayan> kushal: push harder?\n\n\n<django_master> kushal, What if you are not getting selected. How can anyone get selected for Pycon India\n\n\n<kushal> sayan, whom should I push?\n\n\n<sayan> Try to find something which is not on the internet?\n\n\n<django_master> kushal, weren't you a speaker last time?\n\n\n<sayan> kushal: you just need to work harder?\n\n\n<kushal> sayan, or like a senior software engineer\nQuestion:\nEveryone here talks about importance of blogging. How frequently we should blog and on what topics?\nAnswer:\n<mbuf> django_master, Stephen King (the writer) says the ratio of reading to writing is 10:1\n\n\n<mbuf> django_master, if you read 10x, then you will write a 1x piece\n\n\n<django_master> mbuf, amazing answer\n\n\n<mbuf> django_master, writing is an important habit in our culture, and we want everyone to write, get it reviewed by others as well\n\n\n<mbuf> django_master, it is useful documentation not only for you, but, for others as well\n\n\n<mbuf> I will encourage every one to read this Stephen King's book \"On Writing\" https://en.wikipedia.org/wiki/On_Writing:_A_Memoir_of_the_Craft\n\n\n<mbuf> he talks a lot about writing fiction, but, the practices are very good; I will not give away the answer, but, will let you all read it\n\n\n<mbuf> Try to write at least one blog post per week; that is a good number\n\n\n<mbuf> of course, if you are working on a massive piece, you can take your time on it\n\n\n<mbuf> documentation is as important as writing code, and we emphasize that a lot; so, if you have good writing habits, you will do well here andtra in life; otherwise, start working on it"
  },
  {
    "objectID": "posts/2019/2019-07-23-EmacsvsVim_war.html",
    "href": "posts/2019/2019-07-23-EmacsvsVim_war.html",
    "title": "Emacs vs VIM war",
    "section": "",
    "text": "Emacs and vim are arguably among the best text editors of all time. You may occasionally find war tweets like this:\n\nI met one of my neighbours and we both have lot of things in common. We both use GNU Linux and use Arch Linux as well. Yet there is something big seperating us, I use emacs while my neighbour use vim. - arocks\n\nDuring FOSSASIA Summit 2019 we had a election. Graham William, Director Microsoft asked to audience who all use Emacs here? A lot of hands got raised. Then he asked who all use vim.\nThat day: vim won the war with a large majority .\nI voted that day for vim, even though I didnâ€™t know Emacs then. Recently I picking up Emacs and find it really useful with lot of features. You can even call Emacs the editor of lifetime as this speaker said.\nMy Rant: Taste both Emacs and vim before you next vote"
  },
  {
    "objectID": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#introduction",
    "href": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#introduction",
    "title": "Research paper Summary",
    "section": "Introduction",
    "text": "Introduction\nIn this paper we discuss about neural network-based system for Text to Speech(TTS) synthesis that can generate speech audio in the voice of different speakers, including those unseen during training. This research paper was presented during 32nd conference of NeurIPS 2018, Montreal, Canada and is authored by researchers from Google Inc.\nThe goal of this work is to build a TTS(Text to speech) system which can generate natural speech for a variety of speakers in a data efficent manner. We specifically address a zero-shot type learning system, where a speakers cloned audio can be generated for any speech on getting few seconds of untranscribed reference audio.\nHere the approach is to decouple speaker modelling from speech synthesis by independently training a speaker-discriminative network that captures charteristics from small amount of speaker audio data."
  },
  {
    "objectID": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#multispeaker-speach-synthesis-model",
    "href": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#multispeaker-speach-synthesis-model",
    "title": "Research paper Summary",
    "section": "Multispeaker speach synthesis model",
    "text": "Multispeaker speach synthesis model\nOur system is composed of three independently trained neural networks:\n\nA recurrent speaker encoder which computes a fixed dimensional vector from a speech signal.\nSequence-to-sequence synthesizer which predicts a mel spectrogram from a sequence of grapheme inputs, conditioned on speaker emedding vector.\nAn autoregressive Wavenet vocoder, which converts the spectogram into time domain waveforms.\n\n\nSpeaker Encoder\nIt is used to condition the synthesis network on reference speech signal from the desired target speaker. It is critical for capturing the crucial characteristics of different speaker and identify the phonetic signal, background noise.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM layers of 768 cells, each followed by a projection to 256 dimensions. The final embedding is created by L2-normalizing the output of the top layer at the final frame. During inference, an arbitrary length utterance is broken into 800ms windows, overlapped by 50%.\nThe network is not optimized directly to learn a representation which captures speaker characteristics relevant to synthesis, we find that training on a speaker discrimination task leads to an embedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n\nSynthesizer\nThe synthesizer is trained to convert a sequnce of phoneme or graheme sequence(the text or audio) and is converted into log-mel spectograms, which are commonly used in Audio applications of Deep learning. The Tactron2 architecture to support multiple speakers is extended to recurrent sequence-to-sequence with attention.\nAn embedding vector for the target speaker is concatenated with synthesizer encoder output at each step, thus passing embeeding to the attention layer converges across different speakers.The predicted mel spectrogram by synthesizer captures all the relevant details needed for higly qualidy synthesis of various voices.\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to a sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words and proper nouns. The network is trained in a transfer learning configuration, using a pretrained speaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio, i.e.Â the speaker reference signal is the same as the target speech during training\n\n\nNeural Vocoder\nThe Wavenet acts as a vocoder to invert the synthesized mel spectrograms emitted by the synthesis network into time-domain waveforms, ie the final voice you are hearning. Wavent architecture is composed of 30 dilated convolution layers. The network is not directly conditioned on the output of the speaker encoder.\n\n\nInterference and zero-shot speaker adaption\nIn practise we find that using a single audio clip of a few sconds duration is sufficent to synthesize new speech with the corresponding speaker characteristics, representing zero-shot adaption to novel speakers.\nSurprising the results on interference of male and female show some characteristics like strongly pitched voice of menâ€™s voice while females voice are found to be having larger speaking rate for a embedded voice generated. Itâ€™s amazing to see how Machines have learned all this."
  },
  {
    "objectID": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#experiments",
    "href": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#experiments",
    "title": "Research paper Summary",
    "section": "Experiments",
    "text": "Experiments\nThe Google team performed extensive reseach and trained the speaker encoder on a properietary voice seach corpus engine containing 36M utterances and compared resuults two public datasets for speech syntesis and vocoder networks. a) VCTK - contains 44 hours of clean speech from 109 speakers b) LibriSpeech - contains 436 hours of speech from 1172 speakers.\nThe model performed really well, yet not reached the human level of intellgences which is measured in Mean Opinion score(MOS).\nThey compared the architecture based on the following parameters: - Speech Naturalness - Speaker similarity - Speaker verification - Speaker embedding space - Number of speaker encoder training speakers - Fictious speakers\nDetailed results can be found within the paper."
  },
  {
    "objectID": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#conclusion",
    "href": "posts/2019/2019-09-11-MultispeakerTTS_summary.html#conclusion",
    "title": "Research paper Summary",
    "section": "Conclusion",
    "text": "Conclusion\nIf you have reached till this point, kudos to you as most of research papers are so intimidating to read and even reading a summary of itâ€™s contents are no less easy.\nThis system gives the SOTA for Text to speech systems and is able to achieve near-human accuracy on comparing Speaker simiilarity. This research paper is currently implemented withGoogle clouds Text to Speech API. We learned about the of MultiSpeaker TTS system and covered the various components like Encoder, Vocoder , Synthesiser in this post."
  },
  {
    "objectID": "posts/2019/2019-09-29-oldproject.html",
    "href": "posts/2019/2019-09-29-oldproject.html",
    "title": "Pinch of old project",
    "section": "",
    "text": "So our projects aim was to create an app which was to:\n1.Which automatically detects the plants whenever you walk in a garden and tell what plant it is. 2.On top of it if you like the plant we detected we will connect you to plant producers who sell that plant in your locality. 3.Identify plant from an image you showAs from the article, we need to collect photos of indigenous plants from my area.\nI created a dataset of 10 types of Trees in Indian-continental condition with about 1000 images for each plant. I created this dataset using the google-images-download library. Itâ€™s pretty straight forward to use:\ngoogleimagesdownload --keywords \"Polar bears, baloons, Beaches\" --limit 20\nI remember to download more than 100 images we need to insert an API key, or something like that. More documentation can be found here.\nI created an ML model using tensorflow by using a custom model which was retrained using Inceptionv3 model. retrain.py file from Tensorflow was used for this project. If I am implementing my prject currently I would prefer to use more efficent architectures like Resnet 50, Efficent Net and others.\nThe model was neatly made into an App by my friend using Kotlin. Then came another issue, that is ML model was about 90-100 MBs and big size is always an issue for the app. For model deployments, tensorflow created an API called TensorflowLITE which can substantially make your models very light about 8 MB. We dropped aim (b), as creating such partnership didnâ€™t look feasible.\nThe developed apps demo for this can be found in my Linkedin Post here which was made one year back and the source code of old Machine Learning model is."
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html",
    "href": "posts/2019/2019-08-07-projecttopics.html",
    "title": "Final Year Project Abstract",
    "section": "",
    "text": "According to our teachers before doing a project. The following premilinary works needs to be completed:"
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html#college-attendance-monitoring-footfall",
    "href": "posts/2019/2019-08-07-projecttopics.html#college-attendance-monitoring-footfall",
    "title": "Final Year Project Abstract",
    "section": "College Attendance monitoring/ footfall",
    "text": "College Attendance monitoring/ footfall\nCurrently most of the colleges does attendance monitoring manually. The process of automating this process looks promising in the coming years. Using face recognition, we can mark the corresponing attendance for each student coming to college. And the footfall of students coming to the college in general can be monitored by object detection algorithms like RCNN, YOLO, Retinanet, .. and choose the best of the worlds.\nPaper: in folder"
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html#medical-image-segmentation-for-mri-data",
    "href": "posts/2019/2019-08-07-projecttopics.html#medical-image-segmentation-for-mri-data",
    "title": "Final Year Project Abstract",
    "section": "Medical Image segmentation for MRI data",
    "text": "Medical Image segmentation for MRI data\nImage segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images. We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation.\npaper: https://arxiv.org/pdf/1902.09383v2.pdf github: https://github.com/xamyzhao/brainstorm"
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html#medical-image-classification-of-retiopathy",
    "href": "posts/2019/2019-08-07-projecttopics.html#medical-image-classification-of-retiopathy",
    "title": "Final Year Project Abstract",
    "section": "Medical Image classification of Retiopathy",
    "text": "Medical Image classification of Retiopathy\nAn easy way to detect diabetic retiopathy which causes bindness to adults.\n. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospitalâ€™s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium\nCurrently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be.\nIn this synchronous Kernels-only competition, youâ€™ll build a machine learning model to speed up disease detection. Youâ€™ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.\nDetect there what stage a disease and at what stage is the disease? https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5961805/"
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html#how-closer-are-we-to-human-eyes",
    "href": "posts/2019/2019-08-07-projecttopics.html#how-closer-are-we-to-human-eyes",
    "title": "Final Year Project Abstract",
    "section": "How closer are we to Human eyes?",
    "text": "How closer are we to Human eyes?\nWe are currently working in projects and works with greater impact: - CNN(can even beat a human in Imagenet challenge) CNN is an architecture in my opinion which uses Max pooling to increase the image size, and use a filter to obtain the output from each layer. At the end you have fully connected layers which are capable of producing obtaining what the image is in typical CNN. fc for a single digit is 9."
  },
  {
    "objectID": "posts/2019/2019-08-07-projecttopics.html#text-to-speech-system-for-generating-ads-of-famous-people",
    "href": "posts/2019/2019-08-07-projecttopics.html#text-to-speech-system-for-generating-ads-of-famous-people",
    "title": "Final Year Project Abstract",
    "section": "Text to speech system for generating ADS of famous people",
    "text": "Text to speech system for generating ADS of famous people\nClone a voice in 5 seconds to generate arbitary speech in real-time. https://github.com/CorentinJ/Real-Time-Voice-Cloning\npaper"
  },
  {
    "objectID": "posts/2019/2019-09-21-Paper_summaries.html",
    "href": "posts/2019/2019-09-21-Paper_summaries.html",
    "title": "Literature review for my project",
    "section": "",
    "text": "Abstract:\nCurrently, there are increasing interests in text-to-speech (TTS) syn- thesis to use sequence-to-sequence models with attention. These models are end-to-end meaning that they learn both co-articulation and duration properties directly from text and speech. Since these models are entirely data-driven, they need large amounts of data to generate synthetic speech with good quality. However, in chal- lenging speaking styles, such as Lombard speech, it is difficult to record sufficiently large speech corpora. Therefore, in this study we propose a transfer learning method to adapt a sequence-to-sequence based TTS system of normal speaking style to Lombard style. More- over, we experiment with a WaveNet vocoder in synthesis of Lom- bard speech. We conducted subjective evaluations to assess the per- formance of the adapted TTS systems. The subjective evaluation re- sults indicated that an adaptation system with the WaveNet vocoder clearly outperformed the conventional deep neural network based TTS system in synthesis of Lombard speech.\nThe results of the style similarity test are plotted in Figure 4. From the right pane of the figure, it can be observed that synthesized speech by all adapted systems were rated to sound different from natural normal speech with high confidence. When compared to the natural Lombard reference (left pane), system S5 was rated highest, followed by systems S3, S4, S2, and S1. System S5 was built us- ing the Seq2Seq-TTS model and the mel-spectrogram as output. It can be clearly seen that those systems that employed the WaveNet vocoder got higher scores than the ones that used the more tradi- tional World vocoder. Further, system S5, which is based on con- ditioning the WaveNet vocoder with mel-spectrograms, got a higher score than the ones that used the World vocoder, confirming the find- ings made in a earlier study [33]. From this results we can conclude that even though we trained the WaveNet vocoder with only 30 min- utes of Lombard speech, system S5 generated synthetic speech that was most Lombard-like among the systems compared.\nConclusion\nThis paper compared different TTS models and vocoders to adapt the speaking style of speech synthesis from normal to Lombard. The study proposes using an adaptation method based on fine-tuning combined with sequence-to-sequence based TTS models and the WaveNet vocoder conditioned using mel-spectrograms. Listening tests show that the proposed method outperformed the previous best method that was developed using a LSTM-RNN based adapted sys- tem. Future work includes an extensive subjective evaluations and training both the WaveNet and Seq2Seq-TTS model in a single pipeline."
  },
  {
    "objectID": "posts/2019/2019-09-21-Paper_summaries.html#fastspeech-fast-robust-and-controllable-speech",
    "href": "posts/2019/2019-09-21-Paper_summaries.html#fastspeech-fast-robust-and-controllable-speech",
    "title": "Literature review for my project",
    "section": "FastSpeech: Fast, Robust and Controllable speech",
    "text": "FastSpeech: Fast, Robust and Controllable speech\nAbstract Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end- to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of con- trollability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in paral- lel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive mod- els in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech. We will release the code on Github.3\nBackground\nText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech given text, has long been a hot research topic in the field of artificial intelligence. The research on TTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to neural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality of the synthesized speech by end-to-end models is close to human parity. Neural network based end-to-end TTS models usually first convert the text to acoustic features (e.g., mel-spectrograms) and then transform mel-spectrograms into audio samples. However, most neural TTS systems generate mel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech usually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody control). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively, which sufficiently handles the above problems.\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the encoder-decoder framework: The encoder takes the source sequence as input and generates a set of representations. After that, the decoder estimates the conditional probability of each target element given the source representations and its preceding elements. The attention mechanism [2] is further introduced between the encoder and decoder in order to find which source representations to focus on when predicting the current element, and is an important component for sequence to sequence learning. In this work, instead of using the conventional encoder-attention-decoder framework for sequence to sequence learning, we propose a feed-forward network to generate a sequence in parallel.\nFast Speech Architecture:\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel- spectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the encoder-attention-decoder based architecture as adopted by most sequence to sequence based autore- gressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of FastSpeech is shown:\n\nFeedForward Transformer\nLength Regulator\nDuration Predictor\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22] and 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in Figure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram transformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with a length regulator (which will be described in the next subsection) in between to bridge the length gap between the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and 1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head attention to extract the cross-position information. Different from the 2-layer dense network in Transformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that the adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram sequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the experimental section. Following Transformer [22], residual connections, layer normalization, and dropout are added after the self-attention network and 1D convolutional network respectively.\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme and spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and part of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram sequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of the mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe how to predict phoneme duration in the next subsection). Based on the phoneme duration d, the length regulator expands the hidden states of the phoneme sequence d times, and then the total length of the hidden states equals the length of the mel-spectrograms.\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration predictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by the layer normalization and the dropout layer, and an extra linear layer to output a scalar, which is exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT blocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of mel-spectrograms for each phoneme with the mean square error (MSE) loss."
  },
  {
    "objectID": "posts/2019/2019-09-22-Break_Half-way_mark.html",
    "href": "posts/2019/2019-09-22-Break_Half-way_mark.html",
    "title": "Break the Half-way mark(0.52 LB score)",
    "section": "",
    "text": "Data Science Network(DSNet) organised a Kaggle competition as part of Kaggle days workshop. This competition will be a beginner friendly competition aimed at helping you get started with kaggle. Special recognition and awards will be granted to those who create useful and healthy discussions and kernels.\nI created a beginner friendly kernel, which gave good introduction to get a Decent score in this competition with this competition.\nI am just 1 bronze medal away from being a Kaggle Expert in Kernels. I have two ideas for new kernels:\n\nFastAI code explained for Kannada MNIST\nDVC for Cats vs Dogs problem"
  },
  {
    "objectID": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html",
    "href": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html",
    "title": "What is DevSprints (MEC.conf)?",
    "section": "",
    "text": "Has contributing to Open source projects been one of those plans for you that just remained a plan? Or did you try contributing to an Open source project but got lost somewhere in the middle due to lack of guidance? Well, dev sprints are here to save the day! During Dev sprints, Mentors/Contributors of Open source projects guide participants through everything, from generating the development build to submitting a patch for a bug.\nIt doesnt matter if you are a newbie or an experienced contributor, just dive in and get to work. It is also a great opportunity to understand and experience the ways of the Open source. Although, there will be situations when you might feel stuck, but there is nothing to worry about as the mentors present there can provide you instant help to get you moving. We can say this from experience, that participants of these dev sprints have not been limited to their Open source contribution just for the conference days.They are still active contributors.\nSo at MEC.Conf we are having these organisations mainly:\nAnd there will be a bunch of interesting personal projects as well."
  },
  {
    "objectID": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#format",
    "href": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#format",
    "title": "What is DevSprints (MEC.conf)?",
    "section": "Format",
    "text": "Format\nOur Devsprints are planned to be held for 14 hours from 27th July,2019 to 28th July. So unlike hackathons, in Devsprints there is no prizes and biggest take away is being part of a community and help in some long term project by contribute to open source projects by putting up a few PRs for the organisation. Unlike when you are talking and contributing to a unkwown personâ€™s project. Here we have mentors who will help you get started contributing to various projects."
  },
  {
    "objectID": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#prerequisites",
    "href": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#prerequisites",
    "title": "What is DevSprints (MEC.conf)?",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you are trying to come for our Devsprint and want to get a good vibe during event. Always believe in yourself and donâ€™t under-estimate your skill. If you into coding, there are a bunch of amazing projects in Devsprints organisation. Being proactive and coming setting up your interested projects development environment is recommended. Knowing the basics of git may help you contribute quickly.\nHey if you are a non-coding person! You can help various organisations write documentation and in designing things."
  },
  {
    "objectID": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#how-to-participate",
    "href": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#how-to-participate",
    "title": "What is DevSprints (MEC.conf)?",
    "section": "How to participate?",
    "text": "How to participate?\nAnyone who have registered for the MEC.Conf will be able to attend devsprints."
  },
  {
    "objectID": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#references",
    "href": "posts/2019/2019-07-25-Understanding_what_is_DevSprints_MEC.Conf.html#references",
    "title": "What is DevSprints (MEC.conf)?",
    "section": "References",
    "text": "References\n\nhttps://treyhunner.com/2019/04/making-the-most-of-the-pycon-sprints/\nhttps://in.pycon.org/blog/2017/understanding-dev-sprints.html\n\nDisclaimer: Since it is our first time, pardon us for our mistakes. We will try our level best to make this event memorable for you"
  },
  {
    "objectID": "posts/2019/2019-09-17-Precision_values.html",
    "href": "posts/2019/2019-09-17-Precision_values.html",
    "title": "Float/decimal points precision in C++ and python3",
    "section": "",
    "text": "Manipulating and printing outut format with specific no of digits in precision is necessary for many cases. This kind of questions are sometimes asked in Coding interviews and you find in weird case were you know answer, but donâ€™t know to format the output.\nIn C++ Float precision is implemented by the header <iomanip>. Note std::fixed input, will extend the digits even if it does not exist\n#include <iostream>     // std::cout, std::fixed\n#include <iomanip>      // std::setprecision\n\nint main () {\n  double f =3.14159;\n  std::cout << std::setprecision(5) << f << '\\n';\n  std::cout << std::setprecision(9) << f << '\\n';\n  std::cout << std::fixed;\n  std::cout << std::setprecision(5) << f << '\\n';\n  std::cout << std::setprecision(9) << f << '\\n';\n  return 0;\n}\nWhile in languages like Python it follows Float point precision similar to C programming formatter like:\nf = 3.14159\nprint(\"%.5f\" %f)\nprint(\"%.9f\" %f)\nprint(\"%.5f\" %f)\nprint(\"%.9f\" %f)\n\nComplement:\nBIT Manipulation ie XOR(^) , Left Shift(<<), Right shift(>>)"
  },
  {
    "objectID": "posts/2019/2019-08-14-aimal.html",
    "href": "posts/2019/2019-08-14-aimal.html",
    "title": "à´†àµ¼à´Ÿàµà´Ÿà´¿à´«à´¿à´·àµà´¯àµ½ à´‡à´¨à´±à´²à´¿à´œàµ»à´¸àµà´‚ à´•à´®àµà´®àµà´¯àµ‚à´£à´¿à´¸àµà´±àµà´±àµ à´ªà´šàµà´šà´¯àµà´‚",
    "section": "",
    "text": "This is a blogpost which is not written by me. This was orginally written by GopiKrishnan.\nà´…à´Ÿà´¿ à´•à´ªàµà´ªàµà´¯à´¾à´°àµ‡ à´•àµ‚à´Ÿàµà´Ÿà´®à´£à´¿ à´Žà´¨àµà´¨ à´šà´¿à´¤àµà´°à´‚ à´¨à´®àµà´®àµ¾ à´Žà´²àµà´²à´¾à´µà´°àµà´‚ à´•à´£àµà´Ÿà´¿à´Ÿàµà´Ÿàµà´£àµà´Ÿà´¾à´•àµà´‚. à´…à´¤à´¿àµ½ à´•à´žàµà´šà´¾à´µàµ à´šàµ†à´Ÿà´¿ à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾à´¤àµ† à´…à´¤à´¿à´¨àµ à´µàµ†à´³àµà´³à´‚ à´’à´´à´¿à´šàµà´šà´¿à´Ÿàµà´Ÿàµ à´ªàµ‹à´•àµà´¨àµà´¨ à´®àµà´•àµ‡à´·à´¿à´¨àµ† à´¨à´®àµà´®àµ¾ à´•à´£àµà´Ÿà´¤à´¾à´£àµ. à´‡àµ— à´¸àµ€àµ» à´’à´´à´¿à´µà´¾à´•àµà´•à´¾àµ» à´•àµ‡à´°à´³ à´ªàµ‹à´²àµ€à´¸à´¿à´¨àµà´±àµ† à´•à´žàµà´šà´¾à´µàµ à´µàµ‡à´Ÿàµà´Ÿà´¯àµà´•àµà´•àµ à´µàµ‡à´£àµà´Ÿà´¿ à´¨à´¿à´™àµà´™àµ¾ à´’à´°àµ à´®àµŠà´¬àµˆàµ½ à´†à´ªàµ à´‰à´£àµà´Ÿà´¾à´•àµà´•àµà´• à´†à´£àµ†à´¨àµà´¨àµ à´µà´¿à´šà´¾à´°à´¿à´•àµà´•àµà´•. à´…à´¤à´¾à´¯à´¤àµ à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´†à´ªàµ à´µà´šàµà´šàµ à´šàµ†à´Ÿà´¿à´¯àµà´Ÿàµ† à´ªà´Ÿà´‚ à´ªà´¿à´Ÿà´¿à´šàµà´šà´¾àµ½ à´…à´¤àµ à´•à´žàµà´šà´¾à´µà´¾à´£àµ‹ à´…à´²àµà´²à´¯àµ‹ à´Žà´¨àµà´¨àµ à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´¯à´¾àµ» à´•à´´à´¿à´¯àµà´‚. à´¸à´¾à´§à´¾à´°à´£ à´•à´®àµà´ªàµà´¯àµ‚à´Ÿàµà´Ÿàµ¼ à´ªàµà´°àµ‹à´—àµà´°à´¾à´®à´¿à´™àµ à´µà´šàµà´šàµ à´‡à´¤àµ à´šàµ†à´¯àµà´¯àµà´• à´µà´³à´°àµ† à´¬àµà´¦àµà´§à´¿à´®àµà´Ÿàµà´Ÿà´¾à´£àµ.\nà´‡à´™àµà´™à´¨àµ† à´‰à´³àµà´³ à´¸à´¾à´¹à´šà´°àµà´¯à´¤àµà´¤à´¿à´²à´¾à´£àµ à´¨à´®àµà´®àµ¾ machine learning à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨à´¤àµ. à´‡à´µà´¿à´Ÿàµ† à´¨à´®àµà´®àµ¾ à´šàµ†à´¯àµà´¯àµà´¨àµà´¨à´¤àµ à´¨à´®àµà´•àµà´•àµ à´’à´°àµ machine learning à´…àµ½à´—àµ‹à´°à´¿à´¤àµà´¤à´‚ à´‰à´£àµà´Ÿàµ à´¨à´®àµà´®àµ¾ à´•à´žàµà´šà´¾à´µàµ à´šàµ†à´Ÿà´¿à´•à´³àµà´Ÿàµ† à´šà´¿à´¤àµà´°à´™àµà´™à´³àµà´‚ à´…à´²àµà´²à´¾à´¤àµà´¤ à´šàµ†à´Ÿà´¿à´•à´³àµà´Ÿàµ† à´šà´¿à´¤àµà´°à´™àµà´™à´³àµà´‚ à´¤àµà´Ÿàµ¼à´šàµà´šà´¯à´¾à´¯à´¿ à´•à´¾à´£à´¿à´šàµà´šàµ à´•àµŠà´Ÿàµà´•àµà´•àµà´¨àµà´¨àµ. à´“à´°àµ‹ à´¤à´µà´£à´¯àµà´‚ à´‡àµ— à´šà´¿à´¤àµà´°à´™àµà´™à´³à´¿àµ½ à´‰à´³àµà´³ à´ªà´¿à´•àµâ€Œà´¸àµ½ à´²àµ†à´µàµ½ à´®àµà´¤àµ½ à´‰à´³àµà´³ à´ªà´¾à´±àµà´±àµ‡à´£àµà´•àµ¾ à´¨à´®àµà´®àµà´Ÿàµ† machine Learning à´…àµ½à´—àµ‹à´°à´¿à´¤à´‚ à´ªà´ à´¿à´•àµà´•àµà´‚. à´‡à´¤à´¿à´¨àµ† à´¨à´®àµà´®àµ¾ à´Ÿàµà´°àµ†à´¯à´¿à´¨à´¿à´‚à´—àµ à´Žà´¨àµà´¨àµ à´µà´¿à´³à´¿à´•àµà´•àµà´‚.\nà´‡àµ— à´Ÿàµà´°àµ†à´¯à´¿à´¨à´¿à´‚à´—àµ à´ªàµà´°àµŠà´¸à´¸àµà´¸àµ à´•à´´à´¿à´¯àµà´®àµà´ªàµ‹àµ¾ à´²àµ‡àµº à´šàµ†à´¯àµà´¤àµ à´µà´šàµà´š à´ªà´¾à´±àµà´±àµ‡à´£àµà´•àµ¾ à´’à´°àµ à´®àµ‹à´¡àµ½ à´†à´¯à´¿ à´¨à´®àµà´®àµà´Ÿàµ† à´…àµ½à´—àµ‹à´°à´¿à´¤àµà´¤à´‚ à´¸àµà´±àµà´±àµ‹à´±àµ à´šàµ†à´¯àµà´¤àµ à´µà´¯àµà´•àµà´•àµà´‚. à´‡à´¨à´¿ à´‡àµ— à´®àµ‹à´¡àµ½ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµ à´¨à´®àµà´•àµà´•àµ à´’à´°àµ à´šàµ†à´Ÿà´¿ à´•à´žàµà´šà´¾à´µà´¾à´£àµ‹ à´…à´²àµà´²à´¯àµ‹ à´Žà´¨àµà´¨àµ à´ªàµà´°àµ†à´Ÿà´¿à´•àµà´±àµà´±àµ à´šàµ†à´¯àµà´¯à´¾àµ» à´•à´´à´¿à´¯àµà´‚.\nà´‡à´™àµà´™à´¨àµ† à´¨à´¾à´‚ à´‰à´£àµà´Ÿà´¾à´•àµà´•àµà´¨àµà´¨ à´®àµ†à´·àµ€àµ» à´²àµ‡à´£à´¿à´‚à´—àµ à´®àµ‹à´¡à´²àµà´•àµ¾ à´ªàµ‚àµ¼à´£à´®à´¾à´¯àµà´‚ à´…à´¤à´¿à´¨àµ à´ªà´ à´¿à´•àµà´•à´¾àµ» à´•àµŠà´Ÿàµà´•àµà´•àµà´¨àµà´¨ à´¡à´¾à´±àµà´± à´ªàµ‹à´²àµ† à´‡à´°à´¿à´•àµà´•àµà´‚. à´¤àµ†à´±àµà´±à´¾à´¯ à´¡à´¾à´±àµà´± à´•àµŠà´Ÿàµà´¤àµà´¤à´¾àµ½ à´…à´¤à´¿à´¨à´¨àµà´¸à´°à´¿à´šàµà´šàµà´³àµà´³ predictions à´†à´¯à´¿à´°à´¿à´•àµà´•àµà´‚ à´¨à´®àµà´®àµà´•àµà´•àµ à´²à´­à´¿à´•àµà´•àµà´¨àµà´¨à´¤àµ. à´‡àµ— à´ªàµà´°à´¶àµà´¨à´™àµà´™àµ¾ à´Žà´²àµà´²à´¾ machine learning software à´•à´³à´¿à´²àµà´‚ à´‰à´£àµà´Ÿà´¾à´•àµà´‚. à´‡à´¤à´¿à´¨àµ† à´†à´£àµ à´®à´¿à´•àµà´•à´ªàµà´ªàµ‹à´´àµà´‚ à´®à´¾à´§àµà´¯à´®à´²àµ‹à´•à´‚ AI à´…à´ªàµà´ªàµŠà´•àµà´•à´¾à´²à´¿à´ªàµà´¸àµ à´Žà´¨àµà´¨àµŠà´•àµà´•àµ† à´ªà´±à´žàµà´žàµ à´¤à´³àµà´³à´¿ à´µà´¿à´Ÿàµà´¨àµà´¨à´¤àµ.\nà´…à´¤à´¿à´¨àµà´±àµ† à´’à´°àµ à´‰à´¦à´¾à´¹à´°à´£à´‚ à´¨à´®àµà´®àµà´Ÿàµ† à´†à´ªà´¿à´²àµ† à´®àµ‹à´¡àµ½ à´Ÿàµà´°àµ†à´¯à´¿àµ» à´šàµ†à´¯àµà´¯à´¿à´•àµà´•àµà´µà´¾àµ» à´¨à´®àµà´®àµ¾ à´•àµŠà´Ÿàµà´•àµà´•àµà´¨àµà´¨ à´¡à´¾à´±àµà´±à´¯à´¿àµ½ à´’à´°àµ à´šàµ†à´±à´¿à´¯ à´¤àµ†à´±àµà´±àµ à´ªà´±àµà´±à´¿.à´•à´žàµà´šà´¾à´µà´¾à´£àµ†à´¨àµà´¨àµ à´ªà´±à´žàµà´žàµ à´¨à´®àµà´®àµ¾ à´•àµŠà´Ÿàµà´¤àµà´¤ à´šà´¿à´¤àµà´°à´™àµà´™àµ¾ à´µàµ‡à´±àµ† à´šàµ†à´Ÿà´¿à´•à´³àµà´Ÿàµ† à´†à´£àµ†à´™àµà´•à´¿àµ½ à´¨à´®àµà´®àµà´Ÿàµ† à´®àµ‹à´¡àµ½ à´ªà´ à´¿à´•àµà´•àµà´¨àµà´¨à´¤àµ à´† à´ªà´¾à´±àµà´±àµà´±àµ‡àµºà´¸àµ à´†à´¯à´¿à´°à´¿à´•àµà´•àµà´‚. à´† à´¤àµ†à´±àµà´±à´¾à´¯ à´®àµ‹à´¡àµ½ à´‰à´³àµà´³ à´†à´ªàµ à´ªà´¿à´±àµà´±àµ‡à´¦à´¿à´µà´¸à´‚ à´®à´¨àµ‹à´°à´®à´¾ à´²àµ‡à´–à´•àµ» à´Ÿàµ†à´¸àµà´±àµà´±àµ à´šàµ†à´¯àµà´¯àµà´•à´¯àµà´‚ à´¶àµ‡à´·à´‚ à´•à´®àµà´®àµà´¯àµ‚à´£à´¿à´¸àµà´±àµà´±àµ à´ªà´šàµà´š à´•à´žàµà´šà´¾à´µà´¾à´£àµ†à´¨àµà´¨àµ à´†àµ¼à´Ÿàµà´Ÿà´¿à´«à´¿à´·àµà´¯àµ½ à´‡à´¨àµà´±à´²à´¿à´œàµ»à´¸àµ à´ªà´ à´¨à´‚ à´¤àµ†à´³à´¿à´¯à´¿à´šàµà´šàµ à´Žà´¨àµà´¨àµ à´µà´¾àµ¼à´¤àµà´¤ à´•àµŠà´Ÿàµà´•àµà´•àµà´•à´¯àµà´‚ à´šàµ†à´¯àµà´‚ðŸ˜‚"
  },
  {
    "objectID": "posts/2019/2019-05-14-Monoliths_vs_microservices.html",
    "href": "posts/2019/2019-05-14-Monoliths_vs_microservices.html",
    "title": "Monoliths vs Microservices",
    "section": "",
    "text": "In this article, I will be talking about monoliths and microservice system architecture. Recently Hrishikesh Bhaskaran gave a talk on this topic at Facebook F8 meetup. Th.is talk inspired me and changed my thoughts about both. So I am going to highlight some of key takeways in this blogpost."
  },
  {
    "objectID": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#monoliths",
    "href": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#monoliths",
    "title": "Monoliths vs Microservices",
    "section": "Monoliths",
    "text": "Monoliths\nMonoliths can roughly be described as a big combined stack. it includes user interfaces and data access all in the same silo. So taking Social media as an example - all functionalities like userfeed, authentication, photos are all enclosed in a single application.\nIn case of Microservices, all the functionalities are seperated whenever required so we can use the bleeding edge technology whenever required.\nSome basic terminology\nVertical scaling - increase RAM/My of CPU service Horizontal scaling - more replicas Loadbalancers - A load balancer is a device that distributes network or application traffic across a cluster of servers. Load balancing improves responsiveness and increases availability of applications. One of the easiest methods for scaling.\n\nWhy Monlith are easy to build?\n\nLess time for initial build\nEasy installation and deployment of services\ntesting of service is very easy\n\nNext we will discuss about microservices.Today microservices is a buzz word like AI, Machine Learning, Blockchain. This makes everyone very tempting to use microservices for whatever you make. As Hrishi said, please note monolith is not a out-dated technology as many think about it. It recommended to use monoliths when you are building a stack with no previous experience in it.\nExtras :DockervsKuberneetes:Not an or question"
  },
  {
    "objectID": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#micoservices",
    "href": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#micoservices",
    "title": "Monoliths vs Microservices",
    "section": "Micoservices",
    "text": "Micoservices\nSome of the characteristics of microservices are:\n\nPer Martin Fowler and other experts, services in a microservice architecture (MSA) are often processes that communicate over a network to fulfill a goal using technology-agnostic protocols such as HTTP.\nIncreased uptime for servers, as only one service need to made down to scale a paricular component compared to monoliths which has downtime for entire services.\nUsing new technologies is easy\nVarious Database and Language support for each services is possible like R, JS, Python. You run backend both in flask, rust and golang for various microservices. This allows in using the best of everything.\nHelps in easy onboarding for new member and help in agile architecture of development, which is popular now.\n\n\nDisadvantages:\nMicroservices is distributed computing, so there is a lot of problems are associated with it: - All services should have same IP, the network use VPC for communication of various microservices to each other. - We are assuming usually lattency is zero, bandwidth is infinity in distributed computing. This may not always hold true in real world systems. - Using microservice for small 2 teams is not ideal usually. - Various services in microservice, can communicate each other. So seperate data cost is needed for internal communication."
  },
  {
    "objectID": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#when-to-use-monoliths-vs-microservices",
    "href": "posts/2019/2019-05-14-Monoliths_vs_microservices.html#when-to-use-monoliths-vs-microservices",
    "title": "Monoliths vs Microservices",
    "section": "When to use Monoliths vs Microservices",
    "text": "When to use Monoliths vs Microservices\nThis is actually a million dollar question according to me. Personally I had always used microservices as my defacto method of development and encountered lot of issues with it. Once I was developing a website for 1 day use purpose, to be used at-most maximum of 100 people. And using microservices made it a pain to deploy the service and eventually one of my friends solved the issue by using a static website with everything hard coded and was deployed in less than 1 hour.\nMoral of story: Use the right technology at the right time\nMonolith is ideal for new projects which are meant to be served in a low/medium scale with less knowledge on how to build it.\nMicroservices are ideal for projects which you already have some domain experience on and know what exactly are the services split up needed for satisfying your requirement.\nThanks for reading the article :)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kurian Benoy",
    "section": "",
    "text": "I am a Machine Learning Engineer working in Sentient.io, a startup based in Singapore.\nI have multiple years of experience in Python and Machine learning experience, where I am now looking more into the MLOPs side. I have contributed to various open source organizations like Keras, DVC, HuggingFace, fast.ai, Swathanthra Malayalam Computing, CloudCV etc.\nI like to speak and interact with developer community. I am an active member of fast.ai community and engage in lot of other communities based in Kerala. I have given talk as expert in competence of Machine Learning and core Python in Pycon India, which is the largest gathering of Pythonistas in India for the Python programming language.\nI follow an open to contact policy for anyone to reach me anytime. I am quite easy to reach out via email or feel free to connect or contact me in any of the social media platforms."
  }
]