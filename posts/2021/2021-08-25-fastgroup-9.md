---
aliases:
- /2021/08/25/fastgroup-9
categories:
- fastbook
- myself
- ML
- Deep learning
cover-img: /img/fastgroup-share.jpg
date: '2021-08-25'
layout: post
published: true
readtime: true
share-img: /img/fastgroup-share.jpg
title: Week 10 - Embeddings & Recommender systems, Learning FastBook along with Study
  Group

---

![](/posts/images/fastgroup-share.jpg)

Consider the case of buiding a movie recommendation. We usually use collabrative filtering technique to recommend
a specific movie to a user of XYZ platform. Yet how will we categorize the movies into various categories? We can
do a decent job to recommend movies based on category of movie it belongs to like blockbuster, comic etc. as shown in the picture:

![image](https://user-images.githubusercontent.com/24592806/130835070-6be3023d-5fe3-4dd7-aacd-8378fc97b23f.png)

[Resource 1](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)

Yet realisatically, we won't be able to identify our users taste by just the categories, as people are more diverse in nature.
So this is usually identified by understanding the latent factors corresponding to various users and movies. So in 
a scale of ratings of each movies we can map how much users like it by calculating latent factors as done with `pandas cross-tab` functionality.
This is as shown in below diagram representing movies and users.

![image](https://user-images.githubusercontent.com/24592806/130792010-3941f9e8-f6b3-48e2-a5a9-761dd60763c4.png)

Consider the case of a large orgs like netflix with millions of movies listed. Yet the no of movies watched by a user for our example,
be corresponding to a very empty matrix who just watched 3 movies. We are going to predict,
how much he will like our movies in full list and give some recommendations for the user.

![image](https://user-images.githubusercontent.com/24592806/130836043-e1e73208-efd5-4ba1-9d9d-cf0a796af215.png)

So how are we going to calculate the remaining predictions for rest of movies. It's a 3 step process as mentioned in [Resource 2](https://github.com/fastai/fastbook):

1. Randomly initialize some parameters
2. Calculate our predictions
3. Calculate our loss

> In Deep Learning frameworks, the idea of pandas cross tab to represent the latent factors - which represent the distinct  of a matrix is not
present. To represent our movies like in the below diagram. We need to calculate our particular movie and user combination. Then identify index
of our movie in `user latent factor matrix` and index in `movie latent factor matrix`. Then you need to multiply the predictions dot product.

> The trick is to replace our indices with one-hot encoded vectors to multiply by user factors as shown in code, to calucate latent factors:

```
one_hot_3 = one_hot(3, n_users).float()
user_factors.t() & one_hot_3
```

> So in frameworks like Pytorch, multiplying by a one-hot encoded matrix using the
computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple
concept. The thing that you can multiply the one hot-encoded matrix by (or using a computational shortcut
index into directly) is called **Embedding matrix.** [2]

![image](https://user-images.githubusercontent.com/24592806/130845223-320e23bf-50ec-4e44-90aa-2d5a5999c4ce.png)

Let's look at a practical example of Embedding in real world using github copilot, which uses language modelling over [Codex](https://copilot.github.com/).

![image](https://user-images.githubusercontent.com/24592806/130845726-60cd3e7b-d436-4306-a948-77e14e33d20a.png)


<!-- The user may say, the ads of words in User agents. We need to identify from set of words, and based on 3 dimensional embedding
value. ANd then predict the value. It's optimized using L2. Just by learning , using back propogating. The embedding will
be learned by the program.


- Where is the training data from? How randomly recommend
- Bigger logit layer, then with a distribution. WIth 1 million dataset, you realize there is a lot of issues. Then predict the value. -->

### References

1. [Embedding Google ML Crash Course](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)
2. [Deep Learning for Coders - Pytorch](https://github.com/fastai/fastbook)
3. [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
