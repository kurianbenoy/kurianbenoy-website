<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-08-12">

<title>Kurian Benoy - Week 8 And 9 - Learning FastBook along with Study Group</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kurian Benoy</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://til.kurianbenoy.com/">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/kurianbenoy"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kurianbenoy2"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kurianbenoy"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Week 8 And 9 - Learning FastBook along with Study Group</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastbook</div>
                <div class="quarto-category">myself</div>
                <div class="quarto-category">ML</div>
                <div class="quarto-category">Deep learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 12, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><img src="../../posts/images/fastgroup-share.jpg" class="img-fluid"></p>
<p>So, let’s start another week of learning logs:</p>
<section id="chapter-6-summary" class="level3">
<h3 class="anchored" data-anchor-id="chapter-6-summary">Chapter 6 summary</h3>
<ul>
<li>In chapter 6 of Fastbook we learn that the difference between various loss functions can lead to different utility functions as a whole:</li>
</ul>
<ol type="1">
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">nn.CrossEntropyLoss</a> for single-label multiple class classification</li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">nn.BCEWithLogitsLoss</a> for multi-label classification</li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html">nn.MSELoss</a> for regression</li>
</ol>
<ul>
<li>The question of the loss function, which is used to optimise our model is very critical. Just changing the loss functions in fastai, we can do the different tasks with the same Learner class.</li>
</ul>
</section>
<section id="difference-between-cross-entropy-and-binary-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="difference-between-cross-entropy-and-binary-cross-entropy">Difference between cross-entropy and binary cross-entropy?</h3>
<ul>
<li>We can’t use Cross-Entropy loss for multi-label image classification according to the book due to:</li>
</ul>
<blockquote class="blockquote">
<p>Note that because we have a one-hot-encoded dependent variable, we can’t directly use <code>nll_loss</code> or <code>softmax</code> (and therefore we can’t use <code>cross_entropy</code>):</p>
</blockquote>
<blockquote class="blockquote">
<p><code>softmax</code>, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of <code>exponential</code>); however, we may well have multiple objects that we’re confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be <em>less</em> than one if we don’t think <em>any</em> of the categories appear in an image. <code>nll_loss</code>, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn’t make sense when we have multiple labels.</p>
</blockquote>
<blockquote class="blockquote">
<p>On the other hand, the <code>binary_cross_entropy</code> function, which is just <code>mnist_loss</code> along with <code>log</code>, provides just what we need, thanks to the magic of PyTorch’s elementwise operations. Each activation will be compared to each target for each column, so we don’t have to do anything to make this function work for multiple columns.</p>
</blockquote>
<p>If you forgot, what is cross-entropy loss I highly recommend checking Ravi Mishra’s blogpost on Understanding Cross Entropy](https://ravimashru.dev/blog/2021-07-18-understanding-cross-entropy-loss/)</p>
</section>
<section id="cross-entropy-loss-with-excel" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss-with-excel">Cross-Entropy Loss with Excel</h3>
<p>Cross-Entropy Loss can also be explained using Excel as shown in <a href="https://www.youtube.com/watch?v=CJKnDu2dxOE&amp;t=7482s">Jeremy lesson</a></p>
<p>Let me just briefly summarise, what Jeremy said:</p>
<p>Using Excel, for a Cat, we have the true labels whether it’s a cat or not. Also, our ML model after training predicts whether it’s a cat or not. Based on these differences, when predicting the correct thing very confidently we are giving our model a low loss value, while predicting the labels wrongly should be penalised with a high loss value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/128454662-831c4066-95a4-4422-b2dd-3233732a2b8a.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<p>So cross-entropy loss is basically whether it is a cat multiplied by log of cat activation + whether it’s a dog * (log of dog activation)</p>
<p>If it’s a cat, take the log of cattiness, else it’s a dog, take the log of dogginess ie 1 - log of cattiness. All this works if it’s adding up to 1 only, the probability predictions. The correct activation function to use is softmax, all of the activations add up to one. All activations are greater than 0 as well</p>
</section>
<section id="code-for-cross-entropy-loss-and-binary-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="code-for-cross-entropy-loss-and-binary-cross-entropy">Code for Cross-Entropy Loss and binary Cross Entropy</h3>
<ul>
<li>In Pytorch, softmax + cross entropy loss is <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">nn.crossEntropy</a></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/127782856-66b72af4-57a9-45fa-b0ed-7487997b4b5a.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<ul>
<li>The fundamental thing is loss needs to keep increasing if it’s a simple sigmoid + log + nll. The error is not increasing</li>
<li>The error is increasing only when it’s BCE loss</li>
</ul>
<blockquote class="blockquote">
<p>Note from book: Use the multi-label classification approach in your “multiclassification problems” where you want your model to be able to result in None (which is probably a more common real-world use case)</p>
</blockquote>
<ul>
<li>In the book Jeremy Howards, have described the loss used in binary cross-entropy, as using a sigmoid and then calculating the prediction similar to MNIST loss which we used in chapter 4.</li>
</ul>
<pre><code>def binary_cross_entropy(inputs, targets):
  inputs = inputs.sigmoid()
  return - torch.where(targets==1, inputs,  1- inputs).log().mean()</code></pre>
</section>
<section id="difference-between-sigmoid-and-softmax" class="level3">
<h3 class="anchored" data-anchor-id="difference-between-sigmoid-and-softmax">Difference between Sigmoid and Softmax</h3>
<ul>
<li>Sigmoid is an activation function, which can be defined as follows:</li>
</ul>
<pre><code>def sigmoid(x):
  return 1/(1+torch.exp(-x))</code></pre>
<ul>
<li><p>Sigmoid maps the entire value in the range of 0 to 1. It’s a representation of probability and is commonly used as an activation function in linear regression problems.</p></li>
<li><p>Softmax can be explained as another activation function. It’s defined as the e to the power of probability divided by the sum of the probabilities. PAUSE and watch <a href="https://www.youtube.com/watch?v=CJKnDu2dxOE&amp;t=7482s">Jeremy video on CrossEntropyLoss Explanation using Excel</a>. He very well explains how is softmax calculated, and why is it used in losses like Cross-Entropy loss and label smoothing loss.</p></li>
</ul>
</section>
<section id="lets-about-image-regression-problem-to-classify-center-of-face" class="level3">
<h3 class="anchored" data-anchor-id="lets-about-image-regression-problem-to-classify-center-of-face">Let’s about image regression problem to classify center of face</h3>
<p>Let’s look more at the dataset used in this regression problem that is <a href="https://icu.ee.ethz.ch/research/datsets.html">BIWI Kinect Head Pose Dtaset</a>. In the readme of the dataset more details about the dataset is mentioned:</p>
<pre><code>The database contains 24 sequences acquired with a Kinect sensor. 
20 people (some were recorded twice - 6 women and 14 men)
were recorded while turning their heads, sitting in front of the sensor,
at roughly one meter of distance.

For each sequence, the corresponding .obj file represents a head
template deformed to match the neutral face of that specific person.
In each folder, two .cal files contain calibration information for the depth and the color camera, e.g., the intrinsic camera matrix
of the depth camera and the global rotation and translation to the RGB camera.

For each frame, a _rgb.png and a _depth.bin files are provided, containing color and depth data. The depth (in mm) is already
segmented (the background is removed using a threshold on the distance) and the binary files compressed (an example c code is
provided to show how to read the depth data into memory). The _pose.txt files contain the ground truth information, i.e., the
location of the center of the head in 3D and the head rotation, encoded as a 3x3 rotation matrix.</code></pre>
<p>Let’s load the dataset and look at the file structure of the dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129125896-44532b25-aca8-4e46-b8b1-711467cf718a.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Now we have written a utility function for converting files in subfolder <code>image(_rgb.jpg)</code> and <code>pose file(_pose.txt)</code>. We can easily get all image files using the get_image_files function in fastai. If you are interested in checking how <a href="https://kurianbenoy.com/2021-08-07-get_image_files/">get_image_files work under the hood check my article on it</a>.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129126282-21d48a4c-d94b-428d-9d41-7f0d16b457f9.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>In the Biwi dataset website, it explains the format of the pose text file associated with each image, which shows the location of the centre of the head. This function returns the coordinates as a tensor of two items, ie height and width.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129127875-674a6439-f676-49e6-9ad1-32cddf7cbd8f.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>One important thing is note is that we should not just use a random splitter. The same people appear in multiple images in the dataset, but we want to ensure that our model can generalize to people that it hasn’t seen yet. Each folder contains images for one person. PointBlock is used in the DataBlock block section, to tell fastai that labels represent the coordinates of the centre of face in a way.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129127978-0791cc3c-be03-4a86-893b-74f880a0e43d.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Now we can see for Image Regression, we have used the same Datablock and tell fastai just the dependant and independent variables. For the cnn_leaner, we can create our learner to tell fastai the range of targets ie rescaled between -1 and 1. Similar to how in chapter1 we calculated the ratings in the range of 0.5 to 5.0 for recommendation engines.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129128636-d9465329-a82f-4e62-981f-d0890f9ae5d2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Now it’s all about training and looking at the result. In Learner.show_results. The left side has actual ground truth coordinates and the right side has model performance.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://user-images.githubusercontent.com/24592806/129136205-d5b022d8-6fce-4197-b87f-0bb1ab1263d1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image</figcaption><p></p>
</figure>
</div>
</section>
<section id="sota-technniques-for-image-classification" class="level2">
<h2 class="anchored" data-anchor-id="sota-technniques-for-image-classification">SOTA technniques for Image classification</h2>
<p>In Chapter 7, we learned techniques like :</p>
<ul>
<li>Normalization</li>
<li>Progressive Resizing</li>
<li>Test Time Augmentation</li>
<li>Mixup</li>
<li>Label Smoothing</li>
</ul>
<p>I have been working on normalization in another dataset, and just using normalization alone gave me a 20% boost in accuracy compared to the baseline model used. If you are interested in checking my experiments, just <a href="https://github.com/kurianbenoy/FastAI-notebooks/blob/master/classifying-artists-image-normalization.ipynb">check the notebook here</a>. Also this is the <a href="https://www.kaggle.com/c/dsnet-kaggledays-hackathon">Kaggle In-class comp</a> which I was working on.</p>
<p>Also do checkout <a href="https://twitter.com/amaarora">Aman Arora’s</a> wonderful article on <a href="https://amaarora.github.io/2020/07/18/label-smoothing.html">Label Smoothing</a>. I learned a ton by reading his article and corresponding paper.</p>
<p>Also this week <a href="https://twitter.com/TheZachMueller">Prof.&nbsp;Zach</a> shared how to use the fast.ai framework to its fullest extent in a tweet. Do check it out:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
How can you learn to use the <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw"><span class="citation" data-cites="fastdotai">@fastdotai</span></a> framework to its fullest extent? A thread on what I believe is the most important lesson you can teach yourself: 👇<br><br>1/
</p>
— Zach Mueller (<span class="citation" data-cites="TheZachMueller">@TheZachMueller</span>) <a href="https://twitter.com/TheZachMueller/status/1424030110923907080?ref_src=twsrc%5Etfw">August 7, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The full session recording can be viewed in the below link and thanks for reading 🙏. In case, if I have missed something or to provide feedback, please feel free to reach out to me <span class="citation" data-cites="kurianbenoy2">@kurianbenoy2</span>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Huz-dBghVl8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>