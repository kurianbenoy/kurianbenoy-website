<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kurian Benoy">
<meta name="dcterms.date" content="2019-09-11">

<title>Kurian Benoy - Research paper Summary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kurian Benoy</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://til.kurianbenoy.com/">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/kurianbenoy"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kurianbenoy2"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kurianbenoy"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Research paper Summary</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">audio</div>
                <div class="quarto-category">Deep learning</div>
                <div class="quarto-category">Text To Speech Synthesis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kurian Benoy </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 11, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="transfer-learning-from-speaker-verification-to-multispeaker-text-to-speech-synthesis" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning-from-speaker-verification-to-multispeaker-text-to-speech-synthesis">Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</h2>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this paper we discuss about neural network-based system for Text to Speech(TTS) synthesis that can generate speech audio in the voice of different speakers, including those unseen during training. This research paper was presented during 32nd conference of NeurIPS 2018, Montreal, Canada and is authored by researchers from Google Inc.</p>
<p>The goal of this work is to build a TTS(Text to speech) system which can generate natural speech for a variety of speakers in a data efficent manner. We specifically address a zero-shot type learning system, where a speakers cloned audio can be generated for any speech on getting few seconds of untranscribed reference audio.</p>
<p>Here the approach is to decouple speaker modelling from speech synthesis by independently training a speaker-discriminative network that captures charteristics from small amount of speaker audio data.</p>
</section>
<section id="multispeaker-speach-synthesis-model" class="level2">
<h2 class="anchored" data-anchor-id="multispeaker-speach-synthesis-model">Multispeaker speach synthesis model</h2>
<p>Our system is composed of three independently trained neural networks:</p>
<ol type="1">
<li><p>A <em>recurrent speaker encoder</em> which computes a fixed dimensional vector from a speech signal.</p></li>
<li><p><em>Sequence-to-sequence synthesizer</em> which predicts a mel spectrogram from a sequence of grapheme inputs, conditioned on speaker emedding vector.</p></li>
<li><p>An autoregressive <em>Wavenet vocoder</em>, which converts the spectogram into time domain waveforms.</p></li>
</ol>
<section id="speaker-encoder" class="level3">
<h3 class="anchored" data-anchor-id="speaker-encoder">Speaker Encoder</h3>
<p>It is used to condition the synthesis network on reference speech signal from the desired target speaker. It is critical for capturing the crucial characteristics of different speaker and identify the phonetic signal, background noise.</p>
<p>Input 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM layers of 768 cells, each followed by a projection to 256 dimensions. The final embedding is created by L2-normalizing the output of the top layer at the final frame. During inference, an arbitrary length utterance is broken into 800ms windows, overlapped by 50%.</p>
<p>The network is not optimized directly to learn a representation which captures speaker characteristics relevant to synthesis, we find that training on a speaker discrimination task leads to an embedding which is directly suitable for conditioning the synthesis network on speaker identity.</p>
</section>
<section id="synthesizer" class="level3">
<h3 class="anchored" data-anchor-id="synthesizer">Synthesizer</h3>
<p>The synthesizer is trained to convert a sequnce of phoneme or graheme sequence(the text or audio) and is converted into log-mel spectograms, which are commonly used in Audio applications of Deep learning. The Tactron2 architecture to support multiple speakers is extended to recurrent sequence-to-sequence with attention.</p>
<p>An embedding vector for the target speaker is concatenated with synthesizer encoder output at each step, thus passing embeeding to the attention layer converges across different speakers.The predicted mel spectrogram by synthesizer captures all the relevant details needed for higly qualidy synthesis of various voices.</p>
<p>The synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to a sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words and proper nouns. The network is trained in a transfer learning configuration, using a pretrained speaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio, i.e.&nbsp;the speaker reference signal is the same as the target speech during training</p>
</section>
<section id="neural-vocoder" class="level3">
<h3 class="anchored" data-anchor-id="neural-vocoder">Neural Vocoder</h3>
<p>The Wavenet acts as a vocoder to invert the synthesized mel spectrograms emitted by the synthesis network into time-domain waveforms, ie the final voice you are hearning. Wavent architecture is composed of 30 dilated convolution layers. The network is not directly conditioned on the output of the speaker encoder.</p>
</section>
<section id="interference-and-zero-shot-speaker-adaption" class="level3">
<h3 class="anchored" data-anchor-id="interference-and-zero-shot-speaker-adaption">Interference and zero-shot speaker adaption</h3>
<p>In practise we find that using a single audio clip of a few sconds duration is sufficent to synthesize new speech with the corresponding speaker characteristics, representing zero-shot adaption to novel speakers.</p>
<p>Surprising the results on interference of male and female show some characteristics like strongly pitched voice of men’s voice while females voice are found to be having larger speaking rate for a embedded voice generated. It’s amazing to see how Machines have learned all this.</p>
</section>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>The Google team performed extensive reseach and trained the speaker encoder on a properietary voice seach corpus engine containing 36M utterances and compared resuults two public datasets for speech syntesis and vocoder networks. a) VCTK - contains 44 hours of clean speech from 109 speakers b) LibriSpeech - contains 436 hours of speech from 1172 speakers.</p>
<p>The model performed really well, yet not reached the human level of intellgences which is measured in Mean Opinion score(MOS).</p>
<p>They compared the architecture based on the following parameters: - Speech Naturalness - Speaker similarity - Speaker verification - Speaker embedding space - Number of speaker encoder training speakers - Fictious speakers</p>
<p>Detailed results can be found within <a href="https://arxiv.org/pdf/1806.04558.pdf">the paper</a>.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>If you have reached till this point, kudos to you as most of research papers are so intimidating to read and even reading a summary of it’s contents are no less easy.</p>
<p>This system gives the SOTA for Text to speech systems and is able to achieve near-human accuracy on comparing Speaker simiilarity. This research paper is currently implemented with<a href="https://cloud.google.com/text-to-speech/">Google clouds Text to Speech API</a>. We learned about the of MultiSpeaker TTS system and covered the various components like Encoder, Vocoder , Synthesiser in this post.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>