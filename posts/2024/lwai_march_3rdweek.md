---
title: üì¢ Last Week in AI(LWAI) - March 11th to March 17th üìÖ
date: 2024-03-18
author: "Kurian Benoy"
type: post
image: /posts/images/grok_cover.png
---

Hello everyone, this is Kurian posting about some interesting things which happend in the AI world in last week. 
Do checkout this weeks news and if you find it interesting do let me know via comments or if we ever meet IRL.

üóìÔ∏è Monday:

1Ô∏è‚É£ Netflix cautions against blindly using cosine similarity as a measure of semantic similarity between learned embeddings, as it can yield arbitrary and meaningless results.

[Paper Link](https://arxiv.org/abs/2403.05440)

[Tweet Link](https://twitter.com/_reachsumit/status/1767045820384477575)


2Ô∏è‚É£ Some interesting thoughts by [Peter Gostev](https://www.linkedin.com/in/peter-gostev/) on how LLMs are a lot more cheaper than previous tradional NLP techniques.

[LinkedIn Post Link](https://www.linkedin.com/posts/peter-gostev_it-is-interesting-to-reflect-how-much-llms-activity-7172001358806405122-S6mj/?utm_source=share&utm_medium=member_desktop)

3Ô∏è‚É£ Researchers have introduced a ‚ÄòMind Wipe‚Äô technique for erasing hazardous knowledge from AI systems, ensuring functionality remains while enhancing safety. Alongside, the Weapons of Mass Destruction Proxy (#WMDP) benchmark, with 4,157 questions targeting biosecurity, cybersecurity, and chemical security, has been made public.

[Tweet Link](https://twitter.com/pandeyparul/status/1767190910906057157)

[Blog Link](https://www.wmdp.ai/)

[Paper Link](https://arxiv.org/abs/2403.03218)

[Github Link](https://github.com/centerforaisafety/wmdp)

üóìÔ∏è Tuesday:

1Ô∏è‚É£ Infrastructure details for training llama3 models by facebook has been released.

[Blog Link](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)

[Tweet Link](https://twitter.com/soumithchintala/status/1767579981419315400)

2Ô∏è‚É£ OpenAI team released something open-source in a while. Transformer Debugger(TDB) is a tool developed by OpenAI's Superalignment team with the goal of supporting investigations into specific behaviors of small language models.

[Tweet Link](https://twitter.com/janleike/status/1767347608065106387)

[Github Repo Link](https://github.com/openai/transformer-debugger)

[LinkedIn Post](https://www.linkedin.com/posts/kurianbenoy_transformer-debugger-tdb-is-a-tool-developed-activity-7173231708165619712-hiZ1?utm_source=share&utm_medium=member_desktop)

3Ô∏è‚É£ Devin AI, the first AI software engineer was really the news of this week. Let's even cover reaction of this news separately.

[Tweet Link](https://twitter.com/cognition_labs/status/1767548763134964000)

[Blog Link](https://www.cognition-labs.com/introducing-devin)

{{< video https://www.youtube.com/embed/fjHtjT7GO1c >}}


4Ô∏è‚É£ code2prompt, a CLI tool to convert your codebase into a single LLM prompt with source tree, prompt templating, and token counting was released as open-source software with MIT License by [Mufeed V H](https://twitter.com/mufeedvh)

[Tweet Link](https://twitter.com/mufeedvh/status/1767529667496427601)

[Github Link](https://github.com/mufeedvh/code2prompt)

5Ô∏è‚É£ [Santhosh Thottingal](https://thottingal.in/) gave a fabulous talk on AI and making it work in my mother tongue Malayalam. The youtube video was published on this day while it was actually delivered in a National Seminar organized by the Tirur regional centre of Sree Sankaracharya University of Sanskrit on January 6, 2024.

[Video Link](https://www.youtube.com/watch?v=aE7o62zS_eI)

{{< video https://www.youtube.com/embed/aE7o62zS_eI >}}

üóìÔ∏è Wednesday:

1Ô∏è‚É£ I compiled the reactions to the news of Devin AI, the first AI Engineer by various folks.

[Reaction by Andrej Karpathy](https://twitter.com/karpathy/status/1767598414945292695)

[Reaction by Gergely Orosz](https://twitter.com/GergelyOrosz/status/1767591690938822906)

[Reaction by Sergio Periera](https://twitter.com/SergioRocks/status/1767690345473605973)

[Reaction by Andr√© Oliveira](https://twitter.com/smackingg/status/1767689754324107734)

2Ô∏è‚É£ Claude 3 family of Haiku models was released. Haiku is the fastest and most affordable model in its intelligence class was released by Anthropic.

[Tweet Link](https://twitter.com/AnthropicAI/status/1768018310615151002)

[Blog Link](https://www.anthropic.com/news/claude-3-haiku)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1310697%2Fce98524b153345e95d6ddeb651a55afa%2Fclaude3.png?generation=1710758822290128&alt=media)


3Ô∏è‚É£ Modular with their Max Engine's can give 2-5X improvement without any quanitzation or tricks which reduce the accuracy.

[Tweet Link](https://twitter.com/clattner_llvm/status/1767979691007422821)

[Blog Link](https://www.modular.com/blog/evaluating-max-engine-inference-accuracy-on-the-imagenet-dataset)

üóìÔ∏è Thursday:

1Ô∏è‚É£ AI4Bharat team released Indic LLM Suite, a blueprint for training and fine-tuning LLMs in Indic Languages.

[Blog Link](https://ai4bharat.iitm.ac.in/blog/indicllm-suite/)

[Paper Link](https://arxiv.org/abs/2403.06350)

[Github Repo Link](https://github.com/AI4Bharat/IndicLLMSuite)

[Dataset Link](https://huggingface.co/collections/ai4bharat/indicllmsuite-65ee7d225c337fcfa0991707)

2Ô∏è‚É£ [Hrishi Olickel](https://twitter.com/hrishioa) who is the CTO of Greywing has been writing some awesome articles in huggingface community blog about how to make better RAGs(Retrieval Augmentation Generation). Do check his articles:

[Part 1 Blog Link](https://huggingface.co/blog/hrishioa/retrieval-augmented-generation-1-basics)

[Part 2 Blog Link](https://huggingface.co/blog/hrishioa/retrieval-augmented-generation-2-walking)

[Part 3 Blog Link](https://huggingface.co/blog/hrishioa/retrieval-augmented-generation-3-structure)

[Tweet Link](https://twitter.com/hrishioa/status/1745835962108985737)

3Ô∏è‚É£ [Chip Huyen](https://twitter.com/chipro) went through most popular AI repositories in github, categorized them, and studied their growth trajectories. Check the full analysis in blog and tweet.

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1310697%2Fe9a4b8d860da7e8288e7534bc3c3d902%2FGIqTdX1bkAA8ZRf.jpeg?generation=1710758337730637&alt=media)

[Blog Link](https://huyenchip.com/2024/03/14/ai-oss.html)

[Tweet Link](https://twitter.com/chipro/status/1768388213008445837)

üóìÔ∏è Friday:

1Ô∏è‚É£ Last week, I mentioned about ragas by Jithin James and Shahul ES (my class-mates as well) being selected for Y Combinator.
This was featured in one of leading news dailies in Kerala, Mathrubhumi. I appreciate their editor Manoj K Das and [R Roshan](https://www.linkedin.com/in/rroshandotcom/) for featuring them in your esteemed news daily.

[News Link](https://newspaper.mathrubhumi.com/news/business/business-1.9405970)

[Linkedin Post Link](https://www.linkedin.com/posts/rroshandotcom_ragas-malayalistartup-opensource-activity-7174284949196271617-Lz__?utm_source=share&utm_medium=member_desktop)

2Ô∏è‚É£ [Pratik Desai](https://www.linkedin.com/in/pratikkumardesai/) founder of [KissanAI](https://www.linkedin.com/company/kissanai/) announed a new series of fine-tuned Vision LLMs for pest and disease detection and conversation over cure, symptoms, severity and prevention.
The Dhenu-vision-lora-0.1 is fine-tuned Qwen-VL-chat, for 3 major crops and 10 diseases, giving 2x performance boost over the base and was trained on synthetic data generated for around 9000 disease images. 

[Linkedin Post Link](https://www.linkedin.com/posts/pratikkumardesai_llm-visionllm-agriculture-activity-7174387056020762624-WZVo)

[Model Link](https://huggingface.co/KissanAI/Dhenu-vision-lora-0.1)

3Ô∏è‚É£ Govt of India released an updated advisory toning down what they said earlier. The advisory has been sent only to 8 large social media like organization, some upcoming well-funded AI startups in India has been exempted from this for now.

[News Link](https://www.hindustantimes.com/india-news/in-revised-ai-advisory-it-ministry-removes-requirement-for-government-permission-101710520296018.html)

[Tweet Link](https://twitter.com/kurianbenoy2/status/1768680935263019350)

4Ô∏è‚É£ Hiring managers are now expecting like 6+ years of experience in GenAI, this reminds me of one post by creator of FASTAPI Sebasti√°n Ram√≠rez who said even he didn't have 5+ years of experience in FASTAPI when someone asked for that when hiring.

[Tweet Link](https://twitter.com/jobergum/status/1768390591493140694)

5Ô∏è‚É£ Google released Cappy, a small pre-trained scorer model that enhances and surpasses the performance of large multi-task language models. Cappy has been tested across a variety of complex tasks from PromptSource and Big-Bench.

[Blog Link:](https://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html)

üóìÔ∏è Saturday:

1Ô∏è‚É£ Apple announces the paper MM1 - Methods, Analysis & Insights from Multimodal LLM Pre-training. They discuss how to build good performance multi-modal LLMs which means LLMs would be able to see, speak etc. in future beyond what it's now already doing well that is reading and writing.

[Paper Link](https://arxiv.org/abs/2403.09611)

[Reaction by Dr Jim Fan](https://twitter.com/DrJimFan/status/1769053019939967080)

[Linkedin post](https://www.linkedin.com/posts/hamdi-amroun-phd-141388109_paper-page-mm1-methods-analysis-insights-activity-7174560264241991680-x9Mo?utm_source=share&utm_medium=member_desktop)


2Ô∏è‚É£ Shaheen Gemma 7B, a model being finetuned on Urdu Alpaca dataset. It's great to see more fine-tuned LLMs in all regional languages in India. Lot of folks are putting effort in bringing my mothertongue language to fore in realm of Generative models.

[Model Link](https://huggingface.co/Xhaheen/Shaheen_Gemma_Urdu_)

3Ô∏è‚É£ Anwesha Sen wrote a very well written blog post about the previous AI advisory by govt of India and talk about it's vague clauses, terms which was like a stepping back into license raj.

[News Link](https://thewire.in/tech/indias-ai-advisory-vague-clauses-and-terms-dont-help-anyone)


üóìÔ∏è Sunday:

xAI open sourced their chatbot Grok by releasingg the weights and architecture of our 314 billion parameter Mixture-of-Experts model, Grok-1.

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1310697%2F8184a2b4366a3e0606fb4f10c4ad89c8%2Fgrok_cover.png?generation=1710758322201107&alt=media)

[Blog Link](https://x.ai/blog/grok-os)

[Github Repo Link](https://github.com/xai-org/grok-1)

[Model Weights Link](https://huggingface.co/xai-org/grok-1)

Writing this article and compiling these news took me aboyt 5+ hours. So your contribution to help my work by buying me a coffee would be [absolutely pixel-perfect amazing](https://www.buymeacoffee.com/kurianbkk8).

