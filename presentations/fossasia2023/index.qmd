---
title: OpenAI Whisper and it's amazing power to do finetuning.
author: Kurian Benoy
subtitle: To be presented in FOSSASIA Summit, Singapore
date: 2023-04-15
date-format: full
format:
  revealjs:
    theme: solarized
    footer: "[kurianbenoy.com/presentations/fossasia2023/index.html](https://kurianbenoy.com/presentations/fossasia2023/index.html)"
    slide-number: true
---

## Outline

- OpenAI Whisper(under appreciated model) - 1littecoder video
- Why it's awesome(whisper.cpp, long form transcription, lot of languages, whisper_normalizer)
- What is fine tuning? (Jeremy way of explaining)
- Fine tuning whisper in my language
- Results on fine tuning
- Fine tuned models
- You can also achieve SOT in your language

## OpenAI Whisper

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.

- Green ^[A footnote]
- Hello

::: aside
The name Whisper comes from abbrevation WSPR, which stands for Web-scale Supervised Pretraining for Speech Recognition.
:::


## About model {background-color="black" background-image="https://placekitten.com/100/100" }

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.

## Whisper Models {.scrollable}

There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. 


|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |

## Whisper Models

Below are the names of the available models and their approximate memory requirements and relative speed. Large itself has two version: large-v1 and large-v2.

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.

::: {.notes}
Rewrite this.
:::

## Text

:::: {.columns}

::: {.column width="60%"}
Left column

Hello

- x
- y
:::

::: {.column width="40%"}
Right column
:::

::::
