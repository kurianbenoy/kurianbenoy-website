---
title: OpenAI Whisper and it's amazing power to do finetuning.
author: Kurian Benoy
subtitle: To be presented in FOSSASIA Summit, Singapore
date: 2023-04-15
date-format: full
format:
  revealjs:
    theme: solarized
    footer: "[kurianbenoy.com/presentations/fossasia2023/index.html](https://kurianbenoy.com/presentations/fossasia2023/index.html)"
    slide-number: true
---

## Outline

- OpenAI Whisper(under appreciated model) - 1littecoder video
- Why it's awesome(whisper.cpp, long form transcription, lot of languages, whisper_normalizer)
- What is fine tuning? (Jeremy way of explaining)
- Fine tuning whisper in my language
- Results on fine tuning
- Fine tuned models
- You can also achieve SOT in your language

## OpenAI Whisper

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.

- Green ^[A footnote]
- Hello

::: aside
The name Whisper comes from abbrevation WSPR, which stands for Web-scale Supervised Pretraining for Speech Recognition.
:::


## About model {background-color="black" background-image="https://placekitten.com/100/100" }

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.

## Whisper Models {.scrollable}

There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. 


|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |

## Whisper Models

Below are the names of the available models and their approximate memory requirements and relative speed. Large itself has two version: large-v1 and large-v2.

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.

::: {.notes}
Rewrite this.
:::

## Text

:::: {.columns}

::: {.column width="60%"}
Left column

Hello

- x
- y
:::

::: {.column width="40%"}
Right column
:::

::::

## OpenAI Whisper Features

- English Speech Recognition
- Multi-lingual speech recognition
- Support for multiple tasks
- Can run in almost any devices with whisper.cpp
- Awesome community plugins


## English Speech Recognition



## Multi-lingual Speech recognition

- Whisper supports 99 languages
- It really supports just 57 languages really well though, as these languages are provided in OpenAI Whisper API.

Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.

## Multiple tasks

- Like Language recognition
- Translation of audio from X-> en

## Runs in almost any device

- Whisper.cpp developed by  Georgi Gerganov 
- https://github.com/ggerganov/whisper.cpp

## Awesome community plugins

- 1. WhisperX — Word-level time stamps with Whisper
- . Fine-Tune Whisper For Multilingual ASR with HuggingFace Transformers
- Speaker diarization

https://huggingface.co/spaces/dwarkesh/whisper-speaker-recognition

- Audio classification using OpenAI’s Whisper

https://github.com/jumon/zac

[Thanks to Ramsri Goutham](https://ramsrigoutham.medium.com/openais-whisper-7-must-know-libraries-and-add-ons-built-on-top-of-it-10825bd08f76)

## What is fine tuning?

Given a pre-trained model, in a very small dataset we will train
and improve the model to work really well in our specific tasks.

Whisper is developed as a robust speech recognition system which is
intented to work in out of distribution dataset. While most of previous
approaches relied on training in a few open-source dataset and using them.

## Fine tuning is still relevant

{{< tweet waydegilliam 1641228571611123712 >}}

