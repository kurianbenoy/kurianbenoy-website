---
title: Deep Learning For Coders with fastai Couse - Lesson 3
type: post
published: false
tag: [fastbook, myself, ML, Deep learning, fastai, journey]
readtime: true
author: kurianbenoy
---

There is a minor delay in streaming the lessons today. Lesson 1, 2 are easy for everyone, while
usually lesson 3 is more hard defenitely.

Fastai lesson 0, https://kurianbenoy.com/2021-06-16-fastgroup-1/

Idea of running notebooks, understanding everything in that particular lesson.
Then try to reproduce.(clean notebook approach, to test your understand)
If you can do with different dataset, ie the hard time.

Always study done, with other people is the best activity. This week, with the best no of votes.

My work also got featured, with lot of votes.

Today Jeremy featured, a gradient descend platform. He has been using it for 
paperspace, and it's totally amazing. He got something done by them. He is going
to add walk throughs of various lessons.

In lesson2, it's not taking a particular lesson and use in particular platform. There are two
important pieces:

1) Training piece
2) Deployment piece

train.ipynb
app.ipynb - inference

Finding good image models, by using and looking at a better architecture.

He tried levit_models, didn't work really great

timm.[25.00] - experiment with latest models. It got really good accuracy with
0.0005. Now there are lot of good architectures, which beats resnet really well.
22,000 categories of images. If you can do better, go do it.

37 breeds, then in case of category. Usually that category is in the vocab of dataloaders.

Just run learn.model. Understand what is stuff in learn?

In deep learning lot of trees

[30:00] get modules in pytorch. Layer norm things

What are these numbers? What are these parameters. How can these number figure out it's a   particular dog or not

Partial application, it's just something in lot of languages. How applying quardartic

data matching function with data, adding noise with data
`torch.linspace` which goes from -2 to +4

Create a plot quadratic, which helps in interacting. Using @interact
most simple and common loss functions MSE

Rerun with MSE. Now add MSE, and see if it get's better or worse. This is a manual process,
and does he tweak. When we move up, does the loss get's better. Or if it decreases, does loss goes down.


The derivate is a function, which tells if we increase does it increase or decrease. In pytorch,
youu can automtically done by pytorch.

ipythonwidgets using intereact

return interact. Does mse made by pytorch interact

Rank 1 tensor, 1 D tensor

[49:00] onwards follow the calculation again, and try to do it again.
Now it got and calculated the loss. 

```
With torch.no_grad():

```

inner part of machine learning code. This is basic optimizer. This is gradient descent.
We calculate gradient, and then build models. Not following stuff in lesson [57:00] is not clear for me.

We learned deep learned. This is like how to draw an owl. This is how deep learning is, just
go through the course.

https://pytorch.org/tutorials/

I want to be around 0.001 second. Doing grid search takes a lot of time. Training your good
models, at the first day is not a big requirement. It's very easy to get inputs & outputs,
yet getting segmented output is way harder. The important number, learning_rate to calculate parameters.

After break

Mathematical trip, we want to do a whole bunch of RELUs. We want lot's of variables.
Add all the relus together, then use with different bunch of behaviours. 1000s of RELUs

SIngle operation except last layer, with matrix multiplication.
Linear algebra, almost all time you need is matrix multiplcator. It's multiplying
and adding neural networks together. GPUs are so good at this.

http://matrixmultiplication.xyz/

fast.ai using excel for matrix multiplication. From [1:28:32]

Titanic data, about who survived and who didnt'

validation and metrics optimization. He is going to look into that Kaggle dataset.
