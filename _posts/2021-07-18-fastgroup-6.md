---
title: Log6- Learning FastBook along with Study Group
type: post
published: True
tags: [fastbook, myself, ML, Deep learning]
readtime: true
cover-img: "/img/fastbookgroup.jpeg"
share-img: "/img/fastbookgroup.jpg"
---

So as always, let's start with a quick recap of what was covered during the session:

> TLDR: Covered Torch Optimizer, presizing, what is cross entropy and model entropy.

- We started with PETs dataset. This time we were classifying Breed of Cat/Dog using fastai image classifier.
- The dataset followed a particular heuristics, which represented Species with capital case as Cats and smaller case as Dogs. For the breeds also a similar pattern is there:

`Path('/root/.fastai/data/oxford-iiit-pet/images/Maine_Coon_157.jpg')`
- The breed can be obtained using the regex patterns. It's recommended to read a regex tutorial and solve with a regex problem set.
`re.findall(r''(.+)_\d+.jpg)`
- Most functions and methods in fastai return a special collection class called L(part of fastai core library)
- Datablock used for loading pets breed classifier
```
pets = DataBlock(blocks=(ImageBlock, CategoryBlock),
                 get_items=get_image_files,
                 splitter=RandomSplitter(seed=42),
                 get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
                 item_tfms=Resize(460),
                 batch_tfms=aug_transforms(size=224, min_scale=0.75)
)
dls = pets.dataloaders(path/"images")
```

### *What is Presizing?*

- It's a technique in which we transform input image into a larger size image, and then use augmentation techniques like Random cropping to get 
the full picture of what we are trying to classify.
- Usually for transforming images to larger size like Resize operation we use a CPU(Central processing Unit). Then process of augmentation techniques like cropping,
RandomResizing is usually done with a GPU(Graphic process units) because it requires uniform size images.

![image](https://user-images.githubusercontent.com/24592806/126095655-33e269f0-2841-472c-aedc-39489bc34cf3.png)

- In practise, according to [Aman Arora](https://wandb.ai/aarora) we spend a lot of time in loading data correctly when working with a new dataset. This is
where some errors come. 
- When errors come in our datablock, use the `sumarry method in DataBlock`

![image](https://user-images.githubusercontent.com/24592806/126096059-a4570b1d-853a-44a4-b211-85e8924cdb70.png)

- Now when training our model, across a few epochs(doing a complete pass of data). The learn.fine_tune() method shows average loss over training
dataset and loss of validation set. What is this loss function used, is it the loss function we learned in chapter 4?

![image](https://user-images.githubusercontent.com/24592806/126096364-f189e555-f125-400b-8648-890fb8e125f2.png)

- By default, fastai chooses appropriate loss funciton based on Datablock type. Generally for an image dataset, with categorical outcome we usually use
**cross entropy loss**

### What is Cross Entropy Loss?

Let's look it step by step. Let's first view the *activations and labels of  our data loader*

![image](https://user-images.githubusercontent.com/24592806/126096724-3220e5f0-27ba-458f-8980-fa7c72a55ccd.png)

- We can see our pets are converted into associated tensor values representing the category into 0-37 values in tensor.
- The prediction probabilities sum all add up to 1

*What is Softmax  function?*

- When we were classifying 3s and 7s in MNIST dataset, it was a binary classification problem, with target being a boolen value. We used sigmoid function
- Cross Entropy loss is being used because of two benefits:

1. It works when our dependant variables(like classifying pet breeds) with more than two categories
2. It results in faster and reliable training 

- Now for classifying 2s and 7s, its reliable for a set of random activations to be substracted with difference of activation for three and seven. Followed by calculating a
sigmoid function.

`(acts[:, 0] - acts[:, 1]).sigmoid()`

- The softmax function is represented in formula as below. Obiviously there is a pytorch function for it

```
def softmax(x):
  return (exp(x) /exp(x).sum(dim=1, keepdim=True))
```

![image](https://user-images.githubusercontent.com/24592806/126097349-af9247e6-5461-4562-99c6-c2191783853d.png)

*What is log likelihood loss*

- We need to calculate the loss for each no like mnist_loss(target=1, 1-inputs). Yet for multiple categories, we need a better method
- For this we use the log likelihood loss method. When considering things in log scale, a 99.99% and 99.9% is usally 10 times larger, even though in absolute terms it seems small.

![image](https://user-images.githubusercontent.com/24592806/126097580-613dcdaa-6628-4b69-8c9b-b08163b01078.png)

*Combining together*

![image](https://user-images.githubusercontent.com/24592806/126097634-9f150ecf-b77b-4a60-af68-0fdfc0382fe7.png)

- Softmax + Negative Log Likelihood = Cross Entropy Loss
- Most of the time, we calculate with torch methods like `nn.CrossEntropyLoss(reduction='none')(acts, targ)`.

### Model Interpretation result

- The wrong results, and where our model has been wrong can't be often spotted merely with help of metrics like accuracy.
- So we use a confusion matrix and plot the function with top losses.

![image](https://user-images.githubusercontent.com/24592806/126098073-5a3a8e9f-5a79-4d9e-a4ca-52bcb24d0adb.png)

### Learning rate finder and freezing pretrained layers

- Using a learning rate finder developed by [Leslie Smith](), we can find the minimum step and maximum steep point of learning rate
- To find the appropriate learning rate, using at some point of middle of lr_rate_finder() is a food idea. But always remember to use logarithimic
scale even here to calculate learning rate finder
- When we are using models with transfer learning, they are not trained for the specific model which we are trying to classify.
- For this we freeze the last layers, and train again

![image](https://user-images.githubusercontent.com/24592806/126098417-536b1069-a3df-468d-8859-cbf42ebd7317.png)

- Fastai `fine_tune` method under the hood:

1. Trains randomly for added layers initially for certain epochs
2. Unfreeze all the layers, then train again

### Discriminative learning rate, and rest of portions

- First train the model with slow learning rate, and then later train faster. This is fundamental premise
- Early stopping is a not good method as mentioned in the book below

> Before the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the 
> models saved in each epoch. This is known as *early stopping*. However, this is very unlikely to give you the best answer, because those epochs in the middle occur before the 
> learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfit, what you should actually do 
> is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.

- Using lr_max, with slice method to train the last layers with a faster spped
- We can improve models with deeper architecture like Resnet 34, 50, 101, 152 etc. There are new techniques now like EfficentNet, Transformers, which can give a better accuracy now.
- Then it's a good idea to train with half precision floating points, which speeds up the training process considerably

![image](https://user-images.githubusercontent.com/24592806/126098914-56e1d5de-359c-479e-8344-8333f32e6c79.png)


[![IMAGE ALT TEXT](http://img.youtube.com/vi/bvtr_1TN6MI/0.jpg)](https://youtu.be/bvtr_1TN6MI "Session Recordings of Week 6")



Also at the end of session,  [@Aman Arora](https://github.com/amaarora) shared four blogpost ideas:

1. Write about what `untar_data` method is doing under the hood?
2. Write about what is `fastai foundation class L` is doing. What are the unique features of this special list part of fastai core library?
3. Write about `regex` which is a pretty useful library, and write on what all cool things you can do with it.
4. Write about `cross_entropy_loss` .

My work logs, trying out things mentioned in Chapter 5 can be found [in this Kaggle notebook](https://www.kaggle.com/kurianbenoy/fastbook-ch5/).
