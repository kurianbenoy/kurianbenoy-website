---
title: Learning Logs- Fastbook Week 10 - Embeddings & Recommender systems
type: post
---

In Deep Learning frameworks, the idea of pandas cross tab to represent the latent factors - which represent the distinct  of a matrix is not
present. To represent our movies like in the below diagram.

![image](https://user-images.githubusercontent.com/24592806/130792010-3941f9e8-f6b3-48e2-a5a9-761dd60763c4.png)


```
foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])
bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])
>pd.crosstab(foo, bar, dropna=False)
col_0  d  e  f
row_0
a      1  0  0
b      0  1  0
c      0  0  0
```

In frameworks like Pytorch, multiplying by a one-hot encoded matrix using the computational shortcut
that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple
concept. The thing that you can multiply the one hot-encoded matrix by (or using a computational shortcut
index into directly) is called Embedding matrix.

![image](https://user-images.githubusercontent.com/24592806/130835070-6be3023d-5fe3-4dd7-aacd-8378fc97b23f.png)

![image](https://user-images.githubusercontent.com/24592806/130836043-e1e73208-efd5-4ba1-9d9d-cf0a796af215.png)

![image](https://user-images.githubusercontent.com/24592806/130836136-53a9d509-9bde-4131-8d4e-e7b1b961a970.png)

The user may say, the ads of words in User agents. We need to identify from set of words, and based on 3 dimensional embedding
value. ANd then predict the value. It's optimized using L2. Just by learning , using back propogating. The embedding will
be learned by the program.

![image](https://user-images.githubusercontent.com/24592806/130836419-30875e7e-9f13-4ab7-b6d3-31ec2cd712a2.png)

- Where is the training data from? How randomly recommend
- Bigger logit layer, then with a distribution. WIth 1 million dataset, you realize there is a lot of issues. Then predict the value.
