---
title: Log5- Learning FastBook along with Study Group
type: post
published: false
tags: [fastbook, myself, ML, Deep learning]
readtime: true
---

In the first chapter, we learned Arthur Samuel mentioned the following:

> Samuel said we need an automatic means of testing the effectiveness of any current 
> weight assignment in terms of actual performance. In the case of his checkers program,
> the "actual performance" of a model would be how well it plays. And you could 
> automatically test the performance of two models by setting them to play against each other,
> and seeing which one usually wins.
> Finally, he says we need a mechanism for altering the weight assignment so as to maximize the performance.
> For instance, we could look at the difference in weights between the winning model and the losing model,
> and adjust the weights a little further in the winning direction.

![image](https://user-images.githubusercontent.com/24592806/125390450-89cbe880-e3c0-11eb-9cc6-894d8917a345.png)

![image](https://user-images.githubusercontent.com/24592806/125390478-951f1400-e3c0-11eb-88c3-d0d15d1de9dc.png)


In this lesson we looked more into gradient descent, specifically stochastic gradient descent.

<Note to myself: when writing do all the steps again, think why we are doing this? - answer that question)

- Behind pytorch magic

After the lesson, I tried answering the quiz and my performance was just 20/37



