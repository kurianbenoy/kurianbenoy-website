---
title: Log5- Learning FastBook along with Study Group
type: post
published: false
tags: [fastbook, myself, ML, Deep learning]
readtime: true
---

Just a quick recap In the first chapter, we learned Arthur Samuel mentioned the following:

> Samuel said we need an automatic means of testing the effectiveness of any current 
> weight assignment in terms of actual performance. In the case of his checkers program,
> the "actual performance" of a model would be how well it plays. And you could 
> automatically test the performance of two models by setting them to play against each other,
> and seeing which one usually wins.
> Finally, he says we need a mechanism for altering the weight assignment so as to maximize the performance.
> For instance, we could look at the difference in weights between the winning model and the losing model,
> and adjust the weights a little further in the winning direction.

![image](https://user-images.githubusercontent.com/24592806/125390450-89cbe880-e3c0-11eb-9cc6-894d8917a345.png)

And the book mentions:

> Neural networks - a function that is so flexible to be used to solve for any given problem
> SGD - a mechanism for automatically updating weight for every problem

The whole training loop of any ML task:

![image](https://user-images.githubusercontent.com/24592806/125390478-951f1400-e3c0-11eb-88c3-d0d15d1de9dc.png)

This week in the group we were looking more into Second half of Chapter 4 which covers the gradient descent,
specifically stochastic gradient descent:

## The MNIST Loss Function

Pytorch view function(), to convert a rank3-tensor(matrix) to rank-2 tensor(vectors).

Took training and valdataion_dataset

Used it with a model, initialized by init_params() for weights and bias which are parameters in ML model. Use a linear model

def linear1(xb):
  return xb@weight + bias

Then used mnist_loss() function which is calculation difference with loss and targets

## Sigmoid function

- made predictions of our function smoothen b/w vlaue 0 and 1 with sigmoid in mnist_loss()

## SGD and mini batches

DataLoader can take inputs and split inputs to small batrch to process things in GPU memory. WHole dataset need not always fit in memory

## Putting it together

We are implementing gradient descent algorithm(7 steps here)

call init_params, define train_dl and valid_dl, calculate the gradients(calc_gradients)

Then train_epoch
batch_accuracy
validate_epoch

finally 96% acuracy

## Creating an optimizer

Used Basic Optim function

modified trainin_epoch function

new train_model func()

97% accuracy

## Using a non-linearity

- like reul, Use a Learner, more accruacy imporvement

ie 98.2%

## Going Deeper

ImageDataLoaders.from_folder(path)
cnn_learn(dls, renset18, pretrained=False, ..)
learn.fit_one_cycle(1, 0.1)

got 99% accuracy


<Note to myself: when writing do all the steps again, think why we are doing this? - answer that question)


- Behind pytorch magic

After the lesson, I tried answering the quiz and my performance was just 20/37



