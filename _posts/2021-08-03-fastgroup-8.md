---
title: Log8- Learning FastBook along with Study Group
type: post
published: False
tags: [fastbook, myself, ML, Deep learning]
readtime: true
cover-img: "/img/fastgroup-share.jpg"
share-img: "/img/fastgroup-share.jpg"
---

So, let's start another week of learning logs from week8.

## Difference between cross-entropy and binary cross-entropy?

- In this chapter we learn that difference between various loss functions can lead to different utility functions as a whole.
- The question of loss function, which is used to optimise our model is very critical

- We can't use Cross Entropy loss for multi-label image classification according to book for following reasons:

*Note that because we have a one-hot-encoded dependent variable, we can't directly use `nll_loss` or `softmax` (and therefore we can't use `cross_entropy`):*

- *`softmax`, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of `exp`); however, we may well have multiple objects that we're confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be *less* than 1, if we don't think *any* of the categories appear in an image.*
- *`nll_loss`, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn't make sense when we have multiple labels.*

*On the other hand, the `binary_cross_entropy` function, which is just `mnist_loss` along with `log`, provides just what we need, thanks to the magic of PyTorch's elementwise operations. Each activation will be compared to each target for each column, so we don't have to do anything to make this function work for multiple columns.*


If you forgot, what is cross entropy loss I highly recommend to check [Ravi Mishra's blogpost on Understanding Cross Entropy](https://ravimashru.dev/blog/2021-07-18-understanding-cross-entropy-loss/)

- Cross Entropy can also be explained using Excel as shown in [Jeremy lesson](https://www.youtube.com/watch?v=CJKnDu2dxOE&t=7482s)

Let me just breifly summarise, what jeremy said. Using Excel, for a Cat, we have the true labels whether it's a cat or not. Also 
our ML model after training predicts. Based on these difference, when predicting the correct thing very confidently we are giving our model
a low loss value, while predicting the correct thing wrong should be penalised with a high loss value.

![image](https://user-images.githubusercontent.com/24592806/128454662-831c4066-95a4-4422-b2dd-3233732a2b8a.png)

So cross entropy loss is basically whether it is a cat multiplied by log of cat activation + whether it's a dog * (log of dog activation)

If it's a cat, take log of catiness, else it it's a dog, take log of doginess ie 1 - log of catiness. All this works if it's adding upto 1 only, the probability
predictions.

The correct activation function to use is softmax, all of activations add upto 1. All activations are greater than 0 as well

e power/ (sum of e numbers)

- In Pytorch, softmax + cross entropy loss is nn.crossEntropy

https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html

![image](https://user-images.githubusercontent.com/24592806/127782856-66b72af4-57a9-45fa-b0ed-7487997b4b5a.png)

- The fundamental thing is loss needs to keep increasing, if it's a simple sigmoid + log + nll. The error is not increasing
- The error is increasing only when it's BCE loss

- Note: Use the multi-label classification approach in your "multiclassification problems" where you want your model to be able to result in None (which is probably a more common real world use case) 

- In the book Jeremy Howards, have described the loss used in binary cross entropy, as using a sigmoid and then calculating the prediction similar to MNIST loss
which we used in chapter 4.


```
def binary_cross_entropy(inputs, targets):
  inputs = inputs.sigmoid()
  return - torch.where(targets==1, inputs,  1- inputs).log().mean()
```

## Difference between Sigmoid and Softmax



## Then regression problem

Let's look more about the dataset used in this regression problem that is [BIWI Kinect Head Pose Dtaset](https://icu.ee.ethz.ch/research/datsets.html).
In the readme of dataset more details about dataset is mentioned:

```
The database contains 24 sequences acquired with a Kinect sensor. 20 people (some were recorded twice - 6 women and 14 men)
were recorded while turning their heads, sitting in front of the sensor, at roughly one meter of distance.

For each sequence, the corresponding .obj file represents a head template deformed to match the neutral face of that specific person.
In each folder, two .cal files contain calibration information for the depth and the color camera, e.g., the intrinsic camera matrix
of the depth camera and the global rotation and translation to the rgb camera.

For each frame, a _rgb.png and a _depth.bin files are provided, containing color and depth data. The depth (in mm) is already
segmented (the background is removed using a threshold on the distance) and the binary files compressed (an example c code is
provided to show how to read the depth data into memory). The _pose.txt files contain the ground truth information, i.e., the
location of the center of the head in 3D and the head rotation, encoded as a 3x3 rotation matrix.
```

- How is the dataset being loaded?

- Look at numpy problems

- The fundamental difference is MSE loss (Mean squared loss)



