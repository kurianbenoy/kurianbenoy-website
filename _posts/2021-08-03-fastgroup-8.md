---
title: Log8- Learning FastBook along with Study Group
type: post
published: True
tags: [fastbook, myself, ML, Deep learning]
readtime: true
cover-img: "/img/fastgroup-share.jpg"
share-img: "/img/fastgroup-share.jpg"
---

So, let's start another week of learning logs from session 5:

- We are using the same cnn_learner for Muli-label image classification

## Multi-Image Classification Algorithm(Why are we using binary cross entropy loss?)

- Why are we using BCE Loss, instead of cross entropy?
- The question of loss function, which is used to optimise our model is very critical
- We can't use Cross Entropy loss, why according to book:

![image](https://user-images.githubusercontent.com/24592806/127781469-28d1eb35-f899-4579-9495-cd62e3a0917e.png)

If you forgot, what is cross entropy loss I highly recommend to check [Ravi Mishra's blogpost on Understanding Cross Entropy](https://ravimashru.dev/blog/2021-07-18-understanding-cross-entropy-loss/)

![image](https://user-images.githubusercontent.com/24592806/127782856-66b72af4-57a9-45fa-b0ed-7487997b4b5a.png)

- The fundamental thing is loss needs to keep increasing, if it's a simple sigmoid + log + nll. The error is not increasing
- The error is increasing only when it's BCE loss




## Using accuracy_multi metric

## Then regression problem

- The fundamental difference is MSE loss (Mean squared loss)
- 
