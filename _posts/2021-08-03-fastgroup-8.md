---
title: Log8- Learning FastBook along with Study Group
type: post
published: True
tags: [fastbook, myself, ML, Deep learning]
readtime: true
cover-img: "/img/fastgroup-share.jpg"
share-img: "/img/fastgroup-share.jpg"
---

So, let's start another week of learning logs from session 5:

- We are using the same cnn_learner for Muli-label image classification

## Multi-label Image Classification Algorithm(Why are we using binary cross entropy loss?)

- Why are we using BCE Loss, instead of cross entropy?
- The question of loss function, which is used to optimise our model is very critical
- We can't use Cross Entropy loss, why according to book:

![image](https://user-images.githubusercontent.com/24592806/127781469-28d1eb35-f899-4579-9495-cd62e3a0917e.png)

If you forgot, what is cross entropy loss I highly recommend to check [Ravi Mishra's blogpost on Understanding Cross Entropy](https://ravimashru.dev/blog/2021-07-18-understanding-cross-entropy-loss/)

![image](https://user-images.githubusercontent.com/24592806/127782856-66b72af4-57a9-45fa-b0ed-7487997b4b5a.png)

- The fundamental thing is loss needs to keep increasing, if it's a simple sigmoid + log + nll. The error is not increasing
- The error is increasing only when it's BCE loss

- Note: Use the multi-label classification approach in your "multiclassification problems" where you want your model to be able to result in None (which is probably a more common real world use case) 

- In the book Jeremy Howards, have described the loss used in binary cross entropy, as using a sigmoid and then calculating the prediction similar to MNIST loss
which we used in chapter 4.

```
def binary_cross_entropy(inputs, targets):
  inputs = inputs.sigmoid()
  return - torch.where(targets==1, inputs,  1- inputs).log().mean()
```

> Try to explain it to other as simple as possible



## Then regression problem

Look at dataset (00-13 images)
- Look at numpy problems

- The fundamental difference is MSE loss (Mean squared loss)
- 
